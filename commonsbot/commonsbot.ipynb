{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commonsbot.ipynb, a Python script for uploading files and data to Wikimedia Commons using the API.\n",
    "\n",
    "# (c) 2022 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# ----------------\n",
    "# Global variables\n",
    "# ----------------\n",
    "\n",
    "script_version = '0.5.1'\n",
    "version_modified = '2022-08-24'\n",
    "commons_prefix = 'http://commons.wikimedia.org/wiki/Special:FilePath/'\n",
    "commons_page_prefix = 'https://commons.wikimedia.org/wiki/File:'\n",
    "\n",
    "# -----------------------------------------\n",
    "# Version 0.4 change notes: \n",
    "# - Removed double spaces from labels before they are used to generate image filenames.\n",
    "# - Skip over images with raw filenames that contain spaces and log an error for them to be manually removed.\n",
    "# -----------------------------------------\n",
    "# Version 0.5.1 change notes:\n",
    "# - enable writing of multiple Structured Data in Commons claims in single API call\n",
    "# - support both Artwork (for 2D) and Art Photo (for 3D) templates in the Wikitext\n",
    "# - use appropriate SDC licenses for 3D works\n",
    "# - clean up code and convert login to an object\n",
    "# - remove hard-coded values and replace with YAML configuration file\n",
    "# - improve control of throttling between media file uploads to the Commons API\n",
    "# -----------------------------------------\n",
    "\n",
    "# Generic Commons API reference: https://commons.wikimedia.org/w/api.php\n",
    "\n",
    "# Description of bots on Commons: https://commons.wikimedia.org/wiki/Commons:Bots\n",
    "# See guidelines for operating a bot in Commons: https://commons.wikimedia.org/wiki/Commons:Bots/Requests\n",
    "# Need to decide whether this applies if non autonomous. It probably does.\n",
    "# Bot flag is an indication of community trust and prevents new images/recent changes lists from getting swamped.\n",
    "# It's also an indication of community trust; confirms edits not likely to need manual checking\n",
    "\n",
    "# ----------------\n",
    "# Module imports\n",
    "# ----------------\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "import requests\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import sys\n",
    "import re # regex. Function to check for the particular form of xsd:dateTime required for full dates in Wikidata\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import webbrowser\n",
    "\n",
    "# AWS Python SDK\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Utility functions\n",
    "# ------------------------\n",
    "\n",
    "def read_dict(filename):\n",
    "    \"\"\"Read from a CSV file into a list of dictionaries.\"\"\"\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    \"\"\"Write a list of dictionaries to a CSV file.\"\"\"\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "def validate_iso8601(str_val):\n",
    "    \"\"\"Check a string to determine if it is a valid ISO 8601 dateTime value.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    See https://stackoverflow.com/questions/41129921/validate-an-iso-8601-datetime-string-in-python\n",
    "    \"\"\"\n",
    "    regex = r'^(-?(?:[1-9][0-9]*)?[0-9]{4})-(1[0-2]|0[0-9])-(3[01]|0[0-9]|[12][0-9])T([0][0]):([0][0]):([0][0])(Z)$'\n",
    "    match_iso8601 = re.compile(regex).match\n",
    "\n",
    "    try:            \n",
    "        if match_iso8601(str_val) is not None:\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def validate_time(date_text):\n",
    "    \"\"\"Check for valid abbreviated dates.\"\"\"\n",
    "    try:\n",
    "        if date_text != datetime.strptime(date_text, \"%Y-%m-%d\").strftime(\"%Y-%m-%d\"):\n",
    "            raise ValueError\n",
    "        form = 'day'\n",
    "    except ValueError:\n",
    "        try:\n",
    "            if date_text != datetime.strptime(date_text, \"%Y-%m\").strftime('%Y-%m'):\n",
    "                raise ValueError\n",
    "            form = 'month'\n",
    "        except ValueError:\n",
    "            try:\n",
    "                if date_text != datetime.strptime(date_text, \"%Y\").strftime('%Y'):\n",
    "                    raise ValueError\n",
    "                form = 'year'\n",
    "            except ValueError:\n",
    "                form ='none'\n",
    "    return form\n",
    "\n",
    "def convert_dates(time_string):\n",
    "    \"\"\"Convert times to the format required by Wikidata. Does not include the non-standard leading + .\"\"\"\n",
    "    error = False\n",
    "\n",
    "    value = time_string\n",
    "    date_type = validate_time(value)\n",
    "    # date is YYYY-MM-DD\n",
    "    if date_type == 'day':\n",
    "        time_string = value + 'T00:00:00Z'\n",
    "        precision_number = 11 # precision to days\n",
    "    # date is YYYY-MM\n",
    "    elif date_type == 'month':\n",
    "        time_string = value + '-00T00:00:00Z'\n",
    "        precision_number = 10 # precision to months\n",
    "    # date is YYYY\n",
    "    elif date_type == 'year':\n",
    "        time_string = value + '-00-00T00:00:00Z'\n",
    "        precision_number = 9 # precision to years\n",
    "    # date does not conform to any of the tested options\n",
    "    else:\n",
    "        # date is xsd:dateTime and doesn't need adjustment\n",
    "        if validate_iso8601(value):\n",
    "            time_string = value\n",
    "            precision_number = 11 # assume precision to days since Wikibase doesn't support greater resolution than that\n",
    "        # date form unknown, don't adjust\n",
    "        else:\n",
    "            print('Warning: date', time_string, 'does not conform to any standard format! Check manually.')\n",
    "            error = True\n",
    "            precision_number = ''\n",
    "\n",
    "    return time_string, precision_number, error\n",
    "\n",
    "# ------------------------\n",
    "# Commons identifier/URL conversion functions\n",
    "# ------------------------\n",
    "\n",
    "# There are four identifiers used in Commons:\n",
    "\n",
    "# The most basic one is the filename, unencoded and with file extension.\n",
    "\n",
    "# The Commons web page URL is formed from the filename by prepending a subpath and \"File:\", replacing spaces in the filename with _, and URL-encoding the file name string\n",
    "# The reverse process may be lossy because it assumes that underscores should be turned into spaces and the filename might actuall contain underscores.\n",
    "\n",
    "# The Wikidata IRI identifier for the image is formed from the filename by URL-encoding it and prepending a subpath and \"Special:FilePath/\"\n",
    "# It the reverse process is lossless since it simply reverse URL-encodes the local name part of the IRI.\n",
    "\n",
    "# Each media page is also identified by an M ID, which is the Commons equivalent of a Q ID. Since structured\n",
    "# data on Commons is based on a Wikibase instance, the M ID is used when writing structured data to the API.\n",
    "\n",
    "def commons_url_to_filename(url):\n",
    "    \"\"\"Convert a Wikidata IRI identifier to an unencoded file name.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    The form of the URL is: http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\n",
    "    \"\"\"\n",
    "    string = url.split(commons_prefix)[1] # get local name file part of URL\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_url(filename):\n",
    "    \"\"\"Convert a raw file name to a Wikidata IRI identifier.\"\"\"\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_prefix + encoded_filename\n",
    "    return url\n",
    "\n",
    "def commons_page_url_to_filename(url):\n",
    "    \"\"\"Convert a Commons web page URL to a raw file name.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    The form of the URL is: https://commons.wikimedia.org/wiki/File:Castle_De_Haar_(1892-1913)_-_360%C2%B0_Panorama_of_Castle_%26_Castle_Grounds.jpg\n",
    "    \"\"\"\n",
    "    string = url.split(commons_page_prefix)[1] # get local name file part of URL\n",
    "    string = string.replace('_', ' ')\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_page_url(filename):\n",
    "    \"\"\"Convert a raw file name to a Commons web page URL.\"\"\"\n",
    "    filename = filename.replace(' ', '_')\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_page_prefix + encoded_filename\n",
    "    url = url.replace('%28', '(').replace('%29', ')').replace('%2C', ',')\n",
    "    return url\n",
    "\n",
    "def get_commons_image_pageid(image_filename):\n",
    "    \"\"\"Look up the Commons image page ID (\"M ID\") using the image file name.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    The wbeditentity_upload function (which writes to a Wikibase API) needs the M ID, \n",
    "    the structured data on Commons equivalent of a Q ID. \n",
    "    \"\"\"\n",
    "    # get metadata for a photo including from file page\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'File:' + image_filename,\n",
    "        'prop': 'info'\n",
    "    }\n",
    "\n",
    "    response = requests.get('https://commons.wikimedia.org/w/api.php', params=params)\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    page_dict = data['query']['pages'] # this value is a dict that has the page IDs as keys\n",
    "    page_id_list = list(page_dict.keys()) # the result of the .keys() method is a \"dict_keys\" object, so coerce to a list\n",
    "    page_id = page_id_list[0] # info on only one page was requested, so get item 0\n",
    "    #print('Page ID:',page_id)\n",
    "    \n",
    "    # Don't think I need to add a sleep time for API reads, which are less resource-intensive\n",
    "    # than write operations. Also, only single requests are being made between operations that are time-consuming.\n",
    "    # NOTE: appears to return '-1' when it can't find the page.\n",
    "    return page_id\n",
    "\n",
    "# ------------------------\n",
    "# Login in/authentication object\n",
    "# ------------------------\n",
    "\n",
    "class Wikimedia_api_login:\n",
    "    \"\"\"Log in to a Wikimedia API to instantiate a Requests session and generate a CSRF token.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to credentials file (including filename) relative to either working or home directory.\n",
    "        Defaults to \"commons_credentials.txt\".\n",
    "    relative_to_home : bool\n",
    "        True if path is relative to the home directory, False if relative to working directory.\n",
    "        Defaults to True.\n",
    "    \n",
    "    Required modules\n",
    "    ----------------\n",
    "    requests, Path object from pathlib\n",
    "    \"\"\"\n",
    "    def __init__(self, config_values, path='commons_credentials.txt', relative_to_home=True):\n",
    "        if relative_to_home:\n",
    "            home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "            full_credentials_path = home + '/' + path\n",
    "        else:\n",
    "            full_credentials_path = path\n",
    "        \n",
    "        # Retrieve credentials from local file.\n",
    "        with open(full_credentials_path, 'rt') as file_object:\n",
    "            line_list = file_object.read().split('\\n')\n",
    "        root_url = line_list[0].split('=')[1]\n",
    "        username = line_list[1].split('=')[1]\n",
    "        password = line_list[2].split('=')[1]\n",
    "\n",
    "        resource_url = '/w/api.php' # default API resource URL for all Wikimedia APIs\n",
    "        endpoint_url = root_url + resource_url\n",
    "        self.endpoint = endpoint_url\n",
    "\n",
    "        # Instantiate a Requests session\n",
    "        session = requests.Session()\n",
    "        # Set default User-Agent header so you don't have to send it with every request\n",
    "        user_agent_string = config_values['user_agent_string_template'].replace('%s', script_version)\n",
    "        #print(user_agent_string)\n",
    "        session.headers.update({'User-Agent': user_agent_string})\n",
    "        self.session = session\n",
    "\n",
    "        # Go through the sequence of steps needed to get get the CSRF token\n",
    "\n",
    "        # Get the login token\n",
    "        parameters = {\n",
    "            'action':'query',\n",
    "            'meta':'tokens',\n",
    "            'type':'login',\n",
    "            'format':'json'\n",
    "        }\n",
    "        r = session.get(url=endpoint_url, params=parameters)\n",
    "        data = r.json()\n",
    "        login_token = data['query']['tokens']['logintoken']\n",
    "        \n",
    "        # Perform the session login\n",
    "        parameters = {\n",
    "            'action':'login',\n",
    "            'lgname':username,\n",
    "            'lgpassword':password,\n",
    "            'lgtoken':login_token,\n",
    "            'format':'json'\n",
    "        }\n",
    "        r = session.post(endpoint_url, data=parameters)\n",
    "\n",
    "        # Generate the CSRF token\n",
    "        parameters = {\n",
    "            \"action\": \"query\",\n",
    "            \"meta\": \"tokens\",\n",
    "            \"format\": \"json\"\n",
    "        }\n",
    "        r = session.get(url=endpoint_url, params=parameters)\n",
    "        data = r.json()\n",
    "        self.csrftoken = data['query']['tokens']['csrftoken']\n",
    "\n",
    "# ------------------------\n",
    "# Data upload functions\n",
    "# ------------------------\n",
    "\n",
    "def create_commons_template(n_dimensions, artwork_license_text, photo_license_text, category_strings, templated_institution):\n",
    "    \"\"\"Creates initial file Wikitext. Template metadata omitted since page tables will be populated using structured\n",
    "    data from SDC or Wikidata.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    template_type : string\n",
    "        Name of page template to use. Options are: \"Artwork\", \"Art Photo\", \"Information\", or \"Book\".\n",
    "    category_strings : list of strings\n",
    "        Category names to be assigned to the media item.\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    Currently supports only Artwork (2D) and Art Photo (3D) templates but could potentially include Book and Information\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: on 2022-08-12, the info on visual artwork structured data:\n",
    "    # https://commons.wikimedia.org/wiki/Commons:Structured_data/Modeling/Visual_artworks\n",
    "    # says the license template should match the structured data copyright and license statement.\n",
    "    # That makes sense for 3D works, where the Wikitext license is about the media file (image).\n",
    "    # Not so much for 2D digital surrogates, where the Wikitext license can tell about the underlying subject\n",
    "    # as a reason for why the image is in the public domain. In the examples, the copyright and licensing is \n",
    "    # actually omitted in the structured data, I guess as unnecessary despite the guidelines.\n",
    "    if n_dimensions == '2D':\n",
    "        page_wikitext = '''\n",
    "=={{int:filedesc}}==\n",
    "{{Artwork\n",
    "'''\n",
    "        # Inserts GLAM institution's Institution: template if there is one. Supposed to cause the institution's\n",
    "        # infobox to be transcluded into the media item's page, but doesn't seem to work as of 2022-08-21.\n",
    "        if templated_institution != '':\n",
    "            page_wikitext += ''' |institution = {{Institution:''' + templated_institution + '''}}\n",
    "'''\n",
    "        page_wikitext += '''}}\n",
    "\n",
    "=={{int:license-header}}==\n",
    "{{''' + artwork_license_text + '''}}\n",
    "\n",
    "'''\n",
    "    elif n_dimensions == '3D':\n",
    "        # See https://commons.wikimedia.org/wiki/Commons:Structured_data/Modeling/Copyright\n",
    "        # and example given at https://commons.wikimedia.org/w/index.php?title=File:Dionysos_mask_Louvre_Myr347.jpg&action=edit\n",
    "        page_wikitext = '''\n",
    "=={{int:filedesc}}==\n",
    "{{Art Photo\n",
    "'''\n",
    "        if templated_institution != '':\n",
    "            page_wikitext += ''' |institution = {{Institution:''' + templated_institution + '''}}\n",
    "'''\n",
    "        page_wikitext += ''' |artwork license  = {{''' + artwork_license_text + '''}}\n",
    " |photo license    = {{''' + photo_license_text + '''}}\n",
    "}}\n",
    "\n",
    "'''\n",
    "    # Add all of the categories in the list\n",
    "    for category_string in category_strings:\n",
    "        page_wikitext += '[[Category:' + category_string + ''']]\n",
    "'''\n",
    "    \n",
    "    return page_wikitext\n",
    "\n",
    "def upload_file_to_commons(image_filename, commons_filename, directory_path, relative_to_home, commons_login, wikitext):\n",
    "    \"\"\"Upload local image file and page wikitext to Commons via API.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_filename : str\n",
    "        Local file name of image to be uploaded. In the Vanderbilt Fine Arts Gallery, it contains \n",
    "        the accession number.\n",
    "    commons_filename : str\n",
    "        Name of file in Commons. Constructed from label, description, and local file name. Becaues the\n",
    "        local filename includes the accession number, uniqueness is nearly guaranteed.\n",
    "    directory_path : str\n",
    "        The path to the local image file. May be absolute or relative to the working or home directory.\n",
    "        Includes trailing slash.\n",
    "    relative_to_home : bool\n",
    "        True if directory_path is relative to the home directory. False if an absolute path or relative\n",
    "        to the working directory.\n",
    "    commons_login : Wikimedia_api_login object\n",
    "        Needed to supply the session and csrftoken attributes needed for authentication.\n",
    "    wikitext : str\n",
    "        For new files, the wikitext to create the initial image page. For subsequent uploads, the description\n",
    "        text for the new version (apparently does not affect the page wikitext).\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    API file Upload example: https://www.mediawiki.org/wiki/API:Upload#POST_request\n",
    "    API Sandbox can be used to generate test JSON, but DO NOT RUN since it actually uploads.\n",
    "    Specifically for uploads, see https://commons.wikimedia.org/wiki/Special:ApiSandbox#action=upload&filename=Wiki.png&url=http%3A//upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png&token=123ABC\n",
    "    \"\"\"\n",
    "    if relative_to_home:\n",
    "        home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "        directory_path = home + '/' + directory_path\n",
    "\n",
    "    parameters = {\n",
    "        'action': 'upload',\n",
    "        'filename': commons_filename,\n",
    "        'format': 'json',\n",
    "        'token': commons_login.csrftoken,\n",
    "        'ignorewarnings': 1,\n",
    "        'text': wikitext,\n",
    "        # this is what generates the text in the Description box on user Uploads page and initial edit summary for page\n",
    "        # See https://commons.wikimedia.org/wiki/Commons:First_steps/Quality_and_description#Upload_summary\n",
    "        'comment': 'Uploaded media file and metadata via API'\n",
    "    }\n",
    "    file_path = directory_path + image_filename\n",
    "    file_dict = {'file':(image_filename, open(file_path, 'rb'), 'multipart/form-data')}\n",
    "    #print(parameters)\n",
    "    #print(file_dict)\n",
    "\n",
    "    print('uploading', commons_filename) # This line is important for large TIFF files that will take a while to upload\n",
    "    response = commons_login.session.post('https://commons.wikimedia.org/w/api.php', files=file_dict, data = parameters)\n",
    "    # Trap for errors. Note: as far as I can tell, no sort of error code or HTTP header gets sent identifying the \n",
    "    # cause of the error. So at this point, just report an error by returning an empty dictionary.\n",
    "    #print(response.text)\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except:\n",
    "        data = {}\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    return(data)\n",
    "\n",
    "def wbeditentity_upload(commons_login, maxlag, mid, caption, caption_language, sdc_claims_list):\n",
    "    \"\"\"Wikibase edit entity function. Uploads both caption and all Structured Data statements at once.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    commons_session : requests.Session object\n",
    "        Requests session created in the login() function\n",
    "    commons_csrf_token : \n",
    "        CSRF authorization token generated for the session and passed to the API to authenticate\n",
    "    maxlag : integer\n",
    "        number of seconds value used to prevent writing to quickly to the API\n",
    "    mid : string\n",
    "        M ID identifier used to denote media files in Commons\n",
    "    caption : string\n",
    "        Caption for media item, is a Wikibase label\n",
    "    caption_language : string\n",
    "        language code for the language of the caption\n",
    "    sdc_claims_list : list of dictionaries\n",
    "        list of propert:value pairs used to create Structured Data in Commons claims\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Code hacked from VanderBot https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/vanderbot.py\n",
    "    \"\"\"\n",
    "    # Set up the parameter JSON object that will be passed to the API\n",
    "    parameter_dictionary = {\n",
    "        'action': 'wbeditentity',\n",
    "        'format':'json',\n",
    "        'token': commons_login.csrftoken,\n",
    "        'id': mid, # use id key instead of new since it already exists\n",
    "        'summary': 'Add caption and structured data via API'\n",
    "        }\n",
    "\n",
    "    # This structure will be encoded as JSON, then used as the value of a \"data\" name in the parameter object\n",
    "    # First create the labels part\n",
    "    data_structure = {\n",
    "        'labels': {\n",
    "            caption_language: {\n",
    "                'language': caption_language,\n",
    "                'value': caption\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Now create a JSON array of the claims (Structured data statements) to be added.\n",
    "    json_claims_list = []\n",
    "\n",
    "    for claim in sdc_claims_list:\n",
    "        if claim['property'] == 'P571': # Special handling for inception, which must be formatted as a date and not a Q ID\n",
    "            time_string, precision_number, error = convert_dates(claim['value'])\n",
    "            snak_dict = {\n",
    "                'mainsnak': {\n",
    "                    'snaktype': 'value',\n",
    "                    'property': claim['property'],\n",
    "                    'datavalue':{\n",
    "                        'value': {\n",
    "                            'time': '+' + time_string, # Note Wikibase requires non-standard leading + sign\n",
    "                            'timezone': 0,\n",
    "                            'before': 0,\n",
    "                            'after': 0,\n",
    "                            'precision': precision_number,\n",
    "                            'calendarmodel': \"http://www.wikidata.org/entity/Q1985727\"\n",
    "                            },\n",
    "                        'type': 'time'\n",
    "                        },\n",
    "                    'datatype': 'time'\n",
    "                    },\n",
    "                'type': 'statement',\n",
    "                'rank': 'normal'\n",
    "                }\n",
    "\n",
    "        else: # Properties with Q ID values\n",
    "            snak_dict = {\n",
    "                'mainsnak': {\n",
    "                    'snaktype': 'value',\n",
    "                    'property': claim['property'],\n",
    "                    'datatype': 'wikibase-item',\n",
    "                    'datavalue': {\n",
    "                        'value': {\n",
    "                            'id': claim['value']\n",
    "                            },\n",
    "                        'type': 'wikibase-entityid'\n",
    "                        }\n",
    "                    },\n",
    "                'type': 'statement',\n",
    "                'rank': 'normal'\n",
    "                }\n",
    "        json_claims_list.append(snak_dict)\n",
    "\n",
    "    # Now add the array of claims to the data structure\n",
    "    data_structure['claims'] = json_claims_list\n",
    "\n",
    "    #print(json.dumps(data_structure, indent = 2))\n",
    "    #print()\n",
    "\n",
    "    # Confusingly, the data structure has to be encoded as a JSON string before adding as a value of the data name \n",
    "    # in the parameter object, which will itself be encoded as JSON before passing to the API by the requests module.\n",
    "    parameter_dictionary['data'] = json.dumps(data_structure)\n",
    "\n",
    "    # Support maxlag if the API is too busy\n",
    "    if maxlag > 0:\n",
    "        parameter_dictionary['maxlag'] = maxlag\n",
    "\n",
    "    #print(json.dumps(parameter_dictionary, indent = 2))\n",
    "\n",
    "    response = attempt_post('https://commons.wikimedia.org/w/api.php', parameter_dictionary, commons_login.session)\n",
    "    # Old command that does not respect maxlag:\n",
    "    #response = commons_session.post('https://commons.wikimedia.org/w/api.php', data = parameter_dictionary)\n",
    "    return response\n",
    "\n",
    "\n",
    "# This function attempts to post and handles maxlag errors\n",
    "# Code reused from VanderBot https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/vanderbot.py\n",
    "def attempt_post(api_url, parameters, session):\n",
    "    \"\"\"Post to a Wikimedia API while respecting maxlag errors.\"\"\"\n",
    "    starting_delay = 5\n",
    "    max_retries = 10\n",
    "    delay_limit = 300\n",
    "    retry = 0\n",
    "    # maximum number of times to retry lagged server = maxRetries\n",
    "    while retry <= max_retries:\n",
    "        if retry > 0:\n",
    "            print('retry:', retry)\n",
    "        r = session.post(api_url, data = parameters)\n",
    "        #print(r.text)\n",
    "        data = r.json()\n",
    "        try:\n",
    "            # check if response is a maxlag error\n",
    "            # see https://www.mediawiki.org/wiki/Manual:Maxlag_parameter\n",
    "            if data['error']['code'] == 'maxlag':\n",
    "                print('Lag of ', data['error']['lag'], ' seconds.')\n",
    "                # recommended delay is basically useless\n",
    "                # recommended_delay = int(r.headers['Retry-After'])\n",
    "                #if recommended_delay < 5:\n",
    "                    # recommendation is to wait at least 5 seconds if server is lagged\n",
    "                #    recommended_delay = 5\n",
    "                recommended_delay = starting_delay*2**retry # double the delay with each retry \n",
    "                if recommended_delay > delay_limit:\n",
    "                    recommended_delay = delay_limit\n",
    "                if retry != max_retries:\n",
    "                    print('Waiting ', recommended_delay , ' seconds.')\n",
    "                    print()\n",
    "                    sleep(recommended_delay)\n",
    "                retry += 1\n",
    "\n",
    "                # after this, go out of if and try code blocks\n",
    "            else:\n",
    "                # an error code is returned, but it's not maxlag\n",
    "                return data\n",
    "        except:\n",
    "            # if the response doesn't have an error key, it was successful, so return\n",
    "            return data\n",
    "        # here's where execution goes after the delay\n",
    "    # here's where execution goes after maxRetries tries\n",
    "    print('Failed after ' + str(max_retries) + ' retries.')\n",
    "    exit() # just abort the script\n",
    "    \n",
    "# ---------------------------\n",
    "# Major processes functions\n",
    "# ---------------------------\n",
    "\n",
    "def commons_image_upload(image_metadata, config_values, commons_login):\n",
    "    \"\"\"Construct labels, templates, and paths necessary to upload a media file to Commons, then upload.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_metadata : dict\n",
    "        Metadata about a particular image to be uploaded.\n",
    "    config_values : dict\n",
    "        Global configuration values.\n",
    "    commons_login : Wikimedia_api_login object\n",
    "        Needed to supply the session and csrftoken attributes needed for authentication during upload.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the page wikitext based on licensing metadata appropriate for the kind of artwork.\n",
    "    page_wikitext = create_commons_template(image_metadata['n_dimensions'], image_metadata['artwork_license_text'], image_metadata['photo_license_text'], image_metadata['category_strings'], config_values['templated_institution'])\n",
    "    #print(page_wikitext)\n",
    "\n",
    "    # The local_filename is the name of the file as it exists locally.\n",
    "    # NOTE: if the image filename contains a space, it will generate an error when the IIIF manifest link is uploaded\n",
    "    # to the Wikidata API. It's better if the images don't have spaces, so the script will just skip over it and \n",
    "    # flag the image to have its name changed manually, rather than automatically changing spaces to underscores \n",
    "    # (potentially causing a naming collision).\n",
    "    if ' ' in image_metadata['local_filename']:\n",
    "        print('Raw filename \"' + image_metadata['local_filename'] + '\" for ' + work_qid + ' contains spaces that need to be removed manually.')\n",
    "        print('Unallowed spaces in raw filename \"' + image_metadata['local_filename'] + '\" for ' + work_qid, file=log_object)\n",
    "        errors = True\n",
    "        return errors, ''\n",
    "    else:\n",
    "        local_filename = image_metadata['local_filename']\n",
    "\n",
    "    # subdirectory is the directory that contains the local file. It's within the local_image_directory_path. \n",
    "    # Don't include a trailing slash.\n",
    "    # If images are directly in the directory_path, use empty string ('') as the value.\n",
    "    subdirectory = image_metadata['subdir']\n",
    "\n",
    "    label = image_metadata['label']\n",
    "    # NOTE: square brackets [] and colons : are not allowed in the filenames. So replace them if they exist\n",
    "    if '[' in label:\n",
    "        clean_label = label.replace('[', '(')\n",
    "    else:\n",
    "        clean_label = label\n",
    "    if ']' in clean_label:\n",
    "        clean_label = clean_label.replace(']', ')')\n",
    "    if ':' in clean_label:\n",
    "        clean_label = clean_label.replace(':', '-')\n",
    "    # Get rid of double spaces. The API will automatically replace them with single spaces, preventing a match with\n",
    "    # the recorded filename and the filename in the returned value from the Wikidata API. Loop should get rid of\n",
    "    # triple spaces or more.\n",
    "    while '  ' in clean_label:\n",
    "        clean_label = clean_label.replace('  ', ' ')\n",
    "        \n",
    "    filename_institution = image_metadata['filename_institution']\n",
    "    \n",
    "    # file_prefix is descriptive text to be prepended to the local_filename, to be used when the file is in Commons\n",
    "    # Commons filename length limit is 240 bytes. To be safe, limit to 230.\n",
    "    byte_limit = 230 - len((' - ' + filename_institution + ' - ' + local_filename).encode(\"utf8\"))\n",
    "    if len(clean_label.encode(\"utf8\")) < byte_limit:\n",
    "        file_prefix = clean_label\n",
    "    else:\n",
    "        file_prefix = clean_label.encode(\"utf8\")[:byte_limit].decode('utf8')\n",
    "\n",
    "    # Set commons_filename (can include spaces). The API will substitute underscores as it likes.\n",
    "    # For file naming conventions, see: https://commons.wikimedia.org/wiki/Commons:File_naming\n",
    "\n",
    "    commons_filename = file_prefix + ' - ' + filename_institution + ' - ' + local_filename\n",
    "\n",
    "    # When I tried uploading the pyramidal TIFFs to Commons, I got a \n",
    "    # \"The uploaded file contains errors: inconsistent page numbering in TIFF directory\"\n",
    "    # error. So the Commons upload should be done with the raw TIFF files, whose locations are designated by \n",
    "    # the full_path variable.\n",
    "    \n",
    "    # Add the subdirectory (if any) to the path\n",
    "    if subdirectory != '':\n",
    "        full_path = config_values['local_image_directory_path'] + subdirectory + '/'\n",
    "    else:\n",
    "        full_path = config_values['local_image_directory_path']\n",
    "    \n",
    "    data = upload_file_to_commons(local_filename, commons_filename, full_path, config_values['path_is_relative_to_home_directory'], commons_login, page_wikitext)\n",
    "    #data = {'upload': {'result': 'Success'}} # Uncomment this line to test without actually doing the upload\n",
    "    \n",
    "    errors = False\n",
    "    if data == {}: # Handle an error\n",
    "        print('Failed to upload successfully.')\n",
    "        print('Commons file upload failed with non-JSON response for ' + work_qid, file=log_object)\n",
    "        errors = True\n",
    "    else:\n",
    "        #print(json.dumps(data, indent=2))\n",
    "        try:\n",
    "            print('API response:', data['upload']['result'])\n",
    "        except:\n",
    "            print('API did not respond with \"Success\"')\n",
    "            print('Commons file upload failed with non-\"Success\" response for ' + work_qid, file=log_object)\n",
    "            errors = True\n",
    "\n",
    "    return errors, commons_filename\n",
    "\n",
    "def structured_data_upload(image_metadata, config_values, commons_login):\n",
    "    \"\"\"Assemble medata for and upload Commons structured data using the Wikibase API wbeditentity method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_metadata : dict\n",
    "        Metadata about a particular image to be uploaded.\n",
    "    config_values : dict\n",
    "        Global configuration values.\n",
    "    commons_login : Wikimedia_api_login object\n",
    "        Needed to supply the session and csrftoken attributes needed for authentication during upload.\n",
    "    \"\"\"\n",
    "    # Intro on structured data: https://commons.wikimedia.org/wiki/Commons:Structured_data\n",
    "    # See also this on GLAM https://commons.wikimedia.org/wiki/Commons:Structured_data/GLAM\n",
    "    \n",
    "    errors = False\n",
    "    work_qid = image_metadata['work_qid']\n",
    "\n",
    "    # NOTE: 2D artworks will get flagged if they don't have P6243 in their structured data\n",
    "    # non-public domain works get flagged if they don't have a P275 license statement in structured data\n",
    "\n",
    "    # Define claims for structured data in Commons\n",
    "    # See https://commons.wikimedia.org/wiki/Commons:Structured_data/Modeling/Visual_artworks \n",
    "    sdc_claims_list = [\n",
    "        {'property': 'P180', 'value': work_qid}, # depicts artwork in Wikidata\n",
    "        {'property': 'P921', 'value': work_qid}, # main subject is artwork in Wikidata\n",
    "        {'property': 'P170', 'value': image_metadata['photographer_of_work']} # creator of image file\n",
    "    ]\n",
    "    if image_metadata['n_dimensions'] == '2D':\n",
    "        sdc_claims_list.append({'property': 'P6243', 'value': work_qid}) # digital representaion of artwork in Wikidata\n",
    "    else:\n",
    "        # Despite what is said on 2022-08-13 at https://commons.wikimedia.org/wiki/Commons:Structured_data/Modeling/Visual_artworks#Basic_structured_data_(SDC)\n",
    "        # the license templates are mostly about the artwork and are really a justification for omitting \n",
    "        # inclusion of copyright information about the image. The examples don't include it. So there isn't\n",
    "        # really any point in including it in the structured data. See the exemplar at:\n",
    "        # https://commons.wikimedia.org/wiki/File:Clara_Peeters_-_Still_Life_with_Cheeses,_Almonds_and_Pretzels_-_1203_-_Mauritshuis.jpg\n",
    "        sdc_claims_list.append({'property': 'P6216', 'value': image_metadata['photo_copyright_qid']}) # copyright status of image file\n",
    "        sdc_claims_list.append({'property': 'P275', 'value': image_metadata['photo_license_qid']}) # license for image file\n",
    "\n",
    "    if image_metadata['photo_inception'] != '':\n",
    "        sdc_claims_list.append({'property': 'P571', 'value': image_metadata['photo_inception']}) # creation date of image file\n",
    "\n",
    "    # caption and caption_language used in structured data upload\n",
    "    caption = image_metadata['label'] + ', ' + image_metadata['wikidata_description']\n",
    "    \n",
    "    get_id_response = get_commons_image_pageid(image_metadata['commons_filename'])\n",
    "    if get_id_response == '-1': # returns an error\n",
    "        mid = 'error'\n",
    "        print('Could not find Commons page ID. Will not upload structured data!')\n",
    "        print('Could not find Commons page ID for ' + work_qid + ': ' + image_metadata['commons_filename'], file=log_object)\n",
    "        errors = True\n",
    "    else:\n",
    "        mid = \"M\" + get_id_response\n",
    "\n",
    "        print('Uploading structured data')\n",
    "        # Note: the structured data upload respects and processes maxlag, so no hard-coded delay is included here\n",
    "        response = wbeditentity_upload(commons_login, config_values['maxlag'], mid, caption, config_values['default_language'], sdc_claims_list)\n",
    "        #response = {'success': 1} # Uncomment this line to test without actually doing the upload\n",
    "        #print(json.dumps(response, indent=2))\n",
    "        try:\n",
    "            if response['success'] == 1:\n",
    "                print('API reports success')\n",
    "            else:\n",
    "                print('API reports failure')\n",
    "                print('API reports failure of structured data upload for ' + work_qid + ': ' + commons_filename, file=log_object)\n",
    "                errors = True\n",
    "        except:\n",
    "            print('API did not respond with \"Success\"')\n",
    "            print('Structured data upload failed with no \"Success\" response for ' + work_qid + ': ' + commons_filename, file=log_object)\n",
    "            errors = True\n",
    "            \n",
    "    if not errors:\n",
    "        print(filename_to_commons_page_url(image_metadata['commons_filename']))\n",
    "        if config_values['open_browser_tab_after_upload']:\n",
    "            success = webbrowser.open_new_tab(filename_to_commons_page_url(image_metadata['commons_filename']))\n",
    "\n",
    "    return errors, mid\n",
    "\n",
    "def upload_image_to_iiif(image_metadata, config_values):\n",
    "    \"\"\"Upload image to IIIF server S3 bucket.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_metadata : dict\n",
    "        Metadata about a particular image to be uploaded.\n",
    "    config_values : dict\n",
    "        Global configuration values.\n",
    "    \"\"\"\n",
    "    print('in IIIF S3 upload function')\n",
    "    local_filename = image_metadata['local_filename']\n",
    "    \n",
    "    # Code to allow omission of subdirectory in path\n",
    "    if image_metadata['subdir'] == '':\n",
    "        subdirectory = ''\n",
    "        subdirectory_escaped = ''\n",
    "    else:\n",
    "        subdirectory = image_metadata['subdir'] + '/'\n",
    "        subdirectory_escaped = image_metadata['subdir'] + '%2F'\n",
    "    \n",
    "    # This code substitutes the TIFF images that have been converted to pyramidal tiled versions and are in a \n",
    "    # different directory from the original, unconverted images.\n",
    "    tiff_extensions = ['tif', 'TIF', 'tiff', 'TIFF']\n",
    "    file_extension = local_filename.split('.')[-1]\n",
    "    if file_extension in tiff_extensions:\n",
    "        image_directory_path = config_values['tiff_image_directory_path']\n",
    "    else:\n",
    "        image_directory_path = config_values['local_image_directory_path']\n",
    "\n",
    "    if config_values['path_is_relative_to_home_directory']:\n",
    "        home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "        local_file_path = home + '/' + image_directory_path + subdirectory + local_filename\n",
    "    else:\n",
    "        local_file_path = image_directory_path + subdirectory + local_filename\n",
    "\n",
    "    # See https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3.html#uploads\n",
    "    s3_iiif_key = config_values['s3_iiif_project_directory'] + '/' + subdirectory + local_filename\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "    print('Uploading to s3:', local_filename)\n",
    "    s3.upload_file(local_file_path, config_values['s3_iiif_bucket_name'], s3_iiif_key)\n",
    "\n",
    "    # For the image in the \"iiif-library-cantaloupe-storage\" bucket with the key \"gallery/1979/1979.0264P.tif\"\n",
    "    # the IIIF URL would be https://iiif.library.vanderbilt.edu/iiif/3/gallery%2F1979%2F1979.0264P.tif/full/max/0/default.jpg\n",
    "    print(config_values['iiif_server_url_root'] + config_values['s3_iiif_project_directory'] + '%2F' + subdirectory_escaped + local_filename + '/full/1000,/0/default.jpg')\n",
    "    \n",
    "def upload_iiif_manifest_to_s3(image_metadata, config_values):\n",
    "    \"\"\"Generate and write IIIF manifest to S3 bucket.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image_metadata : dict\n",
    "        Metadata about a particular image to be uploaded.\n",
    "    config_values : dict\n",
    "        Global configuration values.\n",
    "    \"\"\"\n",
    "    local_filename = image_metadata['local_filename']\n",
    "    \n",
    "    if config_values['s3_iiif_project_directory'] == '':\n",
    "        s3_iiif_project_directory = ''\n",
    "        s3_iiif_project_directory_escaped = ''\n",
    "    else:\n",
    "        s3_iiif_project_directory = config_values['s3_iiif_project_directory'] + '/'\n",
    "        s3_iiif_project_directory_escaped = config_values['s3_iiif_project_directory'] + '%2F'\n",
    "\n",
    "    if image_metadata['subdir'] == '':\n",
    "        subdirectory = ''\n",
    "        subdirectory_escaped = ''\n",
    "    else:\n",
    "        subdirectory = image_metadata['subdir'] + '/'\n",
    "        subdirectory_escaped = image_metadata['subdir'] + '%2F'\n",
    "\n",
    "    manifest_iri = config_values['manifest_iri_root'] + s3_iiif_project_directory + subdirectory + local_filename + '.json'\n",
    "    label = image_metadata['label']\n",
    "    # Used this manifest as a template: https://www.nga.gov/api/v1/iiif/presentation/manifest.json?cultObj:id=151064\n",
    "\n",
    "    manifest = '''{\n",
    "        \"@context\": \"http://iiif.io/api/presentation/2/context.json\",\n",
    "        \"@id\": \"''' + manifest_iri + '''\",\n",
    "        \"@type\": \"sc:Manifest\",\n",
    "        \"label\": \"''' + label + '''\",\n",
    "        \"description\": \"''' + image_metadata['wikidata_description'] + '''\",\n",
    "    '''\n",
    "\n",
    "    if config_values['iiif_manifest_logo_url'] != '':\n",
    "        manifest += '''         \"logo\": \"''' + config_values['iiif_manifest_logo_url'] + '''\",\n",
    "    '''\n",
    "\n",
    "    manifest += '''        \"attribution\": \"''' + config_values['iiif_manifest_attribution'] + '''\",\n",
    "        \"metadata\": [\n",
    "            {\n",
    "            \"label\": \"Artist\",\n",
    "            \"value\": \"''' + image_metadata['creator_string'] + '''\"\n",
    "            },\n",
    "            {\n",
    "            \"label\": \"Accession Number\",\n",
    "            \"value\": \"''' + image_metadata['inventory_number'] + '''\"\n",
    "            },\n",
    "    '''\n",
    "\n",
    "    if work['inception_val'] != '':\n",
    "        manifest += '''        {\n",
    "            \"label\": \"Creation Year\",\n",
    "            \"value\": \"''' + image_metadata['creation_year'] + '''\"\n",
    "            },\n",
    "    '''\n",
    "\n",
    "    manifest += '''        {\n",
    "            \"label\": \"Title\",\n",
    "            \"value\": \"''' + label + '''\"\n",
    "            }\n",
    "        ],\n",
    "        \"viewingDirection\": \"left-to-right\",\n",
    "        \"viewingHint\": \"individuals\",\n",
    "        \"sequences\": [{\n",
    "            \"@type\": \"sc:Sequence\",\n",
    "            \"label\": \"''' + label + '''\",\n",
    "            \"canvases\": [{\n",
    "                \"@id\": \"''' + manifest_iri + '''#canvas\",\n",
    "                \"@type\": \"sc:Canvas\",\n",
    "                \"width\": ''' + image_metadata['width'] + ''',\n",
    "                \"height\": ''' + image_metadata['height'] + ''',\n",
    "                \"label\": \"''' + label + '''\",\n",
    "                \"images\": [{\n",
    "                    \"@type\": \"oa:Annotation\",\n",
    "                    \"motivation\": \"sc:painting\",\n",
    "                    \"resource\": {\n",
    "                        \"@id\": \"''' + manifest_iri + '''#resource\",\n",
    "                        \"@type\": \"dctypes:Image\",\n",
    "                        \"format\": \"image/jpeg\",\n",
    "                        \"width\": ''' + image_metadata['width'] + ''',\n",
    "                        \"height\": ''' + image_metadata['height'] + ''',\n",
    "                        \"service\": {\n",
    "                            \"@context\": \"http://iiif.io/api/image/2/context.json\",\n",
    "                            \"@id\": \"'''+ config_values['iiif_server_url_root'] + s3_iiif_project_directory_escaped + subdirectory_escaped + local_filename + '''\",\n",
    "                            \"profile\": \"http://iiif.io/api/image/2/level2.json\"\n",
    "                            }\n",
    "                        },\n",
    "                    \"on\": \"''' + manifest_iri + '''#canvas\"\n",
    "                    }],\n",
    "                \"thumbnail\": {\n",
    "                    \"@id\": \"'''+ config_values['iiif_server_url_root'] + s3_iiif_project_directory_escaped + subdirectory_escaped + local_filename + '''/full/!100,100/0/default.jpg\",\n",
    "                    \"@type\": \"dctypes:Image\",\n",
    "                    \"format\": \"image/jpeg\",\n",
    "                    \"width\": 100,\n",
    "                    \"height\": 100\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    s3_manifest_key = s3_iiif_project_directory + subdirectory + local_filename + '.json'\n",
    "    print('Uploading manifest to s3:', local_filename + '.json')\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(config_values['s3_manifest_bucket_name'], s3_manifest_key).put(Body = manifest, ContentType = 'application/json')\n",
    "    iiif_manifest_url = config_values['manifest_iri_root'] + s3_iiif_project_directory + subdirectory + local_filename + '.json'\n",
    "    print(iiif_manifest_url)\n",
    "    return iiif_manifest_url\n",
    "\n",
    "# ---------------------------\n",
    "# Body of main script\n",
    "# ---------------------------\n",
    "\n",
    "# This section contains configuration information and performs necessary logins\n",
    "# No writing is done, so it's \"safe\" to run any time\n",
    "\n",
    "# This section needs to be run prior to running any code that interacts with the Commons API\n",
    "# It generates the CSRF token required to post to the API on behalf of the user whose username and pwd are being used\n",
    "\n",
    "print('Loading data')\n",
    "\n",
    "# Load configuration values\n",
    "with open('commonsbot_config.yml', 'r') as file:\n",
    "    config_values = yaml.safe_load(file)\n",
    "\n",
    "if config_values['working_directory_path'] != '':\n",
    "    # Change working directory to image upload directory\n",
    "    os.chdir(config_values['working_directory_path'])\n",
    "    \n",
    "# Error log should be saved in current working directory\n",
    "# The log_object is a global variable so that it can be accessed in all functions.\n",
    "log_path = 'error_log.txt'\n",
    "log_object = open(log_path, 'wt', encoding='utf-8')\n",
    "errors = False\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Load data from CSVs into DataFrames\n",
    "# ---------------------------\n",
    "\n",
    "# These files are all relative to the current working directory\n",
    "\n",
    "# Note: setting the index to be the Q ID requires that qid has a unique value for each row. This should be the case.\n",
    "works_metadata = pd.read_csv('../works_multiprop.csv', na_filter=False, dtype = str)\n",
    "works_metadata.set_index('qid', inplace=True)\n",
    "\n",
    "raw_metadata = pd.read_csv('../gallery_works_renamed1.csv', na_filter=False, dtype = str)\n",
    "raw_metadata.set_index('accession_number', inplace=True)\n",
    "\n",
    "image_dimensions = pd.read_csv('image_dimensions.csv', na_filter=False, dtype = str)\n",
    "# Convert some columns to integers\n",
    "image_dimensions[['kilobytes', 'height', 'width']] = image_dimensions[['kilobytes', 'height', 'width']].astype(int)\n",
    "\n",
    "works_classification = pd.read_csv('../../gallery_buchanan/works_classification.csv', na_filter=False, dtype = str)\n",
    "works_classification.set_index('qid', inplace=True)\n",
    "\n",
    "works_ip_status = pd.read_csv('../items_status_abbrev.csv', na_filter=False, dtype = str)\n",
    "works_ip_status.set_index('qid', inplace=True)\n",
    "\n",
    "existing_images = pd.read_csv('commons_images.csv', na_filter=False, dtype = str) # Don't make the Q IDs the index!\n",
    "\n",
    "# ---------------------------\n",
    "# Commons API Post Authentication (create session and generate CSRF token)\n",
    "# ---------------------------\n",
    "\n",
    "print('Authenticating')\n",
    "\n",
    "# This is the format of the credentials file. \n",
    "# Username and password are for a bot that you've created.\n",
    "# The file must be plain text. It is recommended to place in your home directory so that you don't accidentally\n",
    "# include it when sharing this script and associated data files. This is the default unless changed when\n",
    "# instantiating a Wikimedia_api_login object.\n",
    "# NOTE: because this script is idiosyncratic to Wikimedia Commons, the endpoint URL is hard-coded. So the\n",
    "# endpointUrl value given in the credentials file is ignored. It is retained for consistency with other \n",
    "# scripts that use credentials like this (e.g. VanderBot).\n",
    "\n",
    "'''\n",
    "endpointUrl=https://test.wikidata.org\n",
    "username=User@bot\n",
    "password=465jli90dslhgoiuhsaoi9s0sj5ki3lo\n",
    "'''\n",
    "\n",
    "# If credentials file location is in a subfolder, include subfolders through file name \n",
    "# with no leading slash and set as the path argument.\n",
    "# Example: myproj/credentials/commons_credentials.txt\n",
    "# [Need to give example for absolute path on Windows - use Unix forward slashes?]\n",
    "# If credentials file is in current working directory or home directory, only filename is necessary.\n",
    "# Omit to use \"commons_credentials.txt\" as the path argument.\n",
    "\n",
    "# If the credentials file location is relative to the working directory or an absolute path, \n",
    "# set the relative_to_home argument to False. Set to True or omit if relative to the home directory.\n",
    "\n",
    "commons_login = Wikimedia_api_login(config_values)\n",
    "\n",
    "\n",
    "print('Beginning uploads')\n",
    "print()\n",
    "\n",
    "items_uploaded = 0\n",
    "commons_upload_sleep_time = 0\n",
    "\n",
    "# The row index is the Q ID and is a string. The work object is the data in the row and is a Pandas series\n",
    "# The items in the row series can be referred to by their labels, which are the column headers, e.g. work['label_en']\n",
    "for index, work in works_metadata.iterrows():\n",
    "    #print(work['label_en'])\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Screen works for appropriate images to upload\n",
    "    # ---------------------------\n",
    "\n",
    "    # Screen out images that are already in Commons\n",
    "    if index in existing_images.qid.values:\n",
    "        continue\n",
    "    \n",
    "    # Screen for 2 dimensional works\n",
    "    #if index in works_classification.index:\n",
    "        # Find the row whose index matches the Q ID of the work, then the item by name within the series (dimension)\n",
    "        # Note: this method of location works because the Q ID index is unique for each row in the lookup table.\n",
    "        #if works_classification.loc[index, 'dimension'] != '2D': # skip this work if not 2D\n",
    "        #    continue\n",
    "\n",
    "    # Screen for public domain images\n",
    "    # NOTE: the IP status was only done for cases where the script was able to match up image file names with accession numbers.\n",
    "    # It should be done again to pick up more images based on the new image_dimensions.csv file after it's cleaned up.\n",
    "    # There are at least a thousand works that will get screened out here because they aren't imaged.\n",
    "    if not index in works_ip_status.index:\n",
    "        continue\n",
    "    else:\n",
    "        ip_status = works_ip_status.loc[index, 'status']\n",
    "        if not ip_status in config_values['public_domain_categories']:\n",
    "            continue\n",
    "        # Handle the special case where the status was determined to be \"assessed to be out of copyright\" but the\n",
    "        # inception date was given as after 1926\n",
    "        if ip_status == 'assessed to be out of copyright':\n",
    "            try:\n",
    "                # convert the year part to an integer; will fail if empty string\n",
    "                # If the date is BCE, it will be a negative integer and it will be processed\n",
    "                inception_date = int(work['inception_val'][:4])\n",
    "                if inception_date > config_values['copyright_cutoff_date']:\n",
    "                    continue  # skip this work if it has an inception date and it's after 1926\n",
    "            except:\n",
    "                pass # if there isn't an inception date, then Kali just determined that the work was really old\n",
    "    \n",
    "    # Screen for high resolution images\n",
    "    image_dimension_frame = image_dimensions.loc[image_dimensions.accession == work['inventory_number']] # result is DataFrame\n",
    "    if len(image_dimension_frame) == 0: # skip any works whose image can't be found in the dimensions data\n",
    "        continue\n",
    "\n",
    "    # Order rows by size in kB, then take the first row\n",
    "    image_dimension_series = image_dimension_frame.sort_values(by=['kilobytes'], ascending=False).iloc[0]\n",
    "    # Skip work if its image doesn't meet the minimum size requirement\n",
    "    if config_values['size_filter'] == 'pixsquared':\n",
    "        if image_dimension_series['height'] * image_dimension_series['width'] < config_values['minimum_pixel_squared']:\n",
    "            print('Image too small.')\n",
    "            print('Inadequate pixel squared for', work['inventory_number'], file=log_object)\n",
    "            errors = True\n",
    "            continue\n",
    "    elif config_values['size_filter'] == 'filesize':\n",
    "        if image_dimension_series['kilobytes'] < config_values['minimum_filesize']:\n",
    "            print('Image too small.')\n",
    "            print('Inadequate file size for', work['inventory_number'], file=log_object)\n",
    "            errors = True\n",
    "            continue\n",
    "    else: # don't apply a size filter\n",
    "        pass\n",
    "    \n",
    "    # Flag possible oversize error. Commons upload interface says limit is 100 MB\n",
    "    if image_dimension_series['kilobytes'] > 102400:\n",
    "        print('Warning: image size exceeds 100 Mb!')\n",
    "        print('Image size exceeds 100 Mb for ' + work['inventory_number'], file=log_object)\n",
    "        errors = True\n",
    "        continue\n",
    "\n",
    "\n",
    "    # ------------\n",
    "    # Set variable values for image metadata \n",
    "    # Uses data from the various input tables previously opened\n",
    "    #-------------\n",
    "    \n",
    "    image_metadata = {}\n",
    "    \n",
    "    image_metadata['photo_inception'] = image_dimension_series['create_date'] # Use any form of yyyy, yyyy-mm, or yyyy-mm-dd\n",
    "\n",
    "    image_metadata['work_qid'] = index\n",
    "    \n",
    "    image_metadata['inventory_number'] = work['inventory_number']\n",
    "    image_metadata['creation_year'] = work['inception_val'][:4]\n",
    "    image_metadata['label'] = work['label_en']\n",
    "    image_metadata['wikidata_description'] = work['description_en']\n",
    "    \n",
    "    # Get raw string data directly from the Artstor download\n",
    "    try:\n",
    "        image_metadata['creator_string'] = raw_metadata.loc[image_metadata['inventory_number']]['creator_string']\n",
    "    except:\n",
    "        print('Failed to load raw metadata!')\n",
    "        print('Failed to load raw metadata for', image_metadata['inventory_number'], file=log_object)\n",
    "        errors = True\n",
    "        continue\n",
    "    \n",
    "    # subdirectory is the directory that contains the local file. It's within the local_image_directory_path. \n",
    "    # Don't include a trailing slash.\n",
    "    # If images are directly in the directory_path, use empty string ('') as the value.\n",
    "    image_metadata['subdir'] = image_dimension_series['subdir']\n",
    "    image_metadata['local_filename'] = image_dimension_series['name']\n",
    "    image_metadata['width'] = str(image_dimension_series['width'])\n",
    "    image_metadata['height'] = str(image_dimension_series['height'])\n",
    "\n",
    "    # ------------\n",
    "    # Set fixed values for image metadata\n",
    "    # These config values are used for all images, but they could at some point be varied image by image.\n",
    "    # ------------\n",
    "    \n",
    "    if works_classification.loc[index, 'dimension'] == '3D':\n",
    "        image_metadata['n_dimensions'] = '3D'\n",
    "        image_metadata['artwork_license_text'] = config_values['artwork_license_text_3d']\n",
    "        image_metadata['photo_license_text'] = config_values['photo_license_text_3d']\n",
    "    else: # appy to 2D but also prints, posters, etc.\n",
    "        image_metadata['n_dimensions'] = '2D'\n",
    "        image_metadata['artwork_license_text'] = config_values['artwork_license_text_2d']\n",
    "        image_metadata['photo_license_text'] = '' # Not used in 2D Wikitext since photo is considered to not involve creativity\n",
    "\n",
    "    image_metadata['photo_copyright_qid'] = config_values['photo_copyright_qid']\n",
    "    image_metadata['photo_license_qid'] = config_values['photo_license_qid']\n",
    "    image_metadata['category_strings'] = config_values['category_strings'] # Commons categories to be added to the image.\n",
    "    image_metadata['filename_institution'] = config_values['filename_institution']\n",
    "    image_metadata['photographer_of_work'] = config_values['photographer_of_work']\n",
    "    \n",
    "    # -----------\n",
    "    # Upload data\n",
    "    # -----------\n",
    "\n",
    "    # Upload the media file to Commons\n",
    "    sleep(commons_upload_sleep_time) # Delay the next media item upload if less than commons_sleep time since the last upload.\n",
    "    upload_error, commons_filename = commons_image_upload(image_metadata, config_values, commons_login)\n",
    "    \n",
    "    # If the media file fails to upload, there is no point in continuing nor to add to the commons_images.csv\n",
    "    # file. Just log the error and go on.\n",
    "    if upload_error:\n",
    "        errors = True\n",
    "        print('Image upload to Commons failed for', work['inventory_number'], file=log_object)\n",
    "        continue\n",
    "    else:\n",
    "        image_metadata['commons_filename'] = commons_filename\n",
    "        \n",
    "    # Begin timing to determine whether enough time has elapsed before doing the next Commons upload\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # Upload structured data for Commons\n",
    "    upload_error, mid = structured_data_upload(image_metadata, config_values, commons_login)\n",
    "    image_metadata['mid'] = mid\n",
    "    \n",
    "    # If the structured data fails to upload, the data from the file upload still needs to be saved.\n",
    "    # So don't continue to the next iteration.\n",
    "    if upload_error:\n",
    "        errors = True\n",
    "        print('Structured data for Commons upload failed for', work['inventory_number'], file=log_object)\n",
    "        image_metadata['iiif_manifest_url'] = ''\n",
    "    else:\n",
    "        # NOTE: the S3 bucket uploads don't seem to ever fail and there isn't an easy way to detect it,\n",
    "        # so the code doesn't really have any error trapping for it.\n",
    "        \n",
    "        # Upload image file to IIIF server S3 bucket\n",
    "        upload_image_to_iiif(image_metadata, config_values)\n",
    "        \n",
    "        # Generate IIIF manifest and upload to S3 bucket\n",
    "        image_metadata['iiif_manifest_url'] = upload_iiif_manifest_to_s3(image_metadata, config_values)\n",
    "\n",
    "    # ----------------\n",
    "    # Add data to record of Commons images\n",
    "    # ----------------\n",
    "\n",
    "    new_image_data = [{\n",
    "        'qid': image_metadata['work_qid'],\n",
    "        'commons_id': image_metadata['mid'],\n",
    "        'accession_number': image_metadata['inventory_number'],\n",
    "        'label_en': image_metadata['label'],\n",
    "        'directory': image_metadata['subdir'],\n",
    "        'local_filename': image_metadata['local_filename'],\n",
    "        'image_name': image_metadata['commons_filename'],\n",
    "        'iiif_manifest': image_metadata['iiif_manifest_url'],\n",
    "        'notes': ''\n",
    "    }]\n",
    "    existing_images = existing_images.append(new_image_data, ignore_index=True, sort=False)\n",
    "    # This output file is used as input by the transfer_to_vanderbot.ipynb script, which adds data to a CSV\n",
    "    # file that can be used to create the statements in Wikidata linking the artwork item to the Commons file.\n",
    "    existing_images.to_csv('commons_images.csv', index = False) # Don't export the numeric index as a column \n",
    "    \n",
    "    print() # Put blank line between uploaded media items\n",
    "    \n",
    "    items_uploaded += 1\n",
    "    \n",
    "    if config_values['max_items_to_upload'] > 0: # Remove limit if zero or negative value.\n",
    "        if items_uploaded >= config_values['max_items_to_upload']:\n",
    "            break\n",
    "            \n",
    "    # Calculate whether the other uploads took enough time that delaying the next Commons media upload is unnecessary.\n",
    "    elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "    commons_upload_sleep_time = config_values['commons_sleep'] - elapsed_time\n",
    "    if commons_upload_sleep_time < 0:\n",
    "        commons_upload_sleep_time = 0\n",
    "        \n",
    "if not errors:\n",
    "    print(items_uploaded, 'items uploaded.')\n",
    "    print('No errors occurred.', file=log_object)\n",
    "log_object.close()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific script to upload Learn Wikidata videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "language_dict = {'en': 'in English', 'es': 'en español', 'zh-Hans': '汉语'}\n",
    "\n",
    "# Category\n",
    "category_strings = ['Wikidata videos', 'Learn Wikidata videos by the Vanderbilt Libraries']\n",
    "\n",
    "# path where image file is located with trailing forward slash\n",
    "# relative to working directory or an absolute path if path_is_relative_to_home_directory = False\n",
    "# relative to home directory if path_is_relative_to_home_directory = True\n",
    "directory_path = 'Documents/video_conversion/webm/'\n",
    "path_is_relative_to_home_directory = True\n",
    "sleeptime = commons_sleep # use zero if running individual cells manually, use commons_sleep if code in single script or looping\n",
    "\n",
    "filename = 'upload_metadata.csv'\n",
    "media_items_list = read_dict(filename)\n",
    "for media_item in media_items_list[12:]:\n",
    "\n",
    "    # Set image_filename to the raw filename (can include spaces). The API will substitute underscores as it likes.\n",
    "    # For file naming conventions, see: https://commons.wikimedia.org/wiki/Commons:File_naming\n",
    "\n",
    "    image_filename = media_item['filename']\n",
    "    print(image_filename)\n",
    "    print()\n",
    "\n",
    "    # The caption has to be set in a separate operation from the file upload\n",
    "    # But it's set here so that its text can be used for the description\n",
    "    # Captions must be less than 255 characters. \n",
    "    # There can be multiple captions in different languages, but only one per language.\n",
    "    caption = media_item['caption']\n",
    "    caption_language = media_item['language'].lower()\n",
    "\n",
    "    # The description doesn't have to be the same as the caption.\n",
    "    # It can be much longer and contain Wiki formatting, such as links. \n",
    "    description = media_item['description']\n",
    "    description_language = media_item['language'].lower()\n",
    "    iso_date = str(pd.to_datetime(media_item['date']))\n",
    "    #print(iso_date)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Upload a local file to Commons and set basic metadata\n",
    "    # ---------------------------\n",
    "\n",
    "    #basic_description_wikitext = '''\n",
    "    page_wikitext = '''\n",
    "=={{int:filedesc}}==\n",
    "{{Information\n",
    "|description={{''' + description_language.lower() + '''|1=''' + description + '''}}\n",
    "|date=''' + iso_date + '''\n",
    "|source=[https://www.learnwikidata.net/?''' + media_item['language'] + ''' Learn Wikidata website]\n",
    "|author=Vanderbilt University\n",
    "|permission=\n",
    "|other versions= <gallery>\n",
    "File:''' + media_item['other_versions1'] + '|[[:File:' + media_item['other_versions1'] + '|' + language_dict[media_item['other_versions1_language']] + ''']]\n",
    "File:''' + media_item['other_versions2'] + '|[[:File:' + media_item['other_versions2'] + '|' + language_dict[media_item['other_versions2_language']] + ''']]\n",
    "</gallery>\n",
    "|other_fields={{Credit line \n",
    " |Author = © Vanderbilt University\n",
    " |Other = Wikimedia Commons\n",
    " |License = [https://creativecommons.org/licenses/by/4.0/ CC BY 4.0]}}}}\n",
    "\n",
    "=={{int:license-header}}==\n",
    "{{self|cc-by-4.0}}\n",
    "\n",
    "'''\n",
    "    # Note: trailing blank line assumes that categories will be appended, see loop below.\n",
    "\n",
    "    # Add all of the categories in the list\n",
    "    for category_string in category_strings:\n",
    "        page_wikitext += '''[[Category:''' + category_string + ''']]\n",
    "'''\n",
    "    # print(page_wikitext)\n",
    "\n",
    "    data = upload_file_to_commons(image_filename, directory_path, path_is_relative_to_home_directory, commons_session, commons_csrf_token, sleeptime, page_wikitext)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    print('Upload:', data['upload']['result'])\n",
    "\n",
    "    # ----------------\n",
    "    # Set the image caption\n",
    "    # ----------------\n",
    "\n",
    "    # This has to be done in an API call separate from the upload \n",
    "    # since the caption is a Wikibase label and not part of the Wikitext\n",
    "\n",
    "    data = set_commons_image_caption(image_filename, caption, caption_language, commons_session, commons_csrf_token, sleeptime)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    if data['success'] == 1:\n",
    "        status_message = 'Success'\n",
    "    else:\n",
    "        status_message = 'Failed'\n",
    "    print('Caption:', status_message)\n",
    "\n",
    "    # ----------------\n",
    "    # Add structured data\n",
    "    # ----------------\n",
    "\n",
    "    property_p_id = 'P275' # copyright license\n",
    "    value_q_id = 'Q20007257' # Creative Commons Attribution 4.0 International\n",
    "    data = create_commons_claim(image_filename, property_p_id, value_q_id, commons_session, commons_csrf_token, sleeptime)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    if data['success'] == 1:\n",
    "        status_message = 'Success'\n",
    "    else:\n",
    "        status_message = 'Failed'\n",
    "    print('License claim:', status_message)\n",
    "\n",
    "    property_p_id = 'P6216' # copyright status\n",
    "    value_q_id = 'Q50423863' # copyrighted\n",
    "    data = create_commons_claim(image_filename, property_p_id, value_q_id, commons_session, commons_csrf_token, sleeptime)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    if data['success'] == 1:\n",
    "        status_message = 'Success'\n",
    "    else:\n",
    "        status_message = 'Failed'\n",
    "    print('Copyrighted claim:', status_message)\n",
    "    print()\n",
    "    print('--------------------------')\n",
    "    print()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
