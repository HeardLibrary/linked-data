{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commonsbot.ipynb, a Python script for uploading files and data to Wikimedia Commons using the API.\n",
    "version = '0.3'\n",
    "created = '2021-12-04'\n",
    "\n",
    "# (c) 2021 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# IMPORTANT NOTE: If you use this script to upload media to Commons, you MUST NOT decrease the delay time between\n",
    "# API writes to less than 5 seconds in order to speed up writing. If you do, then your script isn't \n",
    "# BaskaufCommonsBot and you need to change the user_agent_string to use your own URL and email address.\n",
    "# The same holds true if you make other substantive changes to the way that the script interacts with the API.\n",
    "# This script attempts to respect the \"good citizen\" guidelines for using the API and you should too.\n",
    "\n",
    "# These are recommended delay times to avoid hitting the APIs too frequently and getting blocked\n",
    "sparql_sleep = 0.25 # delay time between calls to Wikidata SPARQL endpoint, probably could be lower (like 0.1)\n",
    "commons_sleep = 5 # non-critical edits to commons no faster than this. https://commons.wikimedia.org/wiki/Commons:Bots#Bot_speed\n",
    "read_api_sleep = 0.1\n",
    "\n",
    "# Description of bots on Commons: https://commons.wikimedia.org/wiki/Commons:Bots\n",
    "# See guidelines for operating a bot in Commons: https://commons.wikimedia.org/wiki/Commons:Bots/Requests\n",
    "# Need to decide whether this applies if non autonomous. It probably does.\n",
    "# Bot flag is an indication of community trust and prevents new images/recent changes lists from getting swamped.\n",
    "# It's also an indication of community trust; confirms edits not likely to need manual checking\n",
    "\n",
    "# Generic Commons API reference: https://commons.wikimedia.org/w/api.php\n",
    "\n",
    "# NOTE: this script recycles code from the more full-featured and better tested VanderBot script:\n",
    "# https://github.com/HeardLibrary/linked-data/tree/master/vanderbot\n",
    "\n",
    "# ----------------\n",
    "# Configuration\n",
    "# ----------------\n",
    "\n",
    "# This section contains import statements and function definitions.\n",
    "# It should be run before running other sections of the code\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import sys\n",
    "import re # regex\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Hard coded values\n",
    "\n",
    "# Change working directory to image upload directory\n",
    "os.chdir('/users/baskausj/github/vandycite/gallery_works/image_upload/')\n",
    "\n",
    "local_image_directory_path = 'gallery_digital_image_archive/'\n",
    "path_is_relative_to_home_directory = True\n",
    "\n",
    "user_agent_string = 'BaskaufCommonsBot/' + version + ' (https://github.com/HeardLibrary/linked-data/tree/master/commonsbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "public_domain_categories = [\n",
    "    'artist died before copyright cutoff', \n",
    "    'artist was born before 1800', \n",
    "    'assessed to be out of copyright', \n",
    "    'from style or period that ended prior to copyright cutoff',\n",
    "    'inception prior to copyright cutoff'\n",
    "]\n",
    "\n",
    "# Options for filtering by image size\n",
    "size_filter = 'pixsquared' # options: filetype, filesize, pixsquared\n",
    "requrired_filetype = 'tiff' # not implemented (yet)\n",
    "minimum_filesize = 1000\n",
    "minimup_pixel_squared = 1000000\n",
    "\n",
    "templated_institution = 'Vanderbilt University Fine Arts Gallery' # Name used in an existing Institution template\n",
    "source_name = 'Vanderbilt University Fine Arts Gallery'\n",
    "category_strings = ['Vanderbilt University Fine Arts Gallery'] # Commons categories to be added to the image.\n",
    "default_language = 'en'\n",
    "\n",
    "# ------------------------\n",
    "# function definitions\n",
    "# ------------------------\n",
    "\n",
    "# Utility functions\n",
    "\n",
    "# gunction to get local name from an IRI\n",
    "def extract_localname(iri):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[len(pieces)-1] # return the last piece\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "# Commons identifier/URL conversion functions\n",
    "# There are three identifiers used in Commons:\n",
    "\n",
    "# The most basic one is the filename, unencoded and with file extension.\n",
    "\n",
    "# The Commons web page URL is formed from the filename by prepending a subpath and \"File:\", replacing spaces in the filename with _, and URL-encoding the file name string\n",
    "# The reverse process may be lossy because it assumes that underscores should be turned into spaces and the filename might actuall contain underscores.\n",
    "\n",
    "# The Wikidata IRI identifier for the image is formed from the filename by URL-encoding it and prepending a subpath and \"Special:FilePath/\"\n",
    "# It the reverse process is lossless since it simply reverse URL-encodes the local name part of the IRI.\n",
    "\n",
    "commons_prefix = 'http://commons.wikimedia.org/wiki/Special:FilePath/'\n",
    "commons_page_prefix = 'https://commons.wikimedia.org/wiki/File:'\n",
    "def commons_url_to_filename(url):\n",
    "    # form of URL is: http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\n",
    "    string = url.split(commons_prefix)[1] # get local name file part of URL\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_url(filename):\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_prefix + encoded_filename\n",
    "    return url\n",
    "\n",
    "def commons_page_url_to_filename(url):\n",
    "    # form of URL is: https://commons.wikimedia.org/wiki/File:Castle_De_Haar_(1892-1913)_-_360%C2%B0_Panorama_of_Castle_%26_Castle_Grounds.jpg\n",
    "    string = url.split(commons_page_prefix)[1] # get local name file part of URL\n",
    "    string = string.replace('_', ' ')\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_page_url(filename):\n",
    "    filename = filename.replace(' ', '_')\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_page_prefix + encoded_filename\n",
    "    url = url.replace('%28', '(').replace('%29', ')').replace('%2C', ',')\n",
    "    return url\n",
    "\n",
    "\n",
    "# Authentication functions\n",
    "\n",
    "def login(path, relative_to_home):\n",
    "    if relative_to_home:\n",
    "        home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "        full_credentials_path = home + '/' + path\n",
    "    else:\n",
    "        full_credentials_path = path\n",
    "    credentials = retrieve_credentials(full_credentials_path)\n",
    "    \n",
    "    resource_url = '/w/api.php' # default API resource URL for all Wikimedia APIs\n",
    "    endpoint_url = credentials['url'] + resource_url\n",
    "\n",
    "    # Instantiate session\n",
    "    session = requests.Session()\n",
    "    # Set default User-Agent header so you don't have to send it with every request\n",
    "    session.headers.update({'User-Agent': user_agent_string})\n",
    "\n",
    "    # Go through the sequence of steps needed to get get the CSRF token\n",
    "    login_token = get_login_token(endpoint_url, session)\n",
    "    data = session_login(endpoint_url, login_token, credentials['username'], credentials['password'], session)\n",
    "    csrf_token = get_csrf_token(endpoint_url, session)\n",
    "    return {'session': session, 'csrftoken': csrf_token, 'endpoint': endpoint_url}\n",
    "\n",
    "def retrieve_credentials(path):\n",
    "    with open(path, 'rt') as file_object:\n",
    "        line_list = file_object.read().split('\\n')\n",
    "    endpoint_url = line_list[0].split('=')[1]\n",
    "    username = line_list[1].split('=')[1]\n",
    "    password = line_list[2].split('=')[1]\n",
    "    #user_agent = line_list[3].split('=')[1]\n",
    "    credentials = {'url': endpoint_url, 'username': username, 'password': password}\n",
    "    return credentials\n",
    "\n",
    "def get_login_token(apiUrl, session):    \n",
    "    parameters = {\n",
    "        'action':'query',\n",
    "        'meta':'tokens',\n",
    "        'type':'login',\n",
    "        'format':'json'\n",
    "    }\n",
    "    r = session.get(url=apiUrl, params=parameters)\n",
    "    data = r.json()\n",
    "    return data['query']['tokens']['logintoken']\n",
    "\n",
    "def session_login(apiUrl, token, username, password, session):\n",
    "    parameters = {\n",
    "        'action':'login',\n",
    "        'lgname':username,\n",
    "        'lgpassword':password,\n",
    "        'lgtoken':token,\n",
    "        'format':'json'\n",
    "    }\n",
    "    r = session.post(apiUrl, data=parameters)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "def get_csrf_token(apiUrl, session):\n",
    "    parameters = {\n",
    "        \"action\": \"query\",\n",
    "        \"meta\": \"tokens\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    r = session.get(url=apiUrl, params=parameters)\n",
    "    data = r.json()\n",
    "    return data[\"query\"][\"tokens\"][\"csrftoken\"]\n",
    "\n",
    "# Data upload functions\n",
    "\n",
    "# API file Upload example: https://www.mediawiki.org/wiki/API:Upload#POST_request\n",
    "# API Sandbox can be used to generate test JSON, but DO NOT RUN since it actually uploads.\n",
    "# Specifically for uploads, see https://commons.wikimedia.org/wiki/Special:ApiSandbox#action=upload&filename=Wiki.png&url=http%3A//upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png&token=123ABC\n",
    "def upload_file_to_commons(image_filename, commons_filename, directory_path, relative_to_home, session, csrftoken, sleeptime, wikitext):\n",
    "    if relative_to_home:\n",
    "        home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "        directory_path = home + '/' + directory_path\n",
    "\n",
    "    parameters = {\n",
    "        'action': 'upload',\n",
    "        'filename': commons_filename,\n",
    "        'format': 'json',\n",
    "        'token': csrftoken,\n",
    "        'ignorewarnings': 1,\n",
    "        'text': wikitext,\n",
    "        # this is what generates the text in the Description box on user Uploads page and initial edit summary for page\n",
    "        # See https://commons.wikimedia.org/wiki/Commons:First_steps/Quality_and_description#Upload_summary\n",
    "        'comment': 'Uploaded media file and metadata via API'\n",
    "    }\n",
    "    #directory_path = 'Downloads/'\n",
    "    file_path = directory_path + image_filename\n",
    "    file_dict = {'file':(image_filename, open(file_path, 'rb'), 'multipart/form-data')}\n",
    "    #print(parameters)\n",
    "    #print(file_dict)\n",
    "\n",
    "    print('uploading', commons_filename) # This line is important for large TIFF files that will take a while to upload\n",
    "    response = session.post('https://commons.wikimedia.org/w/api.php', files=file_dict, data = parameters)\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    # for non-critical applications, do not hit the API rapidly\n",
    "    sleep(sleeptime)\n",
    "    return(data)\n",
    "\n",
    "# Adding the image caption seems to be a hack that uses the Wikibase API command wbsetlabel.\n",
    "# Captions are Wikibase labels (language specific), limit 255 characters length.\n",
    "# See https://commons.wikimedia.org/wiki/Commons:File_captions#Technical\n",
    "def set_commons_image_caption(image_filename, caption, caption_language, session, csrftoken, sleeptime):\n",
    "    parameters = {\n",
    "        'action': 'wbsetlabel',\n",
    "        'format': 'json',\n",
    "        'token': csrftoken,\n",
    "        'site': 'commonswiki',\n",
    "        'title': 'File:' + image_filename,\n",
    "        'value': caption,\n",
    "        'language': caption_language,\n",
    "        'summary': 'Add caption via API'\n",
    "    }\n",
    "\n",
    "    #print(json.dumps(parameters, indent = 2))\n",
    "\n",
    "    response = session.post('https://commons.wikimedia.org/w/api.php', data = parameters)\n",
    "    \n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    sleep(sleeptime)\n",
    "    return(data)\n",
    "\n",
    "# This function is used in the following function, which needs a page ID rather than a name\n",
    "def get_commons_image_pageid(image_filename):\n",
    "    # get metadata for a photo including from file page\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'File:' + image_filename,\n",
    "        'prop': 'info'\n",
    "    }\n",
    "\n",
    "    response = requests.get('https://commons.wikimedia.org/w/api.php', params=params)\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    page_dict = data['query']['pages'] # this value is a dict that has the page IDs as keys\n",
    "    page_id_list = list(page_dict.keys()) # the result of the .keys() method is a \"dict_keys\" object, so coerce to a list\n",
    "    page_id = page_id_list[0] # info on only one page was requested, so get item 0\n",
    "    #print('Page ID:',page_id)\n",
    "    \n",
    "    # Don't think I need to add a sleep time for API reads, which are less resource-intensive\n",
    "    # than write operations\n",
    "    return page_id\n",
    "\n",
    "# Code comes from writeStatement() function at https://github.com/HeardLibrary/digital-scholarship/blob/master/code/wikibase/api/load_csv.py\n",
    "# Described in this blog post: http://baskauf.blogspot.com/2019/06/putting-data-into-wikidata-using.html\n",
    "def create_commons_claim(image_filename, property_p_id, value_q_id, session, csrftoken, sleeptime):\n",
    "    wikibase_subject_id = 'M' + get_commons_image_pageid(image_filename)\n",
    "    #property_p_id = 'P180' # depicts\n",
    "    #value_q_id = 'Q384177' # Egyptian Revival (architecture)\n",
    "\n",
    "    stripped_q_number = value_q_id[1:len(value_q_id)] # remove initial \"Q\" from object string\n",
    "    value_dictionary = {\n",
    "        'entity-type': 'item',\n",
    "        'numeric-id': stripped_q_number\n",
    "    }\n",
    "    value_json_string = json.dumps(value_dictionary)\n",
    "\n",
    "    parameters = {\n",
    "        'action':'wbcreateclaim',\n",
    "        'format':'json',\n",
    "        'token': csrftoken,\n",
    "        'entity': wikibase_subject_id,\n",
    "        'snaktype':'value',\n",
    "        'property': property_p_id,\n",
    "        # note: the value of 'value' is a JSON string, not an actual data structure.  \n",
    "        #It will get URL encoded by requests before posting\n",
    "        'value': value_json_string,\n",
    "        'summary': 'Add structured data via API'\n",
    "    }\n",
    "\n",
    "    #print(json.dumps(parameters, indent = 2))\n",
    "    response = session.post('https://commons.wikimedia.org/w/api.php', data = parameters)\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    sleep(sleeptime)\n",
    "    return data\n",
    "\n",
    "# Create the date_string for various uncertainty situations\n",
    "def create_template_date_string(inception_val, inception_prec, inception_earliest_date_val, inception_earliest_date_prec, inception_latest_date_val, inception_latest_date_prec, inception_sourcing_circumstances):\n",
    "    # See https://commons.wikimedia.org/wiki/Template:Other_date for formatting information\n",
    "    \n",
    "    # Return nothing if there is no inception date\n",
    "    if inception_val == '':\n",
    "        return ''\n",
    "    \n",
    "    if inception_earliest_date_val == '': # no date range\n",
    "        # No VU gallery works have precisions > 9, but it's handled just in case\n",
    "        if inception_prec == '11': # precise to day\n",
    "            date_string = inception_val[:10]\n",
    "        elif inception_prec == '10': # precise to month\n",
    "            date_string = inception_val[:7]\n",
    "        else:\n",
    "            date_string = inception_val[:4] # return the year, regardless of whether precision is year, decade, century, etc.\n",
    "        # Handle circa\n",
    "        if inception_sourcing_circumstances == 'Q5727902': # Q ID for circa\n",
    "            date_string = '{{other date|circa|'+ date_string + '}}'\n",
    "            \n",
    "    else: # date range must be handled\n",
    "        if inception_earliest_date_prec == '11': # precise to day\n",
    "            early_date_string = inception_earliest_date_val[:10]\n",
    "        elif inception_earliest_date_prec == '10': # precise to month\n",
    "            early_date_string = inception_earliest_date_val[:7]\n",
    "        else:\n",
    "            early_date_string = inception_earliest_date_val[:4] # return the year, regardless of whether precision is year, decade, century, etc.\n",
    "            \n",
    "        if inception_latest_date_prec == '11': # precise to day\n",
    "            late_date_string = inception_latest_date_val[:10]\n",
    "        elif inception_latest_date_prec == '10': # precise to month\n",
    "            late_date_string = inception_latest_date_val[:7]\n",
    "        else:\n",
    "            late_date_string = inception_latest_date_val[:4] # return the year, regardless of whether precision is year, decade, century, etc.\n",
    "            \n",
    "        # Handle circa\n",
    "        if inception_sourcing_circumstances == 'Q5727902': # Q ID for circa\n",
    "            date_string = '{{other date|circa|'+ early_date_string + '|'+ late_date_string + '}}'\n",
    "        else:\n",
    "            date_string = '{{other date|between|'+ early_date_string + '|'+ late_date_string + '}}'\n",
    "    \n",
    "    return date_string\n",
    "            \n",
    "\n",
    "# Insert metadata into the Commons Artwork template\n",
    "def create_commons_template(work_qid, label, description, description_language, artist_qid, date_string, width, height, source_name, templated_institution, reference_url, reference_iso_date_string, notes, medium, category_strings):\n",
    "    \n",
    "    # Convert the dateTime formatted string to European style date with month word\n",
    "    datetime_object = datetime.datetime.fromisoformat(reference_iso_date_string[:10])\n",
    "    reference_date = datetime_object.strftime('%d %B %Y')\n",
    "    # Remove leading zero on day if any\n",
    "    if reference_date[0] == '0':\n",
    "        reference_date = reference_date[1:]\n",
    "            \n",
    "    page_wikitext = '''\n",
    "=={{int:filedesc}}==\n",
    "{{Artwork\n",
    " |artist             = {{ Creator | Wikidata = ''' + artist_qid + ''' | Option = {{{1|}}} }}\n",
    " |title              = ''' + \"{{en|'''\" + label + \"'''.}}\" + '''\n",
    " |description        = {{''' + description_language + '''|1=''' + description + '''}}\n",
    " |depicted people    =\n",
    " |depicted place     =\n",
    " |date               = ''' + date_string + '''\n",
    " |medium             = ''' + medium + '''\n",
    " |dimensions         = {{Size|in|''' + width + '|' + height + '''}}\n",
    " |institution        = {{Institution:''' + templated_institution + '''}}\n",
    " |department         =\n",
    " |accession number   = \n",
    " |place of creation  = \n",
    " |place of discovery =\n",
    " |object history     =\n",
    " |exhibition history =\n",
    " |credit line        =\n",
    " |inscriptions       =\n",
    " |notes              = ''' + notes + '''\n",
    " |references         = {{cite web |title=''' + label + ' |url=' + reference_url + ' |accessdate=' + reference_date + '''}}\n",
    " |source             = ''' + source_name + '''\n",
    " |permission         =\n",
    " |other_versions     =\n",
    " |wikidata           = ''' + work_qid + '''\n",
    " |other_fields       =\n",
    "}}\n",
    "\n",
    "=={{int:license-header}}==\n",
    "{{PD-Art|PD-old-100-expired}}\n",
    "\n",
    "'''\n",
    "    # Add all of the categories in the list\n",
    "    for category_string in category_strings:\n",
    "        page_wikitext += '[[Category:' + category_string + ''']]\n",
    "'''\n",
    "    \n",
    "    return page_wikitext\n",
    "\n",
    "# ---------------\n",
    "# Not used yet\n",
    "# ---------------\n",
    "# borrowed from VanderBot\n",
    "\n",
    "# This function attempts to post and handles maxlag errors\n",
    "def attemptPost(apiUrl, parameters):\n",
    "    maxRetries = 10\n",
    "    delayLimit = 300\n",
    "    retry = 0\n",
    "    # maximum number of times to retry lagged server = maxRetries\n",
    "    while retry <= maxRetries:\n",
    "        if retry > 0:\n",
    "            print('retry:', retry)\n",
    "        r = session.post(apiUrl, data = parameters)\n",
    "        print(r.text)\n",
    "        data = r.json()\n",
    "        try:\n",
    "            # check if response is a maxlag error\n",
    "            # see https://www.mediawiki.org/wiki/Manual:Maxlag_parameter\n",
    "            if data['error']['code'] == 'maxlag':\n",
    "                print('Lag of ', data['error']['lag'], ' seconds.')\n",
    "                # recommended delay is basically useless\n",
    "                # recommendedDelay = int(r.headers['Retry-After'])\n",
    "                #if recommendedDelay < 5:\n",
    "                    # recommendation is to wait at least 5 seconds if server is lagged\n",
    "                #    recommendedDelay = 5\n",
    "                recommendedDelay = commons_sleep*2**retry # double the delay with each retry \n",
    "                if recommendedDelay > delayLimit:\n",
    "                    recommendedDelay = delayLimit\n",
    "                if retry != maxRetries:\n",
    "                    print('Waiting ', recommendedDelay , ' seconds.')\n",
    "                    print()\n",
    "                    sleep(recommendedDelay)\n",
    "                retry += 1\n",
    "\n",
    "                # after this, go out of if and try code blocks\n",
    "            else:\n",
    "                # an error code is returned, but it's not maxlag\n",
    "                return data\n",
    "        except:\n",
    "            # if the response doesn't have an error key, it was successful, so return\n",
    "            return data\n",
    "        # here's where execution goes after the delay\n",
    "    # here's where execution goes after maxRetries tries\n",
    "    print('Failed after ' + str(maxRetries) + ' retries.')\n",
    "    exit() # just abort the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Body of main script\n",
    "# ---------------------------\n",
    "\n",
    "# This section contains configuration information and performs necessary logins\n",
    "# It needs to be run once before the rest of the code\n",
    "# No writing is done, so it's \"safe\" to run any time\n",
    "\n",
    "'''\n",
    "# Set the value of the maxlag parameter to back off when the server is lagged\n",
    "# see https://www.mediawiki.org/wiki/Manual:Maxlag_parameter\n",
    "# The recommended value is 5 seconds.\n",
    "# To not use maxlang, set the value to 0\n",
    "# To test the maxlag handler code, set maxlag to a very low number like .1\n",
    "\n",
    "# NOTE: as of 2020-04-27, the function that needs maxlag isn't being used for anything, so this value doesn't matter\n",
    "maxlag = 5\n",
    "'''\n",
    "\n",
    "# This section needs to be run prior to running any code that interacts with the Commons API\n",
    "# It generates the CSRF token required to post to the API on behalf of the user whose username and pwd are being used\n",
    "\n",
    "# This is the format of the credentials file. \n",
    "# Username and password are for a bot that you've created.\n",
    "\n",
    "'''\n",
    "endpointUrl=https://test.wikidata.org\n",
    "username=User@bot\n",
    "password=465jli90dslhgoiuhsaoi9s0sj5ki3lo\n",
    "'''\n",
    "\n",
    "# ---------------------------\n",
    "# Load data from CSVs into DataFrames\n",
    "# ---------------------------\n",
    "\n",
    "# Note: setting the index to be the Q ID requires that qid has a unique value for each row. This should be the case.\n",
    "works_metadata = pd.read_csv('../works_multiprop.csv', na_filter=False, dtype = str)\n",
    "works_metadata.set_index('qid', inplace=True)\n",
    "\n",
    "raw_metadata = pd.read_csv('../gallery_works_renamed1.csv', na_filter=False, dtype = str)\n",
    "raw_metadata.set_index('accession_number', inplace=True)\n",
    "\n",
    "image_dimensions = pd.read_csv('image_dimensions.csv', na_filter=False, dtype = str)\n",
    "# Convert some columns to integers\n",
    "image_dimensions[['kilobytes', 'height', 'width']] = image_dimensions[['kilobytes', 'height', 'width']].astype(int)\n",
    "\n",
    "works_classification = pd.read_csv('../../gallery_buchanan/works_classification.csv', na_filter=False, dtype = str)\n",
    "works_classification.set_index('qid', inplace=True)\n",
    "\n",
    "works_ip_status = pd.read_csv('../items_status_abbrev.csv', na_filter=False, dtype = str)\n",
    "works_ip_status.set_index('qid', inplace=True)\n",
    "\n",
    "existing_images = pd.read_csv('commons_images.csv', na_filter=False, dtype = str)\n",
    "existing_images.set_index('qid', inplace=True)\n",
    "\n",
    "# For testing purposes, just use the first few rows of the works metadata\n",
    "test_rows = 60\n",
    "works_metadata = works_metadata.head(test_rows).copy()\n",
    "\n",
    "# ---------------------------\n",
    "# Commons API Post Authentication (create session and generate CSRF token)\n",
    "# ---------------------------\n",
    "\n",
    "# If credentials file location is relative to current working directory, use subfolders through file name with no leading slash\n",
    "# Example: myproj/credentials/commons_credentials.txt\n",
    "# If credentials file is in current working directory, only filename is necessary\n",
    "# Need to give example for absolute path on Windows - use Unix forward slashes?\n",
    "path = 'commons_credentials.txt'\n",
    "path_is_relative_to_home_directory = True # set to True if relative home directory, False if absolute path or relative to working directory\n",
    "result = login(path, path_is_relative_to_home_directory)\n",
    "# print(result)\n",
    "commons_session = result['session']\n",
    "commons_csrf_token = result['csrftoken']\n",
    "# Commons API endpoint URL is in result['endpoint'], but it is going to be hard coded anyway, so ignore\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Screen works for appropriate images to upload\n",
    "# ---------------------------\n",
    "\n",
    "# The row index is the Q ID and is a string. The work object is the data in the row and is a Pandas series\n",
    "# The items in the row series can be referred to by their labels, which are the column headers, e.g. work['label_en']\n",
    "for index, work in works_metadata.iterrows():    \n",
    "    # Screen out images that are already in Commons\n",
    "    if index in existing_images.index:\n",
    "        continue\n",
    "    \n",
    "    # Screen for 2 dimensional works\n",
    "    if index in works_classification.index:\n",
    "        # Find the row whose index matches the Q ID of the work, then the item by name within the series (dimension)\n",
    "        # Note: this method of location works because the Q ID index is unique for each row in the lookup table.\n",
    "        if works_classification.loc[index, 'dimension'] != '2D': # skip this work if not 2D\n",
    "            continue\n",
    "\n",
    "    # Screen for public domain images\n",
    "    # NOTE: the IP status was only done for cases where the script was able to match up image file names with accession numbers.\n",
    "    # It should be done again to pick up more images based on the new image_dimensions.csv file after it's cleaned up.\n",
    "    # There are at least a thousand works that will get screened out here because they aren't imaged.\n",
    "    if not index in works_ip_status.index:\n",
    "        continue\n",
    "    else:\n",
    "        ip_status = works_ip_status.loc[index, 'status']\n",
    "        if not ip_status in public_domain_categories:\n",
    "            continue\n",
    "    \n",
    "    # Screen for high resolution images\n",
    "    image_dimension_frame = image_dimensions.loc[image_dimensions.accession == work['inventory_number']] # result is DataFrame\n",
    "    if len(image_dimension_frame) == 0: # skip any works whose image can't be found in the dimensions data\n",
    "        continue\n",
    "\n",
    "    # Order rows by size in kB, then take the first row\n",
    "    image_dimension_series = image_dimension_frame.sort_values(by=['kilobytes'], ascending=False).iloc[0]\n",
    "    # Skip work if its image doesn't meet the minimum size requirement\n",
    "    if size_filter == 'pixsquared':\n",
    "        if image_dimension_series['height'] * image_dimension_series['width'] < minimup_pixel_squared:\n",
    "            continue\n",
    "    elif size_filter == 'filesize':\n",
    "        if image_dimension_series['kilobytes'] < minimum_filesize:\n",
    "            continue\n",
    "    else: # don't apply a size filter\n",
    "        pass\n",
    "\n",
    "    # Create inception date string for template from VanderBot upload CSV\n",
    "    inception_val = work['inception_val']\n",
    "    inception_prec = work['inception_prec']\n",
    "    inception_earliest_date_val = work['inception_earliest_date_val']\n",
    "    inception_earliest_date_prec = work['inception_earliest_date_prec']\n",
    "    inception_latest_date_val = work['inception_latest_date_val']\n",
    "    inception_latest_date_prec = work['inception_latest_date_prec']\n",
    "    inception_sourcing_circumstances = work['inception_sourcing_circumstances']\n",
    "    date_string = create_template_date_string(inception_val, inception_prec, inception_earliest_date_val, inception_earliest_date_prec, inception_latest_date_val, inception_latest_date_prec, inception_sourcing_circumstances)\n",
    "\n",
    "    # Get raw string data directly from the Artstor downloae\n",
    "    try:\n",
    "        raw_series = raw_metadata.loc[work['inventory_number']]\n",
    "        gift = raw_series['gift_of']\n",
    "        if gift != '':\n",
    "            notes = 'Gift of ' + gift\n",
    "        else:\n",
    "            notes = ''\n",
    "        medium = raw_series['medium']\n",
    "    except:\n",
    "        notes = ''\n",
    "        medium = ''\n",
    "\n",
    "    # Get the remaining metadata from the VanderBot upload CSV\n",
    "    work_qid = index\n",
    "    label = work['label_en']\n",
    "    wikidata_description = work['description_en']\n",
    "    artist_qid = work['creator']\n",
    "    description = label + ', ' + wikidata_description\n",
    "    description_language = default_language\n",
    "    width = work['width_val']\n",
    "    height = work['height_val']\n",
    "    reference_url = work['inventory_number_ref1_referenceUrl']\n",
    "    reference_date = work['inventory_number_ref1_retrieved_val']\n",
    "\n",
    "    page_wikitext = create_commons_template(work_qid, label, description, description_language, artist_qid, date_string, width, height, source_name, templated_institution, reference_url, reference_date, notes, medium, category_strings)\n",
    "    print(page_wikitext)\n",
    "\n",
    "    # The local_filename is the name of the file as it exists locally.\n",
    "    local_filename = image_dimension_series['name']\n",
    "\n",
    "    # subdirectory is the directory that contains the local file. It's within the local_image_directory_path. \n",
    "    # Don't include a trailing slash.\n",
    "    # If images are directly in the directory_path, use empty string ('') as the value.\n",
    "    subdirectory = image_dimension_series['subdir']\n",
    "\n",
    "    # filename_institution is the name of the institution to be inserted between the descriptive text and the local filename\n",
    "    filename_institution = 'Vanderbilt Fine Arts Gallery'\n",
    "\n",
    "    # file_prefix is descriptive text to be prepended to the local_filename, to be used when the file is in Commons\n",
    "    # Commons filename length limit is 240 bytes. To be safe, limit to 230.\n",
    "    byte_limit = 230 - len((' - ' + filename_institution + ' - ' + local_filename).encode(\"utf8\"))\n",
    "    if len(label.encode(\"utf8\")) < byte_limit:\n",
    "        file_prefix = label\n",
    "    else:\n",
    "        file_prefix = label.encode(\"utf8\")[:byte_limit].decode('utf8')\n",
    "\n",
    "    # Set image_filename to the raw filename (can include spaces). The API will substitute underscores as it likes.\n",
    "    # For file naming conventions, see: https://commons.wikimedia.org/wiki/Commons:File_naming\n",
    "\n",
    "    commons_filename = file_prefix + ' - ' + filename_institution + ' - ' + local_filename\n",
    "\n",
    "    # Add the subdirectory (if any) to the path\n",
    "    if subdirectory != '':\n",
    "        full_path = local_image_directory_path + subdirectory + '/'\n",
    "    else:\n",
    "        full_path = local_image_directory_path\n",
    "\n",
    "    sleeptime = 0 # use zero if running individual cells manually, use commons_sleep if code in single script or looping\n",
    "    data = upload_file_to_commons(local_filename, commons_filename, full_path, path_is_relative_to_home_directory, commons_session, commons_csrf_token, sleeptime, page_wikitext)\n",
    "\n",
    "    #response = commons_session.post(endpointUrl, files=file_dict, data = parameters)\n",
    "    #data = response.json()\n",
    "    print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Set the image caption\n",
    "# ----------------\n",
    "\n",
    "# This has to be done in an API call separate from the upload \n",
    "# since the caption is a Wikibase label and not part of the Wikitext\n",
    "\n",
    "'''\n",
    "parameters = {\n",
    "    'action': 'wbsetlabel',\n",
    "    'format': 'json',\n",
    "    'token': commons_csrf_token,\n",
    "    'site': 'commonswiki',\n",
    "    'title': 'File:' + image_filename,\n",
    "    'value': caption,\n",
    "    'language': caption_language,\n",
    "    'summary': 'Add caption via API'\n",
    "}\n",
    "'''\n",
    "#print(json.dumps(parameters, indent = 2))\n",
    "\n",
    "sleeptime = 0 # use zero if running individual cells manually, use commons_sleep if code in single script or looping\n",
    "\n",
    "# The caption does not have to be the same as the description, but for convenience, I'm making them the same\n",
    "caption = description\n",
    "caption_language = default_language\n",
    "\n",
    "data = set_commons_image_caption(commons_filename, caption, caption_language, commons_session, commons_csrf_token, sleeptime)\n",
    "\n",
    "#response = commons_session.post(endpointUrl, data = parameters)\n",
    "#data = response.json()\n",
    "print(json.dumps(data, indent=2))\n",
    "\n",
    "#sleep(commons_sleep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Add structured data\n",
    "# ----------------\n",
    "\n",
    "# Intro on structured data: https://commons.wikimedia.org/wiki/Commons:Structured_data\n",
    "# See also this on GLAM https://commons.wikimedia.org/wiki/Commons:Structured_data/GLAM\n",
    "\n",
    "# NOTE: artworks will get flagged if they don't have P6243 in their structured data\n",
    "# non-public domain works get flagged if they don't have a P275 license statement in structured data\n",
    "\n",
    "property_p_id = 'P6243' # digital representaion of\n",
    "value_q_id = work_qid # the artwork in Wikidata\n",
    "\n",
    "sleeptime = 0 # use zero if running individual cells manually, use commons_sleep if code in single script or looping\n",
    "data = create_commons_claim(commons_filename, property_p_id, value_q_id, commons_session, commons_csrf_token, sleeptime)\n",
    "'''\n",
    "wikibase_subject_id = 'M' + get_commons_image_pageid(image_filename)\n",
    "property_p_id = 'P180' # depicts\n",
    "value_q_id = 'Q384177' # Egyptian Revival (architecture)\n",
    "\n",
    "stripped_q_number = value_q_id[1:len(value_q_id)] # remove initial \"Q\" from object string\n",
    "value_dictionary = {\n",
    "    'entity-type': 'item',\n",
    "    'numeric-id': stripped_q_number\n",
    "}\n",
    "value_json_string = json.dumps(value_dictionary)\n",
    "\n",
    "parameters = {\n",
    "    'action':'wbcreateclaim',\n",
    "    'format':'json',\n",
    "    'token': commons_csrf_token,\n",
    "    'entity': wikibase_subject_id,\n",
    "    'snaktype':'value',\n",
    "    'property': property_p_id,\n",
    "    # note: the value is a JSON string, not an actual data structure.  I think it will get URL encoded by requests before posting\n",
    "    'value': value_json_string,\n",
    "    'summary': 'Add depicts value structured data via API'\n",
    "}\n",
    "\n",
    "#print(json.dumps(parameters, indent = 2))\n",
    "response = commons_session.post(endpointUrl, data = parameters)\n",
    "data = response.json()\n",
    "'''\n",
    "print(json.dumps(data, indent=2))\n",
    "\n",
    "#sleep(commons_sleep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code development cells\n",
    "\n",
    "Don't run these, they are left for historical reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename = 'Adoration of the Sheperds - Vanderbilt Fine Arts Gallery - 1979.0264P.tif'\n",
    "get_commons_image_pageid(image_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where image file is located with trailing forward slash\n",
    "# relative to working directory or an absolute path if path_is_relative_to_home_directory = False\n",
    "# relative to home directory if path_is_relative_to_home_directory = True\n",
    "directory_path = 'gallery_digital_image_archive/'\n",
    "path_is_relative_to_home_directory = True\n",
    "\n",
    "# The local_filename is the name of the file as it exists locally.\n",
    "local_filename = '1984.021.tif'\n",
    "\n",
    "# subdirectory is the directory that contains the local file. It's within the directory_path. \n",
    "# Don't include a trailing slash.\n",
    "# If images are directly in the directory_path, use empty string ('') as the value.\n",
    "subdirectory = '1984'\n",
    "\n",
    "# file_prefix is descriptive text to be prepended to the local_filename, to be used when the file is in Commons\n",
    "file_prefix = 'A Conversation with Guido di Brettinoro'\n",
    "\n",
    "# filename_institution is the name of the institution to be inserted between the descriptive text and the local filename\n",
    "filename_institution = 'Vanderbilt Fine Arts Gallery'\n",
    "\n",
    "# Set image_filename to the raw filename (can include spaces). The API will substitute underscores as it likes.\n",
    "# For file naming conventions, see: https://commons.wikimedia.org/wiki/Commons:File_naming\n",
    "\n",
    "commons_filename = file_prefix + ' - ' + filename_institution + ' - ' + local_filename\n",
    "\n",
    "# The caption has to be set in a separate operation from the file upload\n",
    "# But it's set here so that its text can be used for the description\n",
    "# Captions must be less than 255 characters. \n",
    "# There can be multiple captions in different languages, but only one per language.\n",
    "caption = \"A Conversation with Guido di Brettinoro, a print by John Flaxman from Illustrations to Dante's Divine Comedy\"\n",
    "caption_language = 'en'\n",
    "\n",
    "# The description doesn't have to be the same as the caption.\n",
    "# It can be much longer and contain Wiki formatting, such as links. \n",
    "description = caption\n",
    "description_language = 'en'\n",
    "\n",
    "# Category\n",
    "category_strings = ['Vanderbilt University Fine Arts Gallery']\n",
    "\n",
    "artist_qid = 'Q366066'\n",
    "label = \"A Conversation with Guido di Brettinoro, (Purgatorio, Canto 14) from Illustrations to Dante's Divine Comedy\"\n",
    "date_string = '1807'\n",
    "width = '13'\n",
    "height = '9'\n",
    "templated_institution = 'Vanderbilt University Fine Arts Gallery'\n",
    "notes = 'Gift of Thomas B. Brumbaugh'\n",
    "reference_url = 'https://library.artstor.org/#/asset/26755766'\n",
    "reference_date = '2 December 2020'\n",
    "source_name = 'Vanderbilt University Fine Arts Gallery'\n",
    "work_qid = 'Q102961225'\n",
    "\n",
    "# ---------------------------\n",
    "# Upload a local file to Commons and set basic metadata\n",
    "# ---------------------------\n",
    "\n",
    "# The 'text' parameter value provides the required file information that shows up in new sections.\n",
    "\n",
    "# Guidelines for providing information using the Information template: https://commons.wikimedia.org/wiki/Template:Information\n",
    "# Note special template for artwork having more extensive metadata: https://commons.wikimedia.org/wiki/Template:Artwork\n",
    "# Historical photographs (e.g. museums) https://commons.wikimedia.org/wiki/Template:Photograph\n",
    "# Art photo template adds to artwork template https://commons.wikimedia.org/wiki/Template:Art_Photo\n",
    "# Credit line template provides attribution text requred for CC BY licenses https://commons.wikimedia.org/wiki/Template:Credit_line\n",
    "\n",
    "# Wiki text based on Artwork template:\n",
    "# Note: script is hardcoded with 'page_wikitext' so need to change that depending on if this one or the Information template is used\n",
    "\n",
    "# See https://commons.wikimedia.org/wiki/Commons:When_to_use_the_PD-Art_tag for info about tagging artwork as Public Domain\n",
    "# See also https://commons.wikimedia.org/wiki/Commons:Licensing#Material_in_the_public_domain\n",
    "# See also https://commons.wikimedia.org/wiki/Commons:Copyright_tags/Country-specific_tags#United_States_of_America\n",
    "\n",
    "# NOTE: when art template is used and a Wikidata Q ID is given, \n",
    "# the page will pick up the Object type and Place of creation automatically from Wikidata, so values don't need to be provided.\n",
    "# Not sure if that's a result of the wikidata field here or providing the P6243 (digital representation) value in Structured data.\n",
    "# Other stuff like accession number and inscriptions also get picked up\n",
    "\n",
    "# artwork_description_wikitext = '''\n",
    "page_wikitext = '''\n",
    "=={{int:filedesc}}==\n",
    "{{Artwork\n",
    " |artist             = {{ Creator | Wikidata = ''' + artist_qid + ''' | Option = {{{1|}}} }}\n",
    " |title              = ''' + \"{{en|'''\" + label + \"'''.}}\" + '''\n",
    " |description        = {{''' + description_language + '''|1=''' + description + '''}}\n",
    " |depicted people    =\n",
    " |depicted place     =\n",
    " |date               = ''' + date_string + '''\n",
    " |medium             =\n",
    " |dimensions         = {{Size|in|''' + width + '|' + height + '''}}\n",
    " |institution        = {{Institution:''' + templated_institution + '''}}\n",
    " |department         =\n",
    " |accession number   = \n",
    " |place of creation  = \n",
    " |place of discovery =\n",
    " |object history     =\n",
    " |exhibition history =\n",
    " |credit line        =\n",
    " |inscriptions       =\n",
    " |notes              = ''' + notes + '''\n",
    " |references         = {{cite web |title=''' + label + ' |url=' + reference_url + ' |accessdate=' + reference_date + '''}}\n",
    " |source             = ''' + source_name + '''\n",
    " |permission         =\n",
    " |other_versions     =\n",
    " |wikidata           = ''' + work_qid + '''\n",
    " |other_fields       =\n",
    "}}\n",
    "\n",
    "=={{int:license-header}}==\n",
    "{{PD-Art|PD-old-100-expired}}\n",
    "\n",
    "'''\n",
    "\n",
    "# Here's what I used for a basic description template (the \"Information\" template):\n",
    "basic_description_wikitext = '''\n",
    "=={{int:filedesc}}==\n",
    "{{Information\n",
    " |description={{''' + description_language + '''|1=''' + description + '''}}\n",
    " |date=2020-12-27 10:00:00\n",
    " |source={{own}}\n",
    " |author=[[User:Baskaufs|Steven J. Baskauf]]\n",
    " |permission=\n",
    " |other versions=\n",
    " |other_fields={{Credit line \n",
    "  |Author = Â© Steven J. Baskauf\n",
    "  |Other = Wikimedia Commons\n",
    "  |License = [https://creativecommons.org/licenses/by/4.0/ CC BY 4.0]}}}}\n",
    "\n",
    "{{Location|41.30941681442741|-72.92922321706307}}\n",
    "\n",
    "=={{int:license-header}}==\n",
    "{{self|cc-by-4.0}}\n",
    "\n",
    "'''\n",
    "# Note: trailing blank line assumes that categories will be appended, see loop below.\n",
    "\n",
    "'''\n",
    "parameters = {\n",
    "    'action': 'upload',\n",
    "    'filename': image_filename,\n",
    "    'format': 'json',\n",
    "    'token': commons_csrf_token,\n",
    "    'ignorewarnings': 1,\n",
    "    'text': page_wikitext,\n",
    "    # this is what generates the text in the Description box on user Uploads page and initial edit summary for page\n",
    "    # See https://commons.wikimedia.org/wiki/Commons:First_steps/Quality_and_description#Upload_summary\n",
    "    'comment': 'Uploaded image file and metadata via API'\n",
    "}\n",
    "'''\n",
    "\n",
    "# Add all of the categories in the list\n",
    "for category_string in category_strings:\n",
    "    page_wikitext += '''[[Category:''' + category_string + ''']]\n",
    "    '''\n",
    "\n",
    "# Add the subdirectory (if any) to the path\n",
    "if subdirectory != '':\n",
    "    full_path = directory_path + subdirectory + '/'\n",
    "else:\n",
    "    full_path = directory_path\n",
    "\n",
    "sleeptime = 0 # use zero if running individual cells manually, use commons_sleep if code in single script or looping\n",
    "data = upload_file_to_commons(local_filename, commons_filename, full_path, path_is_relative_to_home_directory, commons_session, commons_csrf_token, sleeptime, page_wikitext)\n",
    "\n",
    "#response = commons_session.post(endpointUrl, files=file_dict, data = parameters)\n",
    "#data = response.json()\n",
    "print(json.dumps(data, indent=2))\n",
    "\n",
    "# Most page info can be set when the page is created as above.\n",
    "# To edit an existing page, the edit action must be used\n",
    "# API information on edit action: https://commons.wikimedia.org/w/api.php?action=help&modules=edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The local_filename is the name of the file as it exists locally.\n",
    "local_filename = image_dimension_series['name']\n",
    "\n",
    "# subdirectory is the directory that contains the local file. It's within the local_image_directory_path. \n",
    "# Don't include a trailing slash.\n",
    "# If images are directly in the directory_path, use empty string ('') as the value.\n",
    "subdirectory = image_dimension_series['subdir']\n",
    "\n",
    "# file_prefix is descriptive text to be prepended to the local_filename, to be used when the file is in Commons\n",
    "file_prefix = 'Chinese Winter Landscape'\n",
    "\n",
    "# filename_institution is the name of the institution to be inserted between the descriptive text and the local filename\n",
    "filename_institution = 'Vanderbilt Fine Arts Gallery'\n",
    "\n",
    "# Set image_filename to the raw filename (can include spaces). The API will substitute underscores as it likes.\n",
    "# For file naming conventions, see: https://commons.wikimedia.org/wiki/Commons:File_naming\n",
    "\n",
    "commons_filename = file_prefix + ' - ' + filename_institution + ' - ' + local_filename\n",
    "\n",
    "\n",
    "# Add the subdirectory (if any) to the path\n",
    "if subdirectory != '':\n",
    "    full_path = local_image_directory_path + subdirectory + '/'\n",
    "else:\n",
    "    full_path = local_image_directory_path\n",
    "\n",
    "sleeptime = 0 # use zero if running individual cells manually, use commons_sleep if code in single script or looping\n",
    "data = upload_file_to_commons(local_filename, commons_filename, full_path, path_is_relative_to_home_directory, commons_session, commons_csrf_token, sleeptime, page_wikitext)\n",
    "\n",
    "#response = commons_session.post(endpointUrl, files=file_dict, data = parameters)\n",
    "#data = response.json()\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at https://commons.wikimedia.org/wiki/File:USS_Arizona_afloat_after_launch_NARA_19-LC-19A-24.tif\n",
    "# to see how they linked to their collection in Wikidata and also how they did the Record ID\n",
    "# They seem to be using their own NARA template\n",
    "\n",
    "# Categorization: according to https://commons.wikimedia.org/wiki/Commons:Bots#Bot_accounts all uploads are expected to apply at least one category\n",
    "\n",
    "\n",
    "\n",
    "# Linking\n",
    "# Wikilinks from the Commons: https://en.wikipedia.org/wiki/Wikipedia:Wikilinks_from_the_Commons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific script to upload Learn Wikidata videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "language_dict = {'en': 'in English', 'es': 'en espaÃ±ol', 'zh-Hans': 'æ±è¯­'}\n",
    "\n",
    "# Category\n",
    "category_strings = ['Wikidata videos', 'Learn Wikidata videos by the Vanderbilt Libraries']\n",
    "\n",
    "# path where image file is located with trailing forward slash\n",
    "# relative to working directory or an absolute path if path_is_relative_to_home_directory = False\n",
    "# relative to home directory if path_is_relative_to_home_directory = True\n",
    "directory_path = 'Documents/video_conversion/webm/'\n",
    "path_is_relative_to_home_directory = True\n",
    "sleeptime = commons_sleep # use zero if running individual cells manually, use commons_sleep if code in single script or looping\n",
    "\n",
    "filename = 'upload_metadata.csv'\n",
    "media_items_list = read_dict(filename)\n",
    "for media_item in media_items_list[12:]:\n",
    "\n",
    "    # Set image_filename to the raw filename (can include spaces). The API will substitute underscores as it likes.\n",
    "    # For file naming conventions, see: https://commons.wikimedia.org/wiki/Commons:File_naming\n",
    "\n",
    "    image_filename = media_item['filename']\n",
    "    print(image_filename)\n",
    "    print()\n",
    "\n",
    "    # The caption has to be set in a separate operation from the file upload\n",
    "    # But it's set here so that its text can be used for the description\n",
    "    # Captions must be less than 255 characters. \n",
    "    # There can be multiple captions in different languages, but only one per language.\n",
    "    caption = media_item['caption']\n",
    "    caption_language = media_item['language'].lower()\n",
    "\n",
    "    # The description doesn't have to be the same as the caption.\n",
    "    # It can be much longer and contain Wiki formatting, such as links. \n",
    "    description = media_item['description']\n",
    "    description_language = media_item['language'].lower()\n",
    "    iso_date = str(pd.to_datetime(media_item['date']))\n",
    "    #print(iso_date)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Upload a local file to Commons and set basic metadata\n",
    "    # ---------------------------\n",
    "\n",
    "    #basic_description_wikitext = '''\n",
    "    page_wikitext = '''\n",
    "=={{int:filedesc}}==\n",
    "{{Information\n",
    "|description={{''' + description_language.lower() + '''|1=''' + description + '''}}\n",
    "|date=''' + iso_date + '''\n",
    "|source=[https://www.learnwikidata.net/?''' + media_item['language'] + ''' Learn Wikidata website]\n",
    "|author=Vanderbilt University\n",
    "|permission=\n",
    "|other versions= <gallery>\n",
    "File:''' + media_item['other_versions1'] + '|[[:File:' + media_item['other_versions1'] + '|' + language_dict[media_item['other_versions1_language']] + ''']]\n",
    "File:''' + media_item['other_versions2'] + '|[[:File:' + media_item['other_versions2'] + '|' + language_dict[media_item['other_versions2_language']] + ''']]\n",
    "</gallery>\n",
    "|other_fields={{Credit line \n",
    " |Author = Â© Vanderbilt University\n",
    " |Other = Wikimedia Commons\n",
    " |License = [https://creativecommons.org/licenses/by/4.0/ CC BY 4.0]}}}}\n",
    "\n",
    "=={{int:license-header}}==\n",
    "{{self|cc-by-4.0}}\n",
    "\n",
    "'''\n",
    "    # Note: trailing blank line assumes that categories will be appended, see loop below.\n",
    "\n",
    "    # Add all of the categories in the list\n",
    "    for category_string in category_strings:\n",
    "        page_wikitext += '''[[Category:''' + category_string + ''']]\n",
    "'''\n",
    "    # print(page_wikitext)\n",
    "\n",
    "    data = upload_file_to_commons(image_filename, directory_path, path_is_relative_to_home_directory, commons_session, commons_csrf_token, sleeptime, page_wikitext)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    print('Upload:', data['upload']['result'])\n",
    "\n",
    "    # ----------------\n",
    "    # Set the image caption\n",
    "    # ----------------\n",
    "\n",
    "    # This has to be done in an API call separate from the upload \n",
    "    # since the caption is a Wikibase label and not part of the Wikitext\n",
    "\n",
    "    data = set_commons_image_caption(image_filename, caption, caption_language, commons_session, commons_csrf_token, sleeptime)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    if data['success'] == 1:\n",
    "        status_message = 'Success'\n",
    "    else:\n",
    "        status_message = 'Failed'\n",
    "    print('Caption:', status_message)\n",
    "\n",
    "    # ----------------\n",
    "    # Add structured data\n",
    "    # ----------------\n",
    "\n",
    "    property_p_id = 'P275' # copyright license\n",
    "    value_q_id = 'Q20007257' # Creative Commons Attribution 4.0 International\n",
    "    data = create_commons_claim(image_filename, property_p_id, value_q_id, commons_session, commons_csrf_token, sleeptime)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    if data['success'] == 1:\n",
    "        status_message = 'Success'\n",
    "    else:\n",
    "        status_message = 'Failed'\n",
    "    print('License claim:', status_message)\n",
    "\n",
    "    property_p_id = 'P6216' # copyright status\n",
    "    value_q_id = 'Q50423863' # copyrighted\n",
    "    data = create_commons_claim(image_filename, property_p_id, value_q_id, commons_session, commons_csrf_token, sleeptime)\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    if data['success'] == 1:\n",
    "        status_message = 'Success'\n",
    "    else:\n",
    "        status_message = 'Failed'\n",
    "    print('Copyrighted claim:', status_message)\n",
    "    print()\n",
    "    print('--------------------------')\n",
    "    print()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
