{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commons_data.ipynb, a Python script for downloading and scraping data from Wikimedia Commons\n",
    "version = '0.1'\n",
    "created = '2021-10-29'\n",
    "\n",
    "# (c) 2021 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# Generic Commons API reference: https://commons.wikimedia.org/w/api.php\n",
    "# NOTE: See the commonsbot code for more information about the various forms of identifiers used for Commons files.\n",
    "\n",
    "# Common code\n",
    "# ----------------\n",
    "\n",
    "# This section contains import statements and function definitions.\n",
    "# It should be run before running other sections of the code\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import sys\n",
    "import re # regex\n",
    "from bs4 import BeautifulSoup # web-scraping library, use PIP to install beautifulsoup4 (included in Anaconda)\n",
    "\n",
    "read_api_sleep = 0.1 # delay to throttle the script and not hit the server too fast\n",
    "\n",
    "# ------------------------\n",
    "# function definitions\n",
    "# ------------------------\n",
    "\n",
    "# Utility functions\n",
    "\n",
    "# gunction to get local name from an IRI\n",
    "def extract_localname(iri):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[len(pieces)-1] # return the last piece\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Retrieving image data from the Commons MediaWiki API\n",
    "# -------------------\n",
    "\n",
    "# This code is essentially useless because the data retrieved from the API is less extensive and no cleaner \n",
    "# than data scraped from the MediaWiki HTML. I've retained this for historical purposes.\n",
    "\n",
    "# Initially, I thought that it was necessary to know the Wikibase entity ID to set the caption.\n",
    "# So I wrote code to extract that from a query. However, one can use the page title, so that\n",
    "# isn't actually necessary. But it might be needed anyway for the structured data part.\n",
    "\n",
    "# The Wikibase entity ID can be used in lieu of the site+page name.\n",
    "# The format is \"M\" plus the page ID. So page ID 41837276 has the entity ID M41837276\n",
    "# Use action=query&prop=info&titles=File:Pluto-01_Stern_03_Pluto_Color_TXT.jpg and \n",
    "# extract the pageid field from result.\n",
    "\n",
    "# Commons API examples: https://commons.wikimedia.org/wiki/Commons:API/MediaWiki\n",
    "# sandbox: https://commons.wikimedia.org/wiki/Special:ApiSandbox#action=query&format=json&prop=categories%7Cimageinfo&titles=File%3AMasonry_patterns_in_doorway_Tetouan_Morocco.jpg\n",
    "# General Sandbox index: https://commons.wikimedia.org/wiki/Special:ApiSandbox#action=query&format=json&prop=categories%7Cimageinfo&titles\n",
    "\n",
    "def retrieve_commons_api_info(act_id, filename):\n",
    "    apiUrl = 'https://commons.wikimedia.org/w/api.php'\n",
    "\n",
    "    '''\n",
    "    # get photos by a user\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'list': 'allimages',\n",
    "        'aiuser': 'Baskaufs',\n",
    "        'aisort': 'timestamp'\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # get category information about a photo. NOTE: fewer items than categories provided in the extmetadata option\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'File:Masonry_patterns_in_doorway_Tetouan_Morocco.jpg',\n",
    "        'prop': 'categories|imageinfo'\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # get raw metadata embedded in a photo\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'File:Masonry_patterns_in_doorway_Tetouan_Morocco.jpg',\n",
    "        'prop': 'imageinfo',\n",
    "        'iiprop': 'metadata',\n",
    "        'iimetadataversion': 'latest'\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # get metadata for a photo including from file page (does not produce much except the page title, basically the filename)\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'File:' + image_filename,\n",
    "        'prop': 'info'\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    # get metadata for a photo including from file page (lots of data, but doesn't correspond exactly to \n",
    "    # data on image web page)\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'File:' + filename,\n",
    "        'prop': 'imageinfo',\n",
    "        'iiprop': 'extmetadata'\n",
    "    }\n",
    "\n",
    "    response = requests.get(apiUrl, params=params)\n",
    "    data = response.json()\n",
    "    # print(json.dumps(data, indent=2))\n",
    "\n",
    "    info = {'act_id': act_id, 'filename': filename} # dict to collect results\n",
    "    pages = data['query']['pages']\n",
    "    # Oddly, the only key within pages object is the Wikibase ID\n",
    "    # Assuming there is only one ID per page title, just get the first one\n",
    "    wikibase_id = list(pages.keys())[0]\n",
    "    info['wikibase_id'] = wikibase_id\n",
    "    # Also oddly, the imageinfo value is a dict. Not sure why an image would have multiple imageinfos, so just get 1st one\n",
    "    item_data = pages[wikibase_id]['imageinfo'][0]['extmetadata']\n",
    "    # print(json.dumps(item_data, indent = 2))\n",
    "    # print(item_data.keys())\n",
    "\n",
    "    if 'Categories' in item_data:\n",
    "        categories_list = item_data['Categories']['value'].split('|')\n",
    "    else:\n",
    "        categories_list = []\n",
    "    info['categories_list'] = json.dumps(categories_list)\n",
    "\n",
    "    info['artist_name'] = ''\n",
    "    info['artist_description'] = ''    \n",
    "    info['user_name'] = ''\n",
    "    info['user_url'] = ''\n",
    "\n",
    "    if \"Artist\" in item_data:\n",
    "        soup = BeautifulSoup(item_data['Artist']['value'], 'html.parser')\n",
    "        if 'bdi' == soup.contents[0].name:\n",
    "            try:\n",
    "                info['artist_name'] = soup.bdi.a.span.string\n",
    "            except:\n",
    "                info['artist_name'] = str(soup)\n",
    "            try:\n",
    "                info['artist_description'] = soup.bdi.a.span['title']\n",
    "            except:\n",
    "                info['artist_description'] = ''\n",
    "        if 'a' == soup.contents[0].name:\n",
    "            try:\n",
    "                info['user_name'] = soup.a.string\n",
    "            except:\n",
    "                info['user_name'] = str(soup)\n",
    "            try:\n",
    "                info['user_url'] = soup.a['href']\n",
    "            except:\n",
    "                info['user_url'] = ''\n",
    "        if not('bdi' == soup.contents[0].name) and not('a' == soup.contents[0].name):\n",
    "            info['artist_name'] = str(soup)\n",
    "\n",
    "    if 'LicenseShortName' in item_data:\n",
    "        info['license_name'] = item_data['LicenseShortName']['value']\n",
    "    else:\n",
    "        info['license_name'] = ''\n",
    "    if 'License' in item_data:\n",
    "        info['license_code'] = item_data['License']['value']\n",
    "    else:\n",
    "        info['license_code'] = ''\n",
    "\n",
    "    if 'DateTimeOriginal' in item_data:\n",
    "        info['dateTime_original_string'] = item_data['DateTimeOriginal']['value'].split('<')[0].strip()\n",
    "    else:\n",
    "        info['dateTime_original_string'] = ''\n",
    "\n",
    "    if 'ImageDescription' in item_data:\n",
    "        soup = BeautifulSoup(item_data['ImageDescription']['value'], 'html.parser')\n",
    "        try:\n",
    "            info['main_subject_P921_label'] = soup.div.a.span.string\n",
    "            info['main_subject_P921_full'] = soup.div.a.span['title']\n",
    "            info['main_subject_P921_qid'] = extract_localname(soup.div.a['href'])\n",
    "        except:\n",
    "            info['main_subject_P921_label'] = str(soup)\n",
    "            info['main_subject_P921_full'] = ''\n",
    "            info['main_subject_P921_qid'] = ''       \n",
    "    else:\n",
    "        info['main_subject_P921_label'] = ''\n",
    "        info['main_subject_P921_full'] = ''\n",
    "        info['main_subject_P921_qid'] = ''\n",
    "\n",
    "    if 'ObjectName' in item_data:\n",
    "        soup = BeautifulSoup(item_data['ObjectName']['value'], 'html.parser')\n",
    "        # use regex to match any text\n",
    "        my_regex = re.compile(\".*\")\n",
    "        title_soup = soup.find_all(text=my_regex)\n",
    "        title_list = list(title_soup)\n",
    "        clean_title_list = [x.strip() for x in title_list if x != '\\n'] # remove newline items from list of title strings\n",
    "    else:\n",
    "        clean_title_list = []\n",
    "    if len(clean_title_list) > 0:\n",
    "        info['title'] = clean_title_list[0]\n",
    "    else:\n",
    "        info['title'] = ''\n",
    "    info['titles_list'] = json.dumps(clean_title_list)\n",
    "    return info\n",
    "\n",
    "    # print(json.dumps(info, indent = 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Scrape data from table on image page\n",
    "# ---------------------------\n",
    "\n",
    "server_sleep = 0.2\n",
    "\n",
    "# Guidelines for providing information using the Information template: https://commons.wikimedia.org/wiki/Template:Information\n",
    "# Note special template for artwork having more extensive metadata: https://commons.wikimedia.org/wiki/Template:Artwork\n",
    "# Historical photographs (e.g. museums) https://commons.wikimedia.org/wiki/Template:Photograph\n",
    "# Art photo template adds to artwork template https://commons.wikimedia.org/wiki/Template:Art_Photo\n",
    "# Credit line template provides attribution text requred for CC BY licenses https://commons.wikimedia.org/wiki/Template:Credit_line\n",
    "\n",
    "file_path = '../../vandycite/act/processed_lists/works_already_in_wikidata.csv'\n",
    "file_data = read_dict(file_path)\n",
    "\n",
    "with open('all_fields.json', 'rt') as file_object:\n",
    "    all_field_list = json.loads(file_object.read())\n",
    "    field_list = all_field_list # stop using separate lists\n",
    "\n",
    "# Create a list of fields to be used for the CSV output\n",
    "output_fields = ['filename', 'template_type']\n",
    "for field in all_field_list:\n",
    "    output_fields.append(field)\n",
    "\n",
    "nonstandard_fields = []\n",
    "output_list = []\n",
    "#if True:\n",
    "for record in file_data:\n",
    "    print(record['filename'])\n",
    "    image_filename = record['filename']\n",
    "    #image_filename = 'Fra_Angelico_-_The_Coronation_of_the_Virgin_-_WGA0630.jpg'\n",
    "    # image_filename = 'Christ sur la mer de Galilée (Delacroix) Walters Art Museum 37.186.jpg'\n",
    "    # image_filename = 'Christ_and_SocratesSAAM_1974.28.341A_B_1.jpg'\n",
    "    # image_filename = \"Christ's_temptation_(Monreale).jpg\"\n",
    "\n",
    "   # Create a dictionary with keys for all fields in the template type and empty string values\n",
    "    output_dict = {'filename': image_filename}\n",
    "    for field in all_field_list:\n",
    "        output_dict[field] = []\n",
    "    \n",
    "    # Retrieve the page HTML\n",
    "    page_url = 'https://commons.wikimedia.org/wiki/File:' + image_filename\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    # Create a soup object and find the file info table\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.findAll('table', class_= re.compile('fileinfotpl'))\n",
    "    if len(tables) == 0:\n",
    "        output_dict['template_type'] = 'none'\n",
    "        print('No data table')\n",
    "        print()\n",
    "        output_list.append(output_dict)\n",
    "        continue\n",
    "    try:\n",
    "        template_type = str(tables[0]['class'][0]).split('-')[-1]\n",
    "    except:\n",
    "        template_type = 'unknown'\n",
    "    print('Page template:', template_type) # useful types are \"information\" and \"artwork\"\n",
    "    print()\n",
    "    output_dict['template_type'] = template_type\n",
    "        \n",
    "    # stop using separate lists\n",
    "    #with open(template_type + '.json', 'rt') as file_object:\n",
    "    #    field_list = json.loads(file_object.read())\n",
    "    #print(field_list)\n",
    "    \n",
    "    # Step through all of the rows looking for columns with particular labels\n",
    "    rows = tables[0].tbody.findAll('tr', recursive=False) # find only in direct child tr elements\n",
    "    for row in rows:        \n",
    "        columns = row.findAll('td')\n",
    "        if len(columns) > 0:\n",
    "            # In cases where the contents of the td tag are more complext than a simple string (e.g. if the \n",
    "            # contents include text and markup tags), the strings attribute generates a tuple-like iterable\n",
    "            # of strings included inside the tag (a \"generator\"). The first iterable string is always the one we want\n",
    "            string_list = []\n",
    "            field_matched = False\n",
    "            strings = columns[0].strings\n",
    "            for string in strings:\n",
    "                string_list.append(string)\n",
    "            if len(string_list) > 0:\n",
    "                field_name = string_list[0]\n",
    "                for field in field_list:\n",
    "                    if field_name.lower() == field:\n",
    "                        # The value of field will remain as the last matched one\n",
    "                        field_matched = True\n",
    "                        break\n",
    "\n",
    "            if columns[1].string:\n",
    "                # Turn the string into a list of one string\n",
    "                value_string = json.dumps([columns[1].string])\n",
    "            else:\n",
    "                # use regex to match any text\n",
    "                my_regex = re.compile(\".*\")\n",
    "                string_list = columns[1].find_all(text=my_regex)\n",
    "                no_newlines_list = [x.strip() for x in string_list if x != '\\n'] # remove newline items from list of strings\n",
    "                clean_string_list = [x.strip() for x in no_newlines_list if x != ''] # remove newline items from list of strings\n",
    "                if len(clean_string_list) == 1:\n",
    "                    # clean string and turn into list of length 1\n",
    "                    value_string = json.dumps([clean_string_list[0].strip()])\n",
    "                else:\n",
    "                    value_string = json.dumps(clean_string_list)\n",
    "            value_string = value_string.strip()\n",
    "                    \n",
    "            if field_matched:\n",
    "                #print('Matched standard field:', field_name)\n",
    "                #print(value_string)\n",
    "                # Insert the found value into the dict for the matched field\n",
    "                output_dict[field] = value_string\n",
    "            else:\n",
    "                #print('Non-standard field:', field_name)\n",
    "                #print(value_string)\n",
    "                nonstandard_fields.append({'filename': image_filename, 'fieldname': field_name, 'value': value_string})\n",
    "            #print()\n",
    "    sleep(server_sleep) # Don't hit the web server too fast\n",
    "    #print(json.dumps(output_dict, indent = 2))\n",
    "    output_list.append(output_dict)\n",
    "    #print('-----------------')\n",
    "            \n",
    "#print(json.dumps(nonstandard_fields, indent = 2))\n",
    "#write_dicts_to_csv(nonstandard_fields, 'nonstandard_fields.csv', ['filename', 'fieldname', 'value'])\n",
    "write_dicts_to_csv(output_list, 'already_in_templated_data.csv', output_fields)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Look for a Wikidata link on the image page\n",
    "# ---------------------------\n",
    "\n",
    "# This script finds the link for the tiny little Wikidata logo found on many pages that use the artwork template. \n",
    "# It's significant because this links to the abstract artwork even if the file represented on the Commons page\n",
    "# isn't the one used as the value of the image (P18) property in Wikidata.\n",
    "\n",
    "# For references on art in Wikidata, see https://www.wikidata.org/wiki/Wikidata:WikiProject_sum_of_all_paintings\n",
    "# https://www.wikidata.org/wiki/Wikidata:WikiProject_Visual_arts/Item_structure\n",
    "\n",
    "file_path = '../../vandycite/act/create_items/abstract_artworks_out.csv'\n",
    "file_data = read_dict(file_path)\n",
    "\n",
    "output_list = []\n",
    "#if True:\n",
    "for record in file_data:\n",
    "    print(record['image'])\n",
    "\n",
    "    # Retrieve the page HTML\n",
    "    image_filename = record['image']\n",
    "    # image_filename = 'Christ sur la mer de Galilée (Delacroix) Walters Art Museum 37.186.jpg'\n",
    "    # image_filename = 'Ingobertus_001.jpg' # should not work as of 2022-01-13\n",
    "    # image_filename = 'Fra_Filippo_Lippi_-_Madonna_and_Child_with_two_Angels_-_Uffizi.jpg'\n",
    "    # image_filename = 'Drawing of Abbie Sweetwine treating injured.jpg' # should produce nothing\n",
    "    page_url = 'https://commons.wikimedia.org/wiki/File:' + image_filename\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    # Create a soup object and find the file info table\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Restrict only to links found in the mediawiki image page content section\n",
    "    tables = soup.findAll('table', class_ = re.compile('fileinfotpl-type-artwork'))\n",
    "    if len(tables) > 0:\n",
    "        # Find the rows in the image page content table\n",
    "        rows = tables[0].findAll('tr')\n",
    "        if len(rows) > 0:\n",
    "            # The link to the Wikidata item will be in an href in the first table row only\n",
    "            # Sometimes the artist link is to a Wikidata item, so can't screen on subdomain.\n",
    "            anchors = rows[0].findAll('a', title = re.compile('wikidata:'))\n",
    "            if len(anchors) > 0:\n",
    "                link = anchors[0]['href']\n",
    "                qid = extract_localname(link)\n",
    "                print(qid)\n",
    "                retrieved_data = {'act_id': record['act'], 'qid': qid, 'filename': record['image']}\n",
    "                output_list.append(retrieved_data)\n",
    "    sleep(read_api_sleep) # Don't hit the server too fast\n",
    "    \n",
    "fieldnames = ['act_id', 'qid', 'filename']\n",
    "write_dicts_to_csv(output_list, '../../vandycite/act/create_items/wikidata_found.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
