{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of bots on Commons: https://commons.wikimedia.org/wiki/Commons:Bots\n",
    "# See guidelines for operating a bot in Commons: https://commons.wikimedia.org/wiki/Commons:Bots/Requests\n",
    "# Need to decide whether this applies if non autonomous. It probably does.\n",
    "# Bot flag is an indication of community trust and prevents new images/recent changes lists from getting swamped.\n",
    "# It's also an indication of community trust; confirms edits not likely to need manual checking\n",
    "\n",
    "# Generic Commons API reference: https://commons.wikimedia.org/w/api.php\n",
    "# NOTE: this script does not distinguish between the image file name, which contains underscores\n",
    "# between words, and the image file name part of the page title, which contains spaces.\n",
    "# When the image file name is formed by prepending \"File:\" to the image file name,\n",
    "# the API seems to automatically convert the underscores to spaces (or something) and\n",
    "# be able to match with the actual page title. Don't know if it would be important\n",
    "# to explicitly differentiate between the two in case this automatic conversion doesn't always work.\n",
    "\n",
    "# ----------------\n",
    "# Common code\n",
    "# ----------------\n",
    "\n",
    "# This section contains import statements and function definitions.\n",
    "# It should be run before running other sections of the code\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import sys\n",
    "import re # regex\n",
    "from bs4 import BeautifulSoup # web-scraping library, use PIP to install beautifulsoup4 (included in Anaconda)\n",
    "\n",
    "# ------------------------\n",
    "# function definitions\n",
    "# ------------------------\n",
    "\n",
    "# Utility functions\n",
    "\n",
    "# gunction to get local name from an IRI\n",
    "def extract_localname(iri):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[len(pieces)-1] # return the last piece\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Authentication functions\n",
    "\n",
    "def login(path, relative_to_home):\n",
    "    if relative_to_home:\n",
    "        home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "        full_credentials_path = home + '/' + path\n",
    "    else:\n",
    "        full_credentials_path = path\n",
    "    credentials = retrieve_credentials(full_credentials_path)\n",
    "    \n",
    "    resource_url = '/w/api.php' # default API resource URL for all Wikimedia APIs\n",
    "    endpoint_url = credentials['url'] + resource_url\n",
    "\n",
    "    # Instantiate session\n",
    "    session = requests.Session()\n",
    "    # Set default User-Agent header so you don't have to send it with every request\n",
    "    session.headers.update({'User-Agent': credentials['user_agent']})\n",
    "\n",
    "    # Go through the sequence of steps needed to get get the CSRF token\n",
    "    login_token = get_login_token(endpoint_url, session)\n",
    "    data = session_login(endpoint_url, login_token, credentials['username'], credentials['password'], session)\n",
    "    csrf_token = get_csrf_token(endpoint_url, session)\n",
    "    return {'session': session, 'csrftoken': csrf_token, 'endpoint': endpoint_url}\n",
    "\n",
    "def retrieve_credentials(path):\n",
    "    with open(path, 'rt') as file_object:\n",
    "        line_list = file_object.read().split('\\n')\n",
    "    endpoint_url = line_list[0].split('=')[1]\n",
    "    username = line_list[1].split('=')[1]\n",
    "    password = line_list[2].split('=')[1]\n",
    "    user_agent = line_list[3].split('=')[1]\n",
    "    credentials = {'url': endpoint_url, 'username': username, 'password': password, 'user_agent': user_agent}\n",
    "    return credentials\n",
    "\n",
    "def get_login_token(apiUrl, session):    \n",
    "    parameters = {\n",
    "        'action':'query',\n",
    "        'meta':'tokens',\n",
    "        'type':'login',\n",
    "        'format':'json'\n",
    "    }\n",
    "    r = session.get(url=apiUrl, params=parameters)\n",
    "    data = r.json()\n",
    "    return data['query']['tokens']['logintoken']\n",
    "\n",
    "def session_login(apiUrl, token, username, password, session):\n",
    "    parameters = {\n",
    "        'action':'login',\n",
    "        'lgname':username,\n",
    "        'lgpassword':password,\n",
    "        'lgtoken':token,\n",
    "        'format':'json'\n",
    "    }\n",
    "    r = session.post(apiUrl, data=parameters)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "def get_csrf_token(apiUrl, session):\n",
    "    parameters = {\n",
    "        \"action\": \"query\",\n",
    "        \"meta\": \"tokens\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    r = session.get(url=apiUrl, params=parameters)\n",
    "    data = r.json()\n",
    "    return data[\"query\"][\"tokens\"][\"csrftoken\"]\n",
    "\n",
    "# Data upload functions\n",
    "\n",
    "# API file Upload example: https://www.mediawiki.org/wiki/API:Upload#POST_request\n",
    "# API Sandbox can be used to generate test JSON, but DO NOT RUN since it actually uploads.\n",
    "# Specifically for uploads, see https://commons.wikimedia.org/wiki/Special:ApiSandbox#action=upload&filename=Wiki.png&url=http%3A//upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png&token=123ABC\n",
    "def upload_file_to_commons(image_filename, directory_path, relative_to_home, session, csrftoken, sleeptime, wikitext):\n",
    "    if relative_to_home:\n",
    "        home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "        directory_path = home + '/' + directory_path\n",
    "\n",
    "    parameters = {\n",
    "        'action': 'upload',\n",
    "        'filename': image_filename,\n",
    "        'format': 'json',\n",
    "        'token': csrftoken,\n",
    "        'ignorewarnings': 1,\n",
    "        'text': wikitext,\n",
    "        # this is what generates the text in the Description box on user Uploads page and initial edit summary for page\n",
    "        # See https://commons.wikimedia.org/wiki/Commons:First_steps/Quality_and_description#Upload_summary\n",
    "        'comment': 'Uploaded image file and metadata via API'\n",
    "    }\n",
    "    #directory_path = 'Downloads/'\n",
    "    file_path = directory_path + image_filename\n",
    "    file_dict = {'file':(image_filename, open(file_path, 'rb'), 'multipart/form-data')}\n",
    "    #print(parameters)\n",
    "    #print(file_dict)\n",
    "\n",
    "    response = session.post('https://commons.wikimedia.org/w/api.php', files=file_dict, data = parameters)\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    # for non-critical applications, do not hit the API rapidly\n",
    "    sleep(sleeptime)\n",
    "    return(data)\n",
    "\n",
    "# Adding the image caption seems to be a hack that uses the Wikibase API command wbsetlabel.\n",
    "# Captions are Wikibase labels (language specific), limit 255 characters length.\n",
    "# See https://commons.wikimedia.org/wiki/Commons:File_captions#Technical\n",
    "def set_commons_image_caption(image_filename, caption, caption_language, session, csrftoken, sleeptime):\n",
    "    parameters = {\n",
    "        'action': 'wbsetlabel',\n",
    "        'format': 'json',\n",
    "        'token': csrftoken,\n",
    "        'site': 'commonswiki',\n",
    "        'title': 'File:' + image_filename,\n",
    "        'value': caption,\n",
    "        'language': caption_language,\n",
    "        'summary': 'Add caption via API'\n",
    "    }\n",
    "\n",
    "    #print(json.dumps(parameters, indent = 2))\n",
    "\n",
    "    response = session.post('https://commons.wikimedia.org/w/api.php', data = parameters)\n",
    "    \n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    sleep(sleeptime)\n",
    "    return(data)\n",
    "\n",
    "# This function is used in the following function, which needs a page ID rather than a name\n",
    "def get_commons_image_pageid(image_filename):\n",
    "    # get metadata for a photo including from file page\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'File:' + image_filename,\n",
    "        'prop': 'info'\n",
    "    }\n",
    "\n",
    "    response = requests.get('https://commons.wikimedia.org/w/api.php', params=params)\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    page_dict = data['query']['pages'] # this value is a dict that has the page IDs as keys\n",
    "    page_id_list = list(page_dict.keys()) # the result of the .keys() method is a \"dict_keys\" object, so coerce to a list\n",
    "    page_id = page_id_list[0] # info on only one page was requested, so get item 0\n",
    "    #print('Page ID:',page_id)\n",
    "    \n",
    "    # Don't think I need to add a sleep time for API reads, which are less resource-intensive\n",
    "    # than write operations\n",
    "    return page_id\n",
    "\n",
    "# Code comes from writeStatement() function at https://github.com/HeardLibrary/digital-scholarship/blob/master/code/wikibase/api/load_csv.py\n",
    "# Described in this blog post: http://baskauf.blogspot.com/2019/06/putting-data-into-wikidata-using.html\n",
    "def create_commons_claim(image_filename, property_p_id, value_q_id, session, csrftoken, sleeptime):\n",
    "    wikibase_subject_id = 'M' + get_commons_image_pageid(image_filename)\n",
    "    #property_p_id = 'P180' # depicts\n",
    "    #value_q_id = 'Q384177' # Egyptian Revival (architecture)\n",
    "\n",
    "    stripped_q_number = value_q_id[1:len(value_q_id)] # remove initial \"Q\" from object string\n",
    "    value_dictionary = {\n",
    "        'entity-type': 'item',\n",
    "        'numeric-id': stripped_q_number\n",
    "    }\n",
    "    value_json_string = json.dumps(value_dictionary)\n",
    "\n",
    "    parameters = {\n",
    "        'action':'wbcreateclaim',\n",
    "        'format':'json',\n",
    "        'token': csrftoken,\n",
    "        'entity': wikibase_subject_id,\n",
    "        'snaktype':'value',\n",
    "        'property': property_p_id,\n",
    "        # note: the value of 'value' is a JSON string, not an actual data structure.  \n",
    "        #It will get URL encoded by requests before posting\n",
    "        'value': value_json_string,\n",
    "        'summary': 'Add structured data via API'\n",
    "    }\n",
    "\n",
    "    #print(json.dumps(parameters, indent = 2))\n",
    "    response = session.post('https://commons.wikimedia.org/w/api.php', data = parameters)\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    sleep(sleeptime)\n",
    "    return data\n",
    "\n",
    "# ---------------\n",
    "# Not used yet\n",
    "# ---------------\n",
    "\n",
    "# This function attempts to post and handles maxlag errors\n",
    "def attemptPost(apiUrl, parameters):\n",
    "    maxRetries = 10\n",
    "    baseDelay = 5 # Wikidata recommends a delay of at least 5 seconds\n",
    "    delayLimit = 300\n",
    "    retry = 0\n",
    "    # maximum number of times to retry lagged server = maxRetries\n",
    "    while retry <= maxRetries:\n",
    "        if retry > 0:\n",
    "            print('retry:', retry)\n",
    "        r = session.post(apiUrl, data = parameters)\n",
    "        print(r.text)\n",
    "        data = r.json()\n",
    "        try:\n",
    "            # check if response is a maxlag error\n",
    "            # see https://www.mediawiki.org/wiki/Manual:Maxlag_parameter\n",
    "            if data['error']['code'] == 'maxlag':\n",
    "                print('Lag of ', data['error']['lag'], ' seconds.')\n",
    "                # recommended delay is basically useless\n",
    "                # recommendedDelay = int(r.headers['Retry-After'])\n",
    "                #if recommendedDelay < 5:\n",
    "                    # recommendation is to wait at least 5 seconds if server is lagged\n",
    "                #    recommendedDelay = 5\n",
    "                recommendedDelay = baseDelay*2**retry # double the delay with each retry \n",
    "                if recommendedDelay > delayLimit:\n",
    "                    recommendedDelay = delayLimit\n",
    "                if retry != maxRetries:\n",
    "                    print('Waiting ', recommendedDelay , ' seconds.')\n",
    "                    print()\n",
    "                    sleep(recommendedDelay)\n",
    "                retry += 1\n",
    "\n",
    "                # after this, go out of if and try code blocks\n",
    "            else:\n",
    "                # an error code is returned, but it's not maxlag\n",
    "                return data\n",
    "        except:\n",
    "            # if the response doesn't have an error key, it was successful, so return\n",
    "            return data\n",
    "        # here's where execution goes after the delay\n",
    "    # here's where execution goes after maxRetries tries\n",
    "    print('Failed after ' + str(maxRetries) + ' retries.')\n",
    "    exit() # just abort the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Retrieving image data from the Commons MediaWiki API\n",
    "# -------------------\n",
    "\n",
    "# Initially, I thought that it was necessary to know the Wikibase entity ID to set the caption.\n",
    "# So I wrote code to extract that from a query. However, one can use the page title, so that\n",
    "# isn't actually necessary. But it might be needed anyway for the structured data part.\n",
    "\n",
    "# The Wikibase entity ID can be used in lieu of the site+page name.\n",
    "# The format is \"M\" plus the page ID. So page ID 41837276 has the entity ID M41837276\n",
    "# Use action=query&prop=info&titles=File:Pluto-01_Stern_03_Pluto_Color_TXT.jpg and \n",
    "# extract the pageid field from result.\n",
    "\n",
    "# Commons API examples: https://commons.wikimedia.org/wiki/Commons:API/MediaWiki\n",
    "# sandbox: https://commons.wikimedia.org/wiki/Special:ApiSandbox#action=query&format=json&prop=categories%7Cimageinfo&titles=File%3AMasonry_patterns_in_doorway_Tetouan_Morocco.jpg\n",
    "# General Sandbox index: https://commons.wikimedia.org/wiki/Special:ApiSandbox#action=query&format=json&prop=categories%7Cimageinfo&titles\n",
    "def retrieve_commons_api_info(act_id, filename):\n",
    "    apiUrl = 'https://commons.wikimedia.org/w/api.php'\n",
    "\n",
    "    '''\n",
    "    # get photos by a user\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'list': 'allimages',\n",
    "        'aiuser': 'Baskaufs',\n",
    "        'aisort': 'timestamp'\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # get category information about a photo. NOTE: fewer items than categories provided in the extmetadata option\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'File:Masonry_patterns_in_doorway_Tetouan_Morocco.jpg',\n",
    "        'prop': 'categories|imageinfo'\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # get raw metadata embedded in a photo\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'File:Masonry_patterns_in_doorway_Tetouan_Morocco.jpg',\n",
    "        'prop': 'imageinfo',\n",
    "        'iiprop': 'metadata',\n",
    "        'iimetadataversion': 'latest'\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # get metadata for a photo including from file page (does not produce much except the page title, basically the filename)\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'File:' + image_filename,\n",
    "        'prop': 'info'\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    # get metadata for a photo including from file page (lots of data, but doesn't correspond exactly to data on image page)\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'File:' + filename,\n",
    "        'prop': 'imageinfo',\n",
    "        'iiprop': 'extmetadata'\n",
    "    }\n",
    "\n",
    "    response = requests.get(apiUrl, params=params)\n",
    "    data = response.json()\n",
    "    # print(json.dumps(data, indent=2))\n",
    "\n",
    "    info = {'act_id': act_id, 'filename': filename} # dict to collect results\n",
    "    pages = data['query']['pages']\n",
    "    # Oddly, the only key within pages object is the Wikibase ID\n",
    "    # Assuming there is only one ID per page title, just get the first one\n",
    "    wikibase_id = list(pages.keys())[0]\n",
    "    info['wikibase_id'] = wikibase_id\n",
    "    # Also oddly, the imageinfo value is a dict. Not sure why an image would have multiple imageinfos, so just get 1st one\n",
    "    item_data = pages[wikibase_id]['imageinfo'][0]['extmetadata']\n",
    "    # print(json.dumps(item_data, indent = 2))\n",
    "    # print(item_data.keys())\n",
    "\n",
    "    if 'Categories' in item_data:\n",
    "        categories_list = item_data['Categories']['value'].split('|')\n",
    "    else:\n",
    "        categories_list = []\n",
    "    info['categories_list'] = json.dumps(categories_list)\n",
    "\n",
    "    info['artist_name'] = ''\n",
    "    info['artist_description'] = ''    \n",
    "    info['user_name'] = ''\n",
    "    info['user_url'] = ''\n",
    "\n",
    "    if \"Artist\" in item_data:\n",
    "        soup = BeautifulSoup(item_data['Artist']['value'], 'html.parser')\n",
    "        if 'bdi' == soup.contents[0].name:\n",
    "            try:\n",
    "                info['artist_name'] = soup.bdi.a.span.string\n",
    "            except:\n",
    "                info['artist_name'] = str(soup)\n",
    "            try:\n",
    "                info['artist_description'] = soup.bdi.a.span['title']\n",
    "            except:\n",
    "                info['artist_description'] = ''\n",
    "        if 'a' == soup.contents[0].name:\n",
    "            try:\n",
    "                info['user_name'] = soup.a.string\n",
    "            except:\n",
    "                info['user_name'] = str(soup)\n",
    "            try:\n",
    "                info['user_url'] = soup.a['href']\n",
    "            except:\n",
    "                info['user_url'] = ''\n",
    "        if not('bdi' == soup.contents[0].name) and not('a' == soup.contents[0].name):\n",
    "            info['artist_name'] = str(soup)\n",
    "\n",
    "    if 'LicenseShortName' in item_data:\n",
    "        info['license_name'] = item_data['LicenseShortName']['value']\n",
    "    else:\n",
    "        info['license_name'] = ''\n",
    "    if 'License' in item_data:\n",
    "        info['license_code'] = item_data['License']['value']\n",
    "    else:\n",
    "        info['license_code'] = ''\n",
    "\n",
    "    if 'DateTimeOriginal' in item_data:\n",
    "        info['dateTime_original_string'] = item_data['DateTimeOriginal']['value'].split('<')[0].strip()\n",
    "    else:\n",
    "        info['dateTime_original_string'] = ''\n",
    "\n",
    "    if 'ImageDescription' in item_data:\n",
    "        soup = BeautifulSoup(item_data['ImageDescription']['value'], 'html.parser')\n",
    "        try:\n",
    "            info['main_subject_P921_label'] = soup.div.a.span.string\n",
    "            info['main_subject_P921_full'] = soup.div.a.span['title']\n",
    "            info['main_subject_P921_qid'] = extract_localname(soup.div.a['href'])\n",
    "        except:\n",
    "            info['main_subject_P921_label'] = str(soup)\n",
    "            info['main_subject_P921_full'] = ''\n",
    "            info['main_subject_P921_qid'] = ''       \n",
    "    else:\n",
    "        info['main_subject_P921_label'] = ''\n",
    "        info['main_subject_P921_full'] = ''\n",
    "        info['main_subject_P921_qid'] = ''\n",
    "\n",
    "    if 'ObjectName' in item_data:\n",
    "        soup = BeautifulSoup(item_data['ObjectName']['value'], 'html.parser')\n",
    "        # use regex to match any text\n",
    "        my_regex = re.compile(\".*\")\n",
    "        title_soup = soup.find_all(text=my_regex)\n",
    "        title_list = list(title_soup)\n",
    "        clean_title_list = [x.strip() for x in title_list if x != '\\n'] # remove newline items from list of title strings\n",
    "    else:\n",
    "        clean_title_list = []\n",
    "    if len(clean_title_list) > 0:\n",
    "        info['title'] = clean_title_list[0]\n",
    "    else:\n",
    "        info['title'] = ''\n",
    "    info['titles_list'] = json.dumps(clean_title_list)\n",
    "    return info\n",
    "\n",
    "    # print(json.dumps(info, indent = 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Scrape data from table on image page\n",
    "# ---------------------------\n",
    "\n",
    "server_sleep = 0.1\n",
    "\n",
    "# Guidelines for providing information using the Information template: https://commons.wikimedia.org/wiki/Template:Information\n",
    "# Note special template for artwork having more extensive metadata: https://commons.wikimedia.org/wiki/Template:Artwork\n",
    "# Historical photographs (e.g. museums) https://commons.wikimedia.org/wiki/Template:Photograph\n",
    "# Art photo template adds to artwork template https://commons.wikimedia.org/wiki/Template:Art_Photo\n",
    "# Credit line template provides attribution text requred for CC BY licenses https://commons.wikimedia.org/wiki/Template:Credit_line\n",
    "\n",
    "file_path = '../../vandycite/act/processed_lists/add_to_wikidata.csv'\n",
    "file_data = read_dict(file_path)\n",
    "\n",
    "with open('all_fields.json', 'rt') as file_object:\n",
    "    all_field_list = json.loads(file_object.read())\n",
    "    field_list = all_field_list # stop using separate lists\n",
    "\n",
    "# Create a list of fields to be used for the CSV output\n",
    "output_fields = ['filename', 'template_type']\n",
    "for field in all_field_list:\n",
    "    output_fields.append(field)\n",
    "\n",
    "nonstandard_fields = []\n",
    "output_list = []\n",
    "#if True:\n",
    "for record in file_data[200:300]:\n",
    "    print(record['filename'])\n",
    "    image_filename = record['filename']\n",
    "    #image_filename = 'Fra_Angelico_-_The_Coronation_of_the_Virgin_-_WGA0630.jpg'\n",
    "    # image_filename = 'Christ sur la mer de Galilée (Delacroix) Walters Art Museum 37.186.jpg'\n",
    "    # image_filename = 'Christ_and_SocratesSAAM_1974.28.341A_B_1.jpg'\n",
    "    # image_filename = \"Christ's_temptation_(Monreale).jpg\"\n",
    "\n",
    "   # Create a dictionary with keys for all fields in the template type and empty string values\n",
    "    output_dict = {'filename': image_filename}\n",
    "    for field in all_field_list:\n",
    "        output_dict[field] = []\n",
    "    \n",
    "    # Retrieve the page HTML\n",
    "    page_url = 'https://commons.wikimedia.org/wiki/File:' + image_filename\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    # Create a soup object and find the file info table\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.findAll('table', class_= re.compile('fileinfotpl'))\n",
    "    if len(tables) == 0:\n",
    "        output_dict['template_type'] = 'none'\n",
    "        print('No data table')\n",
    "        print()\n",
    "        output_list.append(output_dict)\n",
    "        continue\n",
    "    try:\n",
    "        template_type = str(tables[0]['class'][0]).split('-')[-1]\n",
    "    except:\n",
    "        template_type = 'unknown'\n",
    "    print('Page template:', template_type) # useful types are \"information\" and \"artwork\"\n",
    "    print()\n",
    "    output_dict['template_type'] = template_type\n",
    "        \n",
    "    # stop using separate lists\n",
    "    #with open(template_type + '.json', 'rt') as file_object:\n",
    "    #    field_list = json.loads(file_object.read())\n",
    "    #print(field_list)\n",
    "    \n",
    "    # Step through all of the rows looking for columns with particular labels\n",
    "    rows = tables[0].tbody.findAll('tr', recursive=False) # find only in direct child tr elements\n",
    "    for row in rows:        \n",
    "        columns = row.findAll('td')\n",
    "        if len(columns) > 0:\n",
    "            # In cases where the contents of the td tag are more complext than a simple string (e.g. if the \n",
    "            # contents include text and markup tags), the strings attribute generates a tuple-like iterable\n",
    "            # of strings included inside the tag (a \"generator\"). The first iterable string is always the one we want\n",
    "            string_list = []\n",
    "            field_matched = False\n",
    "            strings = columns[0].strings\n",
    "            for string in strings:\n",
    "                string_list.append(string)\n",
    "            if len(string_list) > 0:\n",
    "                field_name = string_list[0]\n",
    "                for field in field_list:\n",
    "                    if field_name.lower() == field:\n",
    "                        # The value of field will remain as the last matched one\n",
    "                        field_matched = True\n",
    "                        break\n",
    "\n",
    "            if columns[1].string:\n",
    "                # Turn the string into a list of one string\n",
    "                value_string = json.dumps([columns[1].string])\n",
    "            else:\n",
    "                # use regex to match any text\n",
    "                my_regex = re.compile(\".*\")\n",
    "                string_list = columns[1].find_all(text=my_regex)\n",
    "                no_newlines_list = [x.strip() for x in string_list if x != '\\n'] # remove newline items from list of strings\n",
    "                clean_string_list = [x.strip() for x in no_newlines_list if x != ''] # remove newline items from list of strings\n",
    "                if len(clean_string_list) == 1:\n",
    "                    # clean string and turn into list of length 1\n",
    "                    value_string = json.dumps([clean_string_list[0].strip()])\n",
    "                else:\n",
    "                    value_string = json.dumps(clean_string_list)\n",
    "            value_string = value_string.strip()\n",
    "                    \n",
    "            if field_matched:\n",
    "                #print('Matched standard field:', field_name)\n",
    "                #print(value_string)\n",
    "                # Insert the found value into the dict for the matched field\n",
    "                output_dict[field] = value_string\n",
    "            else:\n",
    "                #print('Non-standard field:', field_name)\n",
    "                #print(value_string)\n",
    "                nonstandard_fields.append({'filename': image_filename, 'fieldname': field_name, 'value': value_string})\n",
    "            #print()\n",
    "    sleep(server_sleep) # Don't hit the web server too fast\n",
    "    #print(json.dumps(output_dict, indent = 2))\n",
    "    output_list.append(output_dict)\n",
    "    #print('-----------------')\n",
    "            \n",
    "#print(json.dumps(nonstandard_fields, indent = 2))\n",
    "write_dicts_to_csv(nonstandard_fields, 'nonstandard_fields.csv', ['filename', 'fieldname', 'value'])\n",
    "write_dicts_to_csv(output_list, 'templated_data.csv', output_fields)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Look for a Wikidata link on the image page\n",
    "# ---------------------------\n",
    "\n",
    "# For references on art in Wikidata, see https://www.wikidata.org/wiki/Wikidata:WikiProject_sum_of_all_paintings\n",
    "# https://www.wikidata.org/wiki/Wikidata:WikiProject_Visual_arts/Item_structure\n",
    "\n",
    "file_path = '../../vandycite/act/processed_lists/add_to_wikidata.csv'\n",
    "file_data = read_dict(file_path)\n",
    "\n",
    "output_list = []\n",
    "#if True:\n",
    "for record in file_data[860:]:\n",
    "    print(record['filename'])\n",
    "\n",
    "    # Retrieve the page HTML\n",
    "    image_filename = record['filename']\n",
    "    # image_filename = 'Christ sur la mer de Galilée (Delacroix) Walters Art Museum 37.186.jpg'\n",
    "    # image_filename = 'Christ_and_SocratesSAAM_1974.28.341A_B_1.jpg'\n",
    "    page_url = 'https://commons.wikimedia.org/wiki/File:' + image_filename\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    # Create a soup object and find the file info table\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.findAll('table', class_= re.compile('fileinfotpl'))\n",
    "    if len(tables) > 0:\n",
    "\n",
    "        # Have to check for this span because there are other subtables with a tags besides the one at the top\n",
    "        span = tables[0].findAll('span', id = 'artwork')\n",
    "        if len(span) > 0:\n",
    "            # The link to the Wikidata item will be in an href\n",
    "            anchors = span[0].findAll('a', href = re.compile('https://www.wikidata.org/wiki/'))\n",
    "            if len(anchors) > 0:\n",
    "                link = anchors[0]['href']\n",
    "                qid = extract_localname(link)\n",
    "\n",
    "                retrieved_data = {'act_id': record['RecordNumber'], 'qid': qid, 'filename': record['filename']}\n",
    "                # print(retrieved_data)\n",
    "                output_list.append(retrieved_data)\n",
    "    sleep(read_api_sleep) # Don't hit the API too fast\n",
    "    \n",
    "fieldnames = ['act_id', 'qid', 'filename']\n",
    "write_dicts_to_csv(output_list, 'wikidata_found.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = ['act_id', 'qid', 'filename']\n",
    "write_dicts_to_csv(output_list, 'wikidata_found2.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
