{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Materialize entailed Wikidata triples and federated queries using Python (2020-11-28)\n",
    "\n",
    "(c) 2020 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "\n",
    "Author: Steve Baskauf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set configuration and define functions\n",
    "\n",
    "Run the first cell before any of the others\n",
    "\n",
    "These queries run on a local installation of Apache Jena Fuseki (<https://jena.apache.org/documentation/fuseki2/>) operating as a localhost server on port 3030 (the default). After downloading the .zip file, uncompress it somewhere where you can find it (like your home folder). To start the server from the console (Terminal on Mac, Command Prompt on Windows), change to the directory `apache-jena-fuseki-...` that you unzipped and execute the start command. Alternatively, you can add the directory to your system path and start the server from anywhere.\n",
    "\n",
    "NOTE: it is important that Fuseki be started with the `--update` option, otherwise SPARQL UPDATE operations are disabled. The syntax to start Fuseki from the command line is:\n",
    "\n",
    "```\n",
    "java -Xmx1200M -jar fuseki-server.jar --update --loc=dataDir /myDataset\n",
    "```\n",
    "\n",
    "If you don't care about where the data are stored and you want to set the dataset name when you upload data, you can use:\n",
    "\n",
    "```\n",
    "java -Xmx1200M -jar fuseki-server.jar --update\n",
    "```\n",
    "\n",
    "For details on using SPARQL UPDATE with Fuseki, see Bob DuCharme's blog post: <http://www.bobdc.com/blog/getting-started-with-sparql-up/>\n",
    "\n",
    "The dataset name needs to be set at the beginning of the first cell of the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize entailed Wikidata triples and federated queries using Python (2020-11-28)\n",
    "# (c) 2020 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "    \n",
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import sys # Read CLI arguments\n",
    "\n",
    "# Configure\n",
    "\n",
    "# If not screening by Q ID, set item_values_path to empty string ''\n",
    "graph_data = [\n",
    "    { # 0=Divinity journals\n",
    "        'item_values_path': '../../vandycite/journals/journal-div-qids.csv', \n",
    "        'iri': 'http://journals', \n",
    "        'item_screen_graph_pattern': ''\n",
    "    },\n",
    "    { # 1=Bluffton University presidents\n",
    "        'item_values_path': '', \n",
    "        'iri': 'http://bluffton', \n",
    "        'item_screen_graph_pattern': '''  ?statement1 ps:P108  wd:Q886141 .\n",
    "  ?qid p:P108 ?statement1.\n",
    "  ?statement2 ps:P39 wd:Q61061.\n",
    "  ?qid p:P39 ?statement2.\n",
    "'''\n",
    "    },\n",
    "    { # 2=Fine Arts Gallery works (explicitly listed)\n",
    "        'item_values_path': '../../vandycite/gallery_works/gallery_works_to_write.csv', \n",
    "        'iri': 'http://gallery', \n",
    "        'item_screen_graph_pattern': ''\n",
    "    },\n",
    "    { # 3=people who work at Bluffton (defined by query)\n",
    "        'item_values_path': '', \n",
    "        'iri': 'http://bluffton', \n",
    "        'item_screen_graph_pattern': '''  ?statement1 ps:P108  wd:Q886141 .\n",
    "  ?qid p:P108 ?statement1.\n",
    "'''\n",
    "    },\n",
    "    { # 4=people affiliated with a Vanderbilt unit (defined by query)\n",
    "        'item_values_path': '', \n",
    "        'iri': 'http://researchers', \n",
    "        'item_screen_graph_pattern': '''  ?unit wdt:P749+ wd:Q29052.\n",
    "  ?qid wdt:P1416 ?unit.\n",
    "'''\n",
    "    },\n",
    "    { # 5=people affiliated with a Vanderbilt unit (defined by list)\n",
    "        'item_values_path': '../../vandycite/researchers/vanderbilt-employees.csv', \n",
    "        'iri': 'http://researchers', \n",
    "        'item_screen_graph_pattern': ''\n",
    "    }\n",
    "]\n",
    "\n",
    "query_data = [\n",
    "    { # Query 0: Labels in Wikidata that aren't in the local data\n",
    "        'variables': [\n",
    "                'qid',\n",
    "                'name'\n",
    "            ],\n",
    "        'subquery': '''  ?qid rdfs:label ?name.\n",
    "''',\n",
    "        'local_minus': True,\n",
    "        'order_by': 'qid'\n",
    "    },\n",
    "    { # Query 1: Statements made in Wikidata that weren't made in the local data\n",
    "        'variables': [\n",
    "                'qid',\n",
    "                'name',\n",
    "                'wdt',\n",
    "                'value'\n",
    "            ],\n",
    "        'subquery': '''  ?qid rdfs:label ?name.\n",
    "  filter(lang(?name) = \"en\")\n",
    "  ?qid ?wdt ?value.\n",
    "  filter(substr(str(?wdt),1,37)=\"http://www.wikidata.org/prop/direct/P\")\n",
    "''',\n",
    "        'local_minus': True,\n",
    "        'order_by': 'qid'\n",
    "    },\n",
    "    { # Query 2: References in the local data that aren't in Wikidata\n",
    "        'variables': [\n",
    "                'qid',\n",
    "                'name',\n",
    "                'pr',\n",
    "                'value'\n",
    "            ],\n",
    "        'subquery': '''  ?qid rdfs:label ?name.\n",
    "  filter(lang(?name) = \"en\")\n",
    "  ?qid ?p ?statement.\n",
    "  ?statement prov:wasDerivedFrom ?reference.\n",
    "  ?reference ?pr ?value.\n",
    "  filter(substr(str(?pr),1,40)=\"http://www.wikidata.org/prop/reference/P\")\n",
    "''',\n",
    "        'local_minus': False,\n",
    "        'order_by': 'qid'\n",
    "    },\n",
    "    { # Query 3: English labels in the local data that aren't in Wikidata\n",
    "        'variables': [\n",
    "                'qid',\n",
    "                'name'\n",
    "            ],\n",
    "        'subquery': '''  ?qid rdfs:label ?name.\n",
    "  filter(lang(?name) = \"en\")\n",
    "''',\n",
    "        'local_minus': False,\n",
    "        'order_by': ''\n",
    "    },\n",
    "    { # Query 4: Items and their labels that are in Wikidata but not locally\n",
    "        'variables': [\n",
    "                'qid',\n",
    "                'label'\n",
    "            ],\n",
    "        'subquery': '''  ?qid rdfs:label ?label.\n",
    "  filter(lang(?label) = \"en\")\n",
    "''',\n",
    "        'local_minus': True,\n",
    "        'order_by': 'qid'\n",
    "    },\n",
    "    { # Query 5: People, their names, and the labels of their units that are in Wikidata but not locally\n",
    "        'variables': [\n",
    "                'qid',\n",
    "                'label',\n",
    "                'unitLabel'\n",
    "            ],\n",
    "        'subquery': '''  ?qid rdfs:label ?label.\n",
    "  filter(lang(?label) = \"en\")\n",
    "  ?unit rdfs:label ?unitLabel.\n",
    "  filter(lang(?unitLabel) = \"en\")\n",
    "''',\n",
    "        'local_minus': True,\n",
    "        'order_by': 'qid'\n",
    "    },\n",
    "    { # Query 6: ORCIDs of Vanderbilt people in Wikidata but not locally\n",
    "        'variables': [\n",
    "                'qid',\n",
    "                'label',\n",
    "                'orcid'\n",
    "            ],\n",
    "        'subquery': '''  ?qid rdfs:label ?label.\n",
    "  filter(lang(?label) = \"en\")\n",
    "  ?qid wdt:P496 ?orcid.\n",
    "''',\n",
    "        'local_minus': True,\n",
    "        'order_by': 'qid'\n",
    "    },\n",
    "    { # Query 7: Items that are in Wikidata but not locally\n",
    "        'variables': [\n",
    "                'qid'\n",
    "            ],\n",
    "        'subquery': '',\n",
    "        'local_minus': True,\n",
    "        'order_by': 'qid'\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset_name = \"data\"\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "sparql_endpoint = 'http://localhost:3030/' + dataset_name + '/query'\n",
    "update_endpoint = 'http://localhost:3030/' + dataset_name + '/update'\n",
    "remote_endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "namespaces = '''\n",
    "prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "prefix prov: <http://www.w3.org/ns/prov#>\n",
    "prefix wikibase: <http://wikiba.se/ontology#>\n",
    "prefix  wd:  <http://www.wikidata.org/entity/>\n",
    "prefix  wdt: <http://www.wikidata.org/prop/direct/>\n",
    "prefix  p:  <http://www.wikidata.org/prop/>\n",
    "prefix  pq:  <http://www.wikidata.org/prop/qualifier/>\n",
    "prefix  pr:  <http://www.wikidata.org/prop/reference/>\n",
    "prefix  ps:  <http://www.wikidata.org/prop/statement/>\n",
    "prefix  pqv:  <http://www.wikidata.org/prop/qualifier/value/>\n",
    "prefix  prv:  <http://www.wikidata.org/prop/reference/value/>\n",
    "prefix  psv:  <http://www.wikidata.org/prop/statement/value/>\n",
    "'''\n",
    "\n",
    "value_types = [\n",
    "    {'string': 'time', \n",
    "     'local_names': ['timeValue'], \n",
    "     'datatype':'http://www.w3.org/2001/XMLSchema#dateTime',\n",
    "     'bind': '?literal0'}, \n",
    "    {'string': 'quantity', \n",
    "     'local_names': ['quantityAmount'],\n",
    "     'datatype': 'http://www.w3.org/2001/XMLSchema#decimal',\n",
    "     'bind': '?literal0'}, \n",
    "    {'string': 'globecoordinate', \n",
    "     'local_names': ['geoLatitude', 'geoLongitude'],\n",
    "     'datatype': 'http://www.opengis.net/ont/geosparql#wktLiteral',\n",
    "     'bind': 'concat(\"Point(\", str(?literal0), \" \", str(?literal1), \")\")'}\n",
    "]\n",
    "\n",
    "property_types = ['statement', 'qualifier', 'reference']\n",
    "\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'TestBot/0.1 (mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "def generate_update_header_dictionary():\n",
    "    user_agent_header = 'TestBot/0.1 (mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Content-Type': 'application/sparql-update',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "updateheader = generate_update_header_dictionary()\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts a property pNumber from a Wikidata IRI\n",
    "def extract_pnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[len(pieces) -1]\n",
    "\n",
    "# lookes up a label from a list of dict containing pids and labels\n",
    "def find_label(pid, property_labels):\n",
    "    found_label = ''\n",
    "    for label in property_labels:\n",
    "        if label['pid'] == pid:\n",
    "            found_label = label['label']\n",
    "            break\n",
    "    return found_label\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def load_qids(item_values_path):\n",
    "    # Load item data from csv\n",
    "    print('loading item data from file')\n",
    "    filename = item_values_path\n",
    "    items = read_dict(filename)\n",
    "    print('done loading')\n",
    "    print()\n",
    "\n",
    "    # Create VALUES list for items\n",
    "    item_qids = ''\n",
    "    for item in items:\n",
    "        item_qids += 'wd:' + item['qid'] + '\\n'\n",
    "    # remove trailing newline\n",
    "    item_qids = item_qids[:len(item_qids)-1]\n",
    "\n",
    "    item_values_list = '''\n",
    "          VALUES ?qid\n",
    "        {\n",
    "''' + item_qids + '''\n",
    "        }\n",
    "'''\n",
    "    return(item_values_list)\n",
    "\n",
    "def send_query(query, query_type, endpoint, requestheader):\n",
    "    print('querying SPARQL endpoint to acquire item metadata')\n",
    "    response = requests.post(endpoint, data=query, headers=requestheader)\n",
    "    #print(response.text)\n",
    "    print('done retrieving data')\n",
    "    \n",
    "    if query_type == 'select':\n",
    "        data = response.json()\n",
    "        # extract the values from the response JSON\n",
    "        results = data['results']['bindings']\n",
    "        #print(json.dumps(results, indent=2))\n",
    "    if query_type == 'construct':\n",
    "        data = response.text        \n",
    "    else:\n",
    "        data = response.text\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "def send_update(query, endpoint, requestheader):\n",
    "    #print('sending SPARQL update')\n",
    "    response = requests.post(endpoint, headers=requestheader, data = query)\n",
    "\n",
    "    # nothing is returned from the operation\n",
    "    #print(response.text)\n",
    "    print('update complete')\n",
    "\n",
    "def federated_query(query_number, query_data, graph_number, graph_data):\n",
    "    # build the select string\n",
    "    select_string = ''\n",
    "    for variable in query_data[query_number]['variables']:\n",
    "        select_string += '?' + variable + ' '\n",
    "\n",
    "    # set the direction of the comparison\n",
    "    if query_data[query_number]['local_minus']:\n",
    "        first_service = 'SERVICE <' + remote_endpoint + '> '\n",
    "        minus_service = 'GRAPH <' + graph_data[graph_number]['iri'] + '> '\n",
    "    else:\n",
    "        first_service = 'GRAPH <' + graph_data[graph_number]['iri'] + '> '\n",
    "        minus_service = 'SERVICE <' + remote_endpoint + '> '\n",
    "\n",
    "    # load the Q IDs if any\n",
    "    if graph_data[graph_number]['item_values_path'] == '':\n",
    "        item_values_list = ''\n",
    "    else:\n",
    "        item_values_list = load_qids(graph_data[graph_number]['item_values_path'])\n",
    "\n",
    "    query = '''\n",
    "    SELECT distinct ''' + select_string + '''\n",
    "    WHERE {\n",
    "    ''' + first_service + '{' + item_values_list + graph_data[graph_number]['item_screen_graph_pattern'] + query_data[query_number]['subquery'] + '''}\n",
    "      minus\n",
    "      {\n",
    "    ''' + minus_service + '{' + item_values_list + graph_data[graph_number]['item_screen_graph_pattern'] + query_data[query_number]['subquery'] + '''}\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "    if query_data[query_number]['order_by'] != '':\n",
    "        query += 'ORDER BY ?' + query_data[query_number]['order_by'] + '''\n",
    "    '''\n",
    "\n",
    "    #print(query)\n",
    "\n",
    "    # ----------------\n",
    "    # send request to Fuseki endpoint\n",
    "    # ----------------\n",
    "    text = send_query(namespaces + query, query.split(' ')[0].lower(), sparql_endpoint, requestheader)\n",
    "    #print(text)\n",
    "\n",
    "    data = json.loads(text)\n",
    "    # extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    #print(json.dumps(results, indent=2))\n",
    "\n",
    "    # ----------------\n",
    "    # extract results\n",
    "    # ----------------\n",
    "\n",
    "    fieldnames = list(query_data[query_number]['variables'])\n",
    "    metadata_list = []\n",
    "    for result in results:\n",
    "        row_dict = {}\n",
    "        for variable in query_data[query_number]['variables']:\n",
    "            if result[variable]['type'] == 'uri':\n",
    "                if result[variable]['value'][:32] == 'http://www.wikidata.org/entity/Q':\n",
    "                    row_dict[variable] = extract_qnumber(result[variable]['value'])\n",
    "                elif result[variable]['value'][:29] == 'http://www.wikidata.org/prop/':\n",
    "                    row_dict[variable] = extract_pnumber(result[variable]['value'])                \n",
    "                else:\n",
    "                    row_dict[variable] = result[variable]['value']\n",
    "            else:\n",
    "                row_dict[variable] = result[variable]['value']\n",
    "                if 'xml:lang' in result[variable]:\n",
    "                    row_dict[variable + '_lang'] = result[variable]['xml:lang']\n",
    "                    if variable + '_lang' not in fieldnames:\n",
    "                        fieldnames.append(variable + '_lang')\n",
    "                if 'datatype' in result[variable]:\n",
    "                    row_dict[variable + '_datatype'] = result[variable]['datatype'].split('#')[1]\n",
    "                    if variable + '_datatype' not in fieldnames:\n",
    "                        fieldnames.append(variable + '_datatype')\n",
    "        metadata_list.append(row_dict)\n",
    "\n",
    "    #print(json.dumps(metadata_list, indent=2))\n",
    "    return metadata_list, fieldnames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities for setting up triplestore for querying\n",
    "\n",
    "The first cell clears all graphs from the dataset. \n",
    "\n",
    "After clearing the graphs, Wikidata triples are generated by Ruby gem `rdf-tabular` (<https://github.com/ruby-rdf/rdf-tabular>) according to the W3C \"Generating RDF from Tabular Data on the Web\" Recommendation (<https://www.w3.org/TR/csv2rdf/>). The command to generate the output serialized as RDF/Turtle and redirected to a file is:\n",
    "\n",
    "```\n",
    "rdf serialize --input-format tabular --output-format ttl --metadata csv-metadata.json --minimal > output.ttl\n",
    "```\n",
    "\n",
    "The resulting file is loaded into Fuseki. The second cell then generates the missing triples for expressing the data that are available via the Wikidata Query Service, but that aren't generated directly by rdf-tabular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!! Warning! Warning! Warning! !!!!!!!!!!\n",
    "# This command deletes all triples in the triplestore! There is no way to recover the data, so use with caution!\n",
    "query = 'drop all'\n",
    "#query = 'drop graph <http://bluffton>'\n",
    "\n",
    "# update test\n",
    "data = send_update(query, update_endpoint, updateheader)\n",
    "# Fuseki doesn't send any response for update commands\n",
    "#print('response:\\n', data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the missing value statements using values from value nodes\n",
    "\n",
    "# NOTE: The datatypes for xsd:dateTime and xsd:decimal should be generated automatically by the datatype \n",
    "# designation in the csv-metadata.json schema (for integers, a \".0\" is appended to integers to make them decimals).\n",
    "# However, I don't know how to force the geo:wktLiteral datatype in SPARQL construct, so this may end up not matching\n",
    "# the datatype of literals acquired from the Wikidata Query service.\n",
    "\n",
    "graph_number = 2 # set the appropriate graph number for the graph to be supplemented\n",
    "\n",
    "for value_type in value_types:\n",
    "    for property_type in property_types:\n",
    "        query = '''\n",
    "        with <''' + graph_data[graph_number]['iri'] + '''>\n",
    "        insert {?reference ?directProp ?literal.}\n",
    "        where {\n",
    "          ?reference ?pxv ?value.\n",
    "        '''\n",
    "        for ln_index in range(len(value_type['local_names'])):\n",
    "            query += '  ?value wikibase:' + value_type['local_names'][ln_index] + ' ?literal' + str(ln_index) + '''.\n",
    "        '''\n",
    "        query += '  bind(' + value_type['bind'] + ''' as ?literal)\n",
    "        '''\n",
    "        query += '  filter(substr(str(?pxv),1,45)=\"http://www.wikidata.org/prop/' + property_type + '''/value/\")\n",
    "          bind(substr(str(?pxv),46) as ?id)\n",
    "          bind(iri(concat(\"http://www.wikidata.org/prop/''' + property_type + '''/\", ?id)) as ?directProp)\n",
    "          }\n",
    "          '''\n",
    "        #print(query) \n",
    "        print('updating', property_type, value_type['string'])\n",
    "        send_update(namespaces + query, update_endpoint, updateheader)\n",
    "\n",
    "# Insert the missing \"truthy\" statements from statement value statements\n",
    "\n",
    "query = '''\n",
    "with <''' + graph_data[graph_number]['iri'] + '''>\n",
    "insert {?item ?truthyProp ?value.}\n",
    "where {\n",
    "  ?item ?p ?statement.\n",
    "  ?statement ?ps ?value.\n",
    "  filter(substr(str(?ps),1,40)=\"http://www.wikidata.org/prop/statement/P\")\n",
    "  bind(substr(str(?ps),40) as ?id)\n",
    "  bind(iri(concat(\"http://www.wikidata.org/prop/direct/\", ?id)) as ?truthyProp)\n",
    "  }\n",
    "  '''\n",
    "#print(query)\n",
    "print ('updating truthy statements')\n",
    "send_update(namespaces + query, update_endpoint, updateheader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated queries\n",
    "\n",
    "The following queries compare the local data in Fuseki against the \"real\" data in Wikidata by carrying out a federated query using the local data and data from the Wikidata Query Service.\n",
    "\n",
    "The first cell performs a query (`subquery`) on both the local data and the data in Wikidata, and performs a minus operation to determine what bindings are present on Wikidata but not in the local dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell does the generic querying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the graph and query number (see config section)\n",
    "graph_number = 2 # gallery data\n",
    "query_number = 0 # items in Wikidata but not locally\n",
    "\n",
    "metadata_list, fieldnames = federated_query(query_number, query_data, graph_number, graph_data)\n",
    "\n",
    "if len(metadata_list) == 0:\n",
    "    print('no data retrieved')\n",
    "else:\n",
    "    write_dicts_to_csv(metadata_list, 'federated_test.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special cell for eliminating the bad quantity results from the gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_number = 2 # gallery data (defined by list)\n",
    "query_number = 7 # statements in Wikidata but not locally\n",
    "\n",
    "property_labels = read_dict('property_labels.csv')\n",
    "\n",
    "metadata_list, fieldnames = federated_query(query_number, query_data, graph_number, graph_data)\n",
    "fieldnames = ['qid', 'name', 'name_lang', 'wdt', 'wdt_label', 'value', 'value_datatype']\n",
    "\n",
    "if len(metadata_list) == 0:\n",
    "    print('no data retrieved')\n",
    "else:\n",
    "    # screen out the bad quantity results\n",
    "    output = []\n",
    "    for line in metadata_list:\n",
    "        try:\n",
    "            number = float(line['value'])\n",
    "            if line['value_datatype'] != 'decimal' or number%1 != 0: # check for nothing after the decimal\n",
    "                line['wdt_label'] = find_label(line['wdt'], property_labels)\n",
    "                output.append(line)\n",
    "        except: # if the value isn't a number, then append the record\n",
    "            line['wdt_label'] = find_label(line['wdt'], property_labels)\n",
    "            output.append(line)\n",
    "    \n",
    "    write_dicts_to_csv(output, '../../vandycite/gallery_works/statements_not_locally.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the graph and query number (see config section)\n",
    "graph_number = 4 # people affiliate with Vanderbilt departments (defined by query)\n",
    "query_number = 5 # people and their department names, in Wikidata but not locally\n",
    "# NOTE: this not only gets new people, but also people who have new unit affiliations\n",
    "\n",
    "metadata_list, fieldnames = federated_query(query_number, query_data, graph_number, graph_data)\n",
    "\n",
    "if len(metadata_list) == 0:\n",
    "    print('no data retrieved')\n",
    "else:\n",
    "    write_dicts_to_csv(metadata_list, '../../vandycite/researchers/researchers_not_locally.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the graph and query number (see config section)\n",
    "graph_number = 5 # people affiliate with Vanderbilt departments (defined by list)\n",
    "query_number = 6 # people and their ORCIDs that are in Wikidata but not locally\n",
    "\n",
    "metadata_list, fieldnames = federated_query(query_number, query_data, graph_number, graph_data)\n",
    "\n",
    "if len(metadata_list) == 0:\n",
    "    print('no data retrieved')\n",
    "else:\n",
    "    write_dicts_to_csv(metadata_list, '../../vandycite/researchers/orcids_not_locally.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "Left for historical purposes and future development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to insert a single triple into the default graph\n",
    "\n",
    "query = '''\n",
    "insert {?s ?p ?o}\n",
    "where {\n",
    "bind(uri(\"http://test_subject\") as ?s)\n",
    "bind(uri(\"http://test_predicate\") as ?p)\n",
    "bind(uri(\"http://test_object\") as ?o)\n",
    "}\n",
    "'''\n",
    "\n",
    "# update test\n",
    "data = send_update(query, update_endpoint, updateheader)\n",
    "# Fuseki doesn't send any response for update commands\n",
    "#print('response:\\n', data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test to construct missing value statements using value node values.\n",
    "\n",
    "# NOTE: The datatypes for xsd:dateTime and xsd:decimal should be generated automatically by the datatype \n",
    "# designation in the csv-metadata.json schema (for integers, a \".0\" is appended to integers to make them decimals).\n",
    "# However, I don't know how to force the geo:wktLiteral datatype in SPARQL construct, so this may end up not matching\n",
    "# the datatype of literals acquired from the Wikidata Query service.\n",
    "\n",
    "value_type = value_types[2]\n",
    "property_type = property_types[0]\n",
    "\n",
    "query = '''\n",
    "construct {?reference ?directProp ?literal.}\n",
    "where {\n",
    "  ?reference ?pxv ?value.\n",
    "'''\n",
    "for ln_index in range(len(value_type['local_names'])):\n",
    "    query += '  ?value wikibase:' + value_type['local_names'][ln_index] + ' ?literal' + str(ln_index) + '''.\n",
    "'''\n",
    "query += '  bind(' + value_type['bind'] + ''' as ?literal)\n",
    "'''\n",
    "query += '  filter(substr(str(?pxv),1,45)=\"http://www.wikidata.org/prop/' + property_type + '''/value/\")\n",
    "  bind(substr(str(?pxv),46) as ?id)\n",
    "  bind(iri(concat(\"http://www.wikidata.org/prop/''' + property_type + '''/\", ?id)) as ?directProp)\n",
    "  }\n",
    "  '''\n",
    "print(query) \n",
    "data = send_query(namespaces + query, query.split(' ')[0].lower(), sparql_endpoint, requestheader)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to construct \"truthy\" statements from statement value statements\n",
    "\n",
    "query = '''\n",
    "construct {?item ?truthyProp ?value.}\n",
    "where {\n",
    "  ?item ?p ?statement.\n",
    "  ?statement ?ps ?value.\n",
    "  filter(substr(str(?ps),1,40)=\"http://www.wikidata.org/prop/statement/P\")\n",
    "  bind(substr(str(?ps),40) as ?id)\n",
    "  bind(iri(concat(\"http://www.wikidata.org/prop/direct/\", ?id)) as ?truthyProp)\n",
    "  }\n",
    "  '''\n",
    "print(query) \n",
    "data = send_query(namespaces + query, query.split(' ')[0].lower(), sparql_endpoint, requestheader)\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
