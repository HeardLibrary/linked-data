{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the images to a local directory\n",
    "\n",
    "NOTE: this only needs to be done once for a given set of images. Once they are loaded into the bucket it doesn't need to be run again.\n",
    "\n",
    "This code uses a list of accession numbers (found as a column in a CSV file) to generate IIIF Image API (v2) URLs for JPEG images that are 1000 pixels in the shortest dimension, then download them into a local directory.\n",
    "\n",
    "After generating and downloading the images, they need to be uploaded to the Google Cloud bucket used in the Vision analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google_cloud_vision.ipynb, a Jupyter notebook for analyzing images using the Google Cloud Vision API\n",
    "version = '0.1.0'\n",
    "created = '2023-03-27'\n",
    "\n",
    "# (c) 2023 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "# For more information, see https://github.com/HeardLibrary/linked-data/tree/master/image_analysis\n",
    "\n",
    "# This script carries out three major tasks:\n",
    "# 1. It downloads images from the Vanderbilt University Libraries IIIF server at a standard resolution of 1000 pixels in \n",
    "# the shortest dimension. If the image is already smaller than 1000 pixels in the shortest dimension, then the image is\n",
    "# downloaded at the original resolution.\n",
    "# 2. It analyzes the images using the Google Cloud Vision API using the FACE_DETECTION, LABEL_DETECTION, OBJECT_LOCALIZATION, and\n",
    "# TEXT_DETECTION features. The results are saved to CSV files.\n",
    "# 3. Optionally, it can generate an IIIF annotation file for each image that can be used to display the results of the\n",
    "# object localization analysis of the image.\n",
    "\n",
    "# -----------------------------------------\n",
    "# Version 0.1.0 change notes (2023-03-27):\n",
    "# - Initial version\n",
    "# -----------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shutil # high-level file operations\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image data into a dataframe\n",
    "base_path = '/Users/baskausj/github/vandycite/gallery_buchanan/image_analysis/'\n",
    "download_path = '/Users/baskausj/Downloads/'\n",
    "\n",
    "# Load the source image data into a dataframe\n",
    "source_image_dataframe = pd.read_csv(base_path + 'combined_images.csv', dtype=str)\n",
    "# Set the commons_id column as the index\n",
    "source_image_dataframe = source_image_dataframe.set_index('commons_id')\n",
    "\n",
    "#source_image_dataframe.head()\n",
    "\n",
    "# Import CSV data as a dataframe.\n",
    "accession_dataframe = pd.read_csv(base_path + 'accession_numbers_to_analyze.csv', dtype=str)\n",
    "\n",
    "# Test with a single row\n",
    "#accession_dataframe = accession_dataframe.head(1)\n",
    "\n",
    "\n",
    "# Create a dataframe to hold the accession numbers and dimensions\n",
    "accession_dimensions_dataframe = pd.DataFrame(columns=['accession_number', 'height', 'width'])\n",
    "\n",
    "bad_image_list = []\n",
    "\n",
    "# Loop through the dataframe rows and download the images.\n",
    "for index, row in accession_dataframe.iterrows():\n",
    "    accession_number = row['accession_number']\n",
    "    print(accession_number)\n",
    "    \n",
    "    # Look up the image data in the source image dataframe.\n",
    "    # In cases where there are two images, we want the primary image.\n",
    "    image_series = source_image_dataframe.loc[(source_image_dataframe['accession_number'] == accession_number) & (source_image_dataframe['rank'] == 'primary')]\n",
    "    manifest_url = image_series['iiif_manifest'][0]\n",
    "\n",
    "    # get the manifest from the manifest url\n",
    "    manifest = requests.get(manifest_url).json()\n",
    "    #print(json.dumps(manifest, indent=2))\n",
    "    service_url = manifest['sequences'][0]['canvases'][0]['images'][0]['resource']['service']['@id']\n",
    "    # Because of the error in original manifests, replace version 3 with version 2 in the URL.\n",
    "    #service_url = service_url.replace('/3/', '/2/') # This is no longer needed because the manifests have been fixed.\n",
    "    #print('service_url', service_url)\n",
    "\n",
    "    # Determine the maximum and minimum dimensions of the image.\n",
    "    height = image_series['height'][0]\n",
    "    #print('height', height)\n",
    "    width = image_series['width'][0]\n",
    "    #print('width', width)\n",
    "    shortest_dimension = min(int(height), int(width))\n",
    "    longest_dimension = max(int(height), int(width))\n",
    "    #print('shortest_dimension', shortest_dimension)\n",
    "\n",
    "    # We want to know what the largest dimension needs to be for the shortest dimension to be 1000 pixels.\n",
    "    # If that calculation makes the longest dimension longer than the actual longest dimension, \n",
    "    # then we want to use the actual longest dimension.\n",
    "    # If the shortest dimension is already less than 1000 pixels, then we will just use the longest dimension as is.\n",
    "    if shortest_dimension > 1000:\n",
    "        size = int(1000 * (longest_dimension / shortest_dimension))\n",
    "        if size > longest_dimension:\n",
    "            size = longest_dimension\n",
    "    else:\n",
    "        size = longest_dimension\n",
    "    #print('size', size)\n",
    "\n",
    "    # construct the image url using the \"!\" size option, that keeps the aspect ratio but sizes to the maximum dimension.\n",
    "    image_url = service_url + '/full/!' + str(size) + ',' + str(size) + '/0/default.jpg'\n",
    "    print('image_url', image_url)\n",
    "    print()\n",
    "        \n",
    "    # retrieve the image from the IIIF server\n",
    "    image_object = requests.get(image_url, stream=True).raw\n",
    "\n",
    "    # Save the image to a file\n",
    "    with open(download_path + 'google_vision_images/' + accession_number + '.jpg', 'wb') as out_file:\n",
    "        shutil.copyfileobj(image_object, out_file)\n",
    "        # Force the file to be written to disk\n",
    "        out_file.flush()\n",
    "    \n",
    "    # Find the image dimensions\n",
    "    # Open the image file from disk\n",
    "    with open(download_path + 'google_vision_images/' + accession_number + '.jpg', 'rb') as image_file:\n",
    "        reduced_width = 0\n",
    "        reduced_height = 0\n",
    "        try:\n",
    "            image = Image.open(image_file)\n",
    "            reduced_width, reduced_height = image.size\n",
    "            #print('reduced_width', reduced_width)\n",
    "            #print('reduced_height', reduced_height)\n",
    "        except:\n",
    "            print('bad image')\n",
    "            bad_image_list.append(accession_number)\n",
    "            continue\n",
    "    \n",
    "    # Display the image\n",
    "    #image.show()\n",
    "\n",
    "    # Add the accession number and dimensions to the dataframe\n",
    "    accession_dimensions_dataframe = accession_dimensions_dataframe.append({'accession_number': accession_number, 'max_height': height, 'max_width': width, 'height': reduced_height, 'width': reduced_width}, ignore_index=True)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "accession_dimensions_dataframe.to_csv(base_path + 'accession_dimensions.csv', index=False)\n",
    "\n",
    "print('bad_image_list', bad_image_list)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Vision image analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is part of google_cloud_vision.ipynb\n",
    "# For licensing and other information, see https://github.com/HeardLibrary/linked-data/tree/master/image_analysis\n",
    "\n",
    "# Here's the landing page for Google Cloud Vision\n",
    "# https://cloud.google.com/vision/\n",
    "# From it you can try the api by dragging and dropping an image into the browser. You can then \n",
    "# view the JSON response, which was helpful at first to understand the structure of the response.\n",
    "\n",
    "# The following tutorial contains critical information about enabling the API and creating a role\n",
    "# for the service account to allow it access. This is followed by creating a service account key.\n",
    "# https://cloud.google.com/vision/docs/detect-labels-image-client-libraries\n",
    "\n",
    "# I didn't actually do this tutorial, but it was useful to understand the order of operations that\n",
    "# needed to be done prior to writing to the API.\n",
    "# https://www.cloudskillsboost.google/focuses/2457?parent=catalog&utm_source=vision&utm_campaign=cloudapi&utm_medium=webpage\n",
    "# Because I'm using the Python client library, the part about setting up the request body was irrelevant. \n",
    "# But the stuff about uploading the files to the bucket, making it publicly accessible, etc. was helpful.\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "# Reference for Google Cloud Vision Python client https://cloud.google.com/python/docs/reference/vision/latest\n",
    "from google.cloud import vision\n",
    "from google.cloud import vision_v1\n",
    "from google.cloud.vision_v1 import AnnotateImageResponse\n",
    "\n",
    "# Import from Google oauth library\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "def extract_object_localization_data(accession_number: str, annotation: List[Dict[str, Any]], width: int, height: int) -> Dict[str, Any]:\n",
    "    \"\"\"Extract the object localization data from a hit and turn it into a dict to be added as a row in the dataframe.\"\"\"\n",
    "    #print('annotation', annotation)\n",
    "    description = annotation['name']\n",
    "    score = annotation['score']\n",
    "    left_x = annotation['boundingPoly']['normalizedVertices'][0]['x']\n",
    "    top_y = annotation['boundingPoly']['normalizedVertices'][0]['y']\n",
    "    right_x = annotation['boundingPoly']['normalizedVertices'][2]['x']\n",
    "    bottom_y = annotation['boundingPoly']['normalizedVertices'][2]['y']\n",
    "    #print('description', description)\n",
    "    #print('score', score)\n",
    "    #print('left_x', left_x)\n",
    "    #print('top_y', top_y)\n",
    "    #print('right_x', right_x)\n",
    "    #print('bottom_y', bottom_y)\n",
    "    #print()\n",
    "\n",
    "    row = {'accession_number': accession_number, 'description': description, 'score': score, 'rel_left_x': left_x, 'rel_right_x': right_x, 'rel_top_y': top_y, 'rel_bottom_y': bottom_y, 'abs_left_x': round(left_x * width), 'abs_right_x': round(right_x * width), 'abs_top_y': round(top_y * height), 'abs_bottom_y': round(bottom_y * height)}\n",
    "    return row\n",
    "\n",
    "def extract_face_detection_data(accession_number: str, annotation: List[Dict[str, Any]], width: int, height: int) -> Dict[str, Any]:\n",
    "    \"\"\"Extract the face detection data from a hit and turn it into a dict to be added as a row in the dataframe.\"\"\"\n",
    "    score = annotation['detectionConfidence']\n",
    "    left_x = annotation['boundingPoly']['vertices'][0]['x']\n",
    "    top_y = annotation['boundingPoly']['vertices'][0]['y']\n",
    "    right_x = annotation['boundingPoly']['vertices'][2]['x']\n",
    "    bottom_y = annotation['boundingPoly']['vertices'][2]['y']\n",
    "    roll_angle = annotation['rollAngle']\n",
    "    pan_angle = annotation['panAngle']\n",
    "    tilt_angle = annotation['tiltAngle']\n",
    "    landmarking_confidence = annotation['landmarkingConfidence']\n",
    "    joy_likelihood = annotation['joyLikelihood']\n",
    "    sorrow_likelihood = annotation['sorrowLikelihood']\n",
    "    anger_likelihood = annotation['angerLikelihood']\n",
    "    surprise_likelihood = annotation['surpriseLikelihood']\n",
    "    under_exposed_likelihood = annotation['underExposedLikelihood']\n",
    "    blurred_likelihood = annotation['blurredLikelihood']\n",
    "    headwear_likelihood = annotation['headwearLikelihood']\n",
    "\n",
    "    row = {'accession_number': accession_number, 'score': score, \n",
    "           'rel_left_x': left_x / width, 'rel_right_x': right_x / width, 'rel_top_y': top_y / height, 'rel_bottom_y': bottom_y /height,\n",
    "           'abs_left_x': left_x, 'abs_right_x': right_x, 'abs_top_y': top_y, 'abs_bottom_y': bottom_y,\n",
    "           'roll_angle': roll_angle, 'pan_angle': pan_angle, 'tilt_angle': tilt_angle,\n",
    "           'landmarking_confidence': landmarking_confidence, 'joy_likelihood': joy_likelihood, \n",
    "           'sorrow_likelihood': sorrow_likelihood, 'anger_likelihood': anger_likelihood, \n",
    "           'surprise_likelihood': surprise_likelihood, 'under_exposed_likelihood': under_exposed_likelihood,\n",
    "           'blurred_likelihood': blurred_likelihood, 'headwear_likelihood': headwear_likelihood}\n",
    "    return row\n",
    "\n",
    "def extract_label_detection_data(accession_number: str, annotation: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Extract the label detection data from a hit and turn it into a dict to be added as a row in the dataframe.\"\"\"\n",
    "    mid = annotation['mid']\n",
    "    description = annotation['description']\n",
    "    score = annotation['score']\n",
    "    topicality = annotation['topicality']\n",
    "    row = {'accession_number': accession_number, 'mid': mid, 'description': description, 'score': score, 'topicality': topicality}\n",
    "    return row\n",
    "\n",
    "def extract_text_detection_data(accession_number: str, annotation: List[Dict[str, Any]], width: int, height: int) -> Dict[str, Any]:\n",
    "    \"\"\"Extract the text detection data from a hit and turn it into a dict to be added as a row in the dataframe.\"\"\"\n",
    "    locale = annotation['locale']\n",
    "    description = annotation['description']\n",
    "    left_x = annotation['boundingPoly']['vertices'][0]['x']\n",
    "    top_y = annotation['boundingPoly']['vertices'][0]['y']\n",
    "    right_x = annotation['boundingPoly']['vertices'][2]['x']\n",
    "    bottom_y = annotation['boundingPoly']['vertices'][2]['y']\n",
    "    row = {'accession_number': accession_number, 'locale': locale, 'description': description, \n",
    "           'rel_left_x': left_x / width, 'rel_right_x': right_x / width, 'rel_top_y': top_y / height, 'rel_bottom_y': bottom_y / height,\n",
    "           'abs_left_x': left_x, 'abs_right_x': right_x, 'abs_top_y': top_y, 'abs_bottom_y': bottom_y,\n",
    "           }\n",
    "    return row\n",
    "\n",
    "# Customize for your own computer\n",
    "user_dir = 'baskausj' # Enter your user directory name here\n",
    "base_path = '/Users/baskausj/github/vandycite/gallery_buchanan/image_analysis/' # Location of the accession number data file\n",
    "annotations_base_url = 'https://baskaufs.github.io/iiif/baskauf/'\n",
    "\n",
    "# Set the path to the service account key\n",
    "key_path = '/Users/' + user_dir + '/image-analysis-376619-193859a33600.json'\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Retrieve the service key, create a credentials object, then use it to authenticate and create a `client` object.\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create a credentials object from the service account key\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "# API documentation https://cloud.google.com/python/docs/reference/vision/latest/google.cloud.vision_v1.services.image_annotator.ImageAnnotatorClient#methods\n",
    "# The first two versions have no arguments and the credentials are loaded from the environment variable.\n",
    "#client = vision.ImageAnnotatorClient()\n",
    "# Used this specific v1 to get the JSON conversion to work\n",
    "#client = vision_v1.ImageAnnotatorClient()\n",
    "# Use this line instead of the one above to load the credentials directly from the file\n",
    "client = vision_v1.ImageAnnotatorClient(credentials=credentials)\n",
    "\n",
    "# Load the source data from a CSV. The critical column needed here is the `accession_number` column, since it is the one \n",
    "# that was used to construct the image file name for the uploaded test images.\n",
    "accession_dataframe = pd.read_csv(base_path + 'accession_dimensions.csv', dtype=str)\n",
    "#accession_dataframe.head()\n",
    "\n",
    "# -----------------------------------\n",
    "# Loop through all of the accession numbers and perform the analysis on each of the images.\n",
    "# -----------------------------------\n",
    "\n",
    "# Create a new dataframe to hold the annotations\n",
    "object_localization_dataframe = pd.DataFrame(columns=['accession_number', 'description', 'score', 'rel_left_x', 'rel_right_x', 'rel_top_y', 'rel_bottom_y', 'abs_left_x', 'abs_right_x', 'abs_top_y', 'abs_bottom_y'])\n",
    "face_detection_dataframe = pd.DataFrame(columns=['accession_number', 'score', 'rel_left_x', 'rel_right_x', 'rel_top_y', 'rel_bottom_y', 'abs_left_x', 'abs_right_x', 'abs_top_y', 'abs_bottom_y', 'roll_angle', 'pan_angle', 'tilt_angle', 'landmarking, confidence', 'joy_likelihood', 'sorrow_likelihood', 'anger_likelihood', 'surprise_likelihood', 'under_exposed_likelihood', 'blurred_likelihood', 'headwear_likelihood'])\n",
    "label_detection_dataframe = pd.DataFrame(columns=['accession_number', 'mid', 'description', 'score', 'topicality'])\n",
    "text_detection_dataframe = pd.DataFrame(columns=['accession_number', 'locale', 'description', 'rel_left_x', 'rel_right_x', 'rel_top_y', 'rel_bottom_y', 'abs_left_x', 'abs_right_x', 'abs_top_y', 'abs_bottom_y'])\n",
    "\n",
    "# Loop through the dataframe rows and analyze the images.\n",
    "for index, row in accession_dataframe.iterrows():\n",
    "    accession_number = row['accession_number']\n",
    "    print('accession_number', accession_number)\n",
    "    width = int(row['width'])\n",
    "    height = int(row['height'])\n",
    "\n",
    "    # To access the images, they should be stored in a Google Cloud Storage bucket that is set up for public access.\n",
    "    # It's also possible to use a publicly accessible URL, but that seems to be unreliable.\n",
    "    # The storage costs for a few images are negligible.\n",
    "\n",
    "    # Construct the path to the image file\n",
    "    image_uri = 'gs://vu-gallery/' + accession_number + '.jpg'\n",
    "    #print('image_uri', image_uri)\n",
    "    \n",
    "    # Here is the API documentation for the Feature object.\n",
    "    # https://cloud.google.com/vision/docs/reference/rest/v1/Feature\n",
    "    #analysis_type = vision.Feature.Type.FACE_DETECTION\n",
    "    #analysis_type = vision.Feature.Type.LABEL_DETECTION\n",
    "    #analysis_type = vision.Feature.Type.OBJECT_LOCALIZATION\n",
    "\n",
    "    # This API documentation isn't exactly the one for the .annotate_image method, but it's close enough.\n",
    "    # https://cloud.google.com/vision/docs/reference/rest/v1/projects.images/annotate\n",
    "    # In particular, it links to the AnnotateImageRequest object, which is what we need to pass to the annotate_image method.\n",
    "    response = client.annotate_image({\n",
    "    'image': {'source': {'image_uri': image_uri}},\n",
    "    'features': [\n",
    "        {'type_': vision.Feature.Type.OBJECT_LOCALIZATION},\n",
    "        {'type_': vision.Feature.Type.FACE_DETECTION},\n",
    "        {'type_': vision.Feature.Type.LABEL_DETECTION},\n",
    "        {'type_': vision.Feature.Type.TEXT_DETECTION}  \n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # The API response is a protobuf object, which is not JSON serializable.\n",
    "    # So we need to convert it to a JSON serializable object.\n",
    "    # Solution from https://stackoverflow.com/a/65728119\n",
    "    response_json = AnnotateImageResponse.to_json(response)\n",
    "\n",
    "    # The structure of the response is detailed in the API documentation here:\n",
    "    # https://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse\n",
    "    # The various bits are detailed for each feature type.\n",
    "    # Here's the documentation for entity annotations, with a link to the BoundingPoly object.\n",
    "    # https://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse#EntityAnnotation\n",
    "    response_struct = json.loads(response_json)\n",
    "\n",
    "    # Object localization\n",
    "    # -------------------\n",
    "\n",
    "    for annotation in response_struct['localizedObjectAnnotations']:\n",
    "        row = extract_object_localization_data(accession_number, annotation, width, height)\n",
    "        object_localization_dataframe = object_localization_dataframe.append(row, ignore_index=True)\n",
    "    \n",
    "    # Write the annotations to a CSV file after every image in case the process is interrupted.\n",
    "    object_localization_dataframe.to_csv(base_path + 'object_localization.csv', index=False)\n",
    "    \n",
    "    # Face detection\n",
    "    # --------------\n",
    "    '''\n",
    "    analysis_type = vision.Feature.Type.FACE_DETECTION\n",
    "    response = client.annotate_image({\n",
    "    'image': {'source': {'image_uri': image_uri}},\n",
    "    'features': [{'type_': analysis_type}]\n",
    "    })\n",
    "    response_json = AnnotateImageResponse.to_json(response)\n",
    "    response_struct = json.loads(response_json)\n",
    "    '''\n",
    "    for annotation in response_struct['faceAnnotations']:\n",
    "        row = extract_face_detection_data(accession_number, annotation, width, height)\n",
    "        face_detection_dataframe = face_detection_dataframe.append(row, ignore_index=True)\n",
    "    \n",
    "    # Write the annotations to a CSV file after every image in case the process is interrupted.\n",
    "    face_detection_dataframe.to_csv(base_path + 'face_detection.csv', index=False)\n",
    "    \n",
    "    # Label detection\n",
    "    # ---------------\n",
    "    '''\n",
    "    analysis_type = vision.Feature.Type.LABEL_DETECTION\n",
    "    response = client.annotate_image({\n",
    "    'image': {'source': {'image_uri': image_uri}},\n",
    "    'features': [{'type_': analysis_type}]\n",
    "    })\n",
    "    response_json = AnnotateImageResponse.to_json(response)\n",
    "    response_struct = json.loads(response_json)\n",
    "    # print(json.dumps(response_struct, indent=2))\n",
    "    '''\n",
    "    for annotation in response_struct['labelAnnotations']:\n",
    "        row = extract_label_detection_data(accession_number, annotation)\n",
    "        label_detection_dataframe = label_detection_dataframe.append(row, ignore_index=True)\n",
    "    \n",
    "    # Write the annotations to a CSV file after every image in case the process is interrupted.\n",
    "    label_detection_dataframe.to_csv(base_path + 'label_detection.csv', index=False)\n",
    "    \n",
    "    # Text detection\n",
    "    # --------------\n",
    "    '''\n",
    "    analysis_type = vision.Feature.Type.TEXT_DETECTION\n",
    "    response = client.annotate_image({\n",
    "    'image': {'source': {'image_uri': image_uri}},\n",
    "    'features': [{'type_': analysis_type}]\n",
    "    })\n",
    "    response_json = AnnotateImageResponse.to_json(response)\n",
    "    response_struct = json.loads(response_json)\n",
    "    #print(json.dumps(response_struct, indent=2))\n",
    "    '''\n",
    "    for annotation in response_struct['textAnnotations']:\n",
    "        row = extract_text_detection_data(accession_number, annotation, width, height)\n",
    "        text_detection_dataframe = text_detection_dataframe.append(row, ignore_index=True)\n",
    "\n",
    "    # Write the annotations to a CSV file after every image in case the process is interrupted.\n",
    "    text_detection_dataframe.to_csv(base_path + 'text_detection.csv', index=False)\n",
    "\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create IIIF annotation file\n",
    "\n",
    "To create the annotations, we need to convert the relative dimensions to the absolute pixel dimensions based on the canvas size.\n",
    "\n",
    "The canvas size is given as the dimensions of the full-sized image, which is reported as `max_height` and `max_width` in the dimensions CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is part of google_cloud_vision.ipynb\n",
    "# For licensing and other information, see https://github.com/HeardLibrary/linked-data/tree/master/image_analysis\n",
    "\n",
    "# object_localization.csv contains the results of the object localization analysis\n",
    "object_localization_dataframe = pd.read_csv(base_path + 'object_localization.csv')\n",
    "# accession_dimensions.csv is a temporary file that contains the dimensions of the full-size images (max_height and max_width)\n",
    "# as retrieved from the IIIF manifest\n",
    "accession_dataframe = pd.read_csv(base_path + 'accession_dimensions.csv', dtype=str)\n",
    "\n",
    "# Loop through each accession number and create an annotation for each localized object.\n",
    "for image_index, image_row in accession_dataframe.iterrows():\n",
    "    print('Processing image ' + str(image_index + 1) + ' of ' + str(len(accession_dataframe)))\n",
    "    # Build the resources list for the annotations.\n",
    "    resources = []\n",
    "    \n",
    "    # Loop through each object in the image.\n",
    "    for object_index, object_row in object_localization_dataframe.iterrows():\n",
    "        if object_row['accession_number'] != image_row['accession_number']:\n",
    "            continue\n",
    "\n",
    "        # Create a W3C fragment selector for the annotation.\n",
    "        # https://www.w3.org/TR/annotation-model/#fragment-selector\n",
    "        # Calculate the upper left x and y in absolute canvas coordinates.\n",
    "        x = str(round(object_row['rel_left_x'] * float(image_row['max_width'])))\n",
    "        y = str(round(object_row['rel_top_y'] * float(image_row['max_height'])))\n",
    "\n",
    "        # Calculate the width and height in absolute canvas coordinates.\n",
    "        width = str(round((object_row['rel_right_x'] - object_row['rel_left_x']) * float(image_row['max_width'])))\n",
    "        height = str(round((object_row['rel_bottom_y'] - object_row['rel_top_y']) * float(image_row['max_height'])))\n",
    "\n",
    "        fragment_selector = 'xywh=' + x + ',' + y + ',' + width + ',' + height\n",
    "\n",
    "        # Build the annotation.\n",
    "        on_value = {\n",
    "            '@type': 'oa:SpecificResource',\n",
    "            'full': 'https://iiif-manifest.library.vanderbilt.edu/gallery/' + image_row['accession_number'].split('.')[0] + '/' + image_row['accession_number'] + '.json_1',\n",
    "            'selector': {\n",
    "                'type': 'oa:FragmentSelector',\n",
    "                'value': fragment_selector\n",
    "            },\n",
    "            'within': {\n",
    "                '@id': 'https://iiif-manifest.library.vanderbilt.edu/gallery/' + image_row['accession_number'].split('.')[0] + '/' + image_row['accession_number'] + '.json',\n",
    "                '@type': 'sc:Manifest'\n",
    "            }\n",
    "        }\n",
    "        resource_value = {\n",
    "            '@type': 'dctypes:Text',\n",
    "            'format': 'text/plain',\n",
    "            'chars': object_row['description']\n",
    "        }\n",
    "\n",
    "        annotation = {\n",
    "            '@context': 'http://iiif.io/api/presentation/2/context.json',\n",
    "            '@id': 'https://iiif-manifest.library.vanderbilt.edu/gallery/' + image_row['accession_number'].split('.')[0] + '/' + image_row['accession_number'] + '/annotation/' + str(object_index),\n",
    "            '@type': 'oa:Annotation',\n",
    "            'motivation': [\n",
    "                'oa:commenting'\n",
    "            ],\n",
    "            'on': on_value,\n",
    "            'resource': [\n",
    "                resource_value\n",
    "            ]\n",
    "        }\n",
    "        resources.append(annotation)\n",
    "    \n",
    "    annotations = {\n",
    "        \"@context\": \"http://www.shared-canvas.org/ns/context.json\",\n",
    "        \"@id\": annotations_base_url + image_row['accession_number'].split('.')[0] + \"/\" + image_row['accession_number'] + \"_annotations.json\",\n",
    "        \"@type\": \"sc:AnnotationList\",\n",
    "        \"resources\": resources\n",
    "    }\n",
    "\n",
    "    # Write the annotations to a JSON file.\n",
    "    with open(base_path + 'annotations/' + image_row['accession_number'] + '_annotations.json', 'w') as outfile:\n",
    "        output_text = json.dumps(annotations, indent=2)\n",
    "        outfile.write(output_text)\n",
    "\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the link from the manifest to the annotation URL\n",
    "\n",
    "The annotations file can't be applied to the manifest unless the manifest has a link to it's web address. So an `otherContent` link must be added to the canvas that's being annotated. The link URL has to be a real URL that dereferences, since the annotations have to be retrieved on the fly when the viewer applies the annotations to the canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step through each image in the accession dimensions CSV file.\n",
    "for image_index, image_row in accession_dataframe.iterrows():\n",
    "    print(image_row['accession_number'])\n",
    "\n",
    "    # Look up the manifest URL for the image in the source image dataframe.\n",
    "    manifest_url = source_image_dataframe.loc[source_image_dataframe['accession_number'] == image_row['accession_number'], 'iiif_manifest'].iloc[0]\n",
    "    \n",
    "    # Get the manifest JSON.\n",
    "    manifest_response = requests.get(manifest_url)\n",
    "    manifest_json = manifest_response.json()\n",
    "    \n",
    "    # Create otherContent dictionary.\n",
    "    other_content = [\n",
    "        {\n",
    "        '@id': annotations_base_url + image_row['accession_number'].split('.')[0] + \"/\" + image_row['accession_number'] + \"_annotations.json\",\n",
    "        '@type': 'sc:AnnotationList'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Add the otherContent dictionary to the manifest.\n",
    "    manifest_json['sequences'][0]['canvases'][0]['otherContent'] = other_content\n",
    "\n",
    "    # Write the manifest to a JSON file.\n",
    "    with open(base_path + 'manifests/' + image_row['accession_number'] + '.json', 'w') as outfile:\n",
    "        text = json.dumps(manifest_json, indent=4)\n",
    "        outfile.write(text)\n",
    "\n",
    "print('done')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f96c65e2c1d4fcba82e9525c1be2fd15c6a14102f9c31bd3457b5f48c526190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
