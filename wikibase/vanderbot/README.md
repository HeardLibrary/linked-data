# Editing generic Wikibases using VanderBot and related scripts

This directory contains work related to automating interactions with any wikibase using the [VanderBot](http://vanderbi.lt/vanderbot) and related scripts for interacting with the Wikidata API and wikibase APIs in general. 

This material was developed to support a presentation to the [LD4 Wikibase Working Hour](https://www.wikidata.org/wiki/Wikidata:WikiProject_LD4_Wikidata_Affinity_Group/Wikibase_Working_Hours) on 2023-02-13. The primary source of information is [a web page](https://heardlibrary.github.io/digital-scholarship/lod/wikibase/load/), which provides details on running the software, screenshots, etc.

## Related scripts

In order to use the files in this directory, several scripts are required:

| script/documentation link | filename/link | description |
|--------|---------------|-------------|
| [VanderBot](https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/README.md) | [vanderbot.py](https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/vanderbot.py) |  |
| [VanderDeleteBot]() | [vanderdeletebot.py](https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/vanderdeletebot.py) | uses a Wikimedia API to delete claims (statements) or references based on their unique IDs |
| [VanderPropertyBot]() | [vanderpropertybot.py](https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/vanderpropertybot.py) | uses the API of a non-Wikimedia Foundation wikibase API to create new properties |
| [ConvertConfigToMetadataSchema](https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/convert-config.md) | [convert_config_to_metadata_schema.py](https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/convert_config_to_metadata_schema.py) | generates CSV column headers and W3C standard CSV description file used by VanderBot from a YAML mapping configuration file |
| [AcquireWikidataMetadata](https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/acquire_wikidata.md) | [acquire_wikidata_metadata.py](https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/acquire_wikidata_metadata.py) | uses SPARQL to download existing data from Wikidata or other wikibases |

## Files in this directory

Descriptions of files in this directory and subdirectories

| file | description |
|------|-------------|
| `config.yaml` (default filename) | YAML configuration file that maps CSV columns to custom wikibase properties. This particular file describes the `statues.csv` file used in the webpage examples |
| `config_wikidata.yaml` | configuration file that maps the same column headers as `config.yaml`, but to corresponding Wikidata P IDs. Used to download existing data. |
| `csv-metadata.json` | metadata description file in standard format described by the W3C [Generating RDF from Tabular Data on the Web](https://www.w3.org/TR/csv2rdf/) Recommendation and used by VanderBot to interpret the CSV containing source data to upload. This one was generated from `config.yaml` by the `convert_config_to_metadata_schema.py` script. |
| `hstatues.csv` | CSV column headers generated from the `config.yaml` configuration file |
| `qids.csv` | table containing the Q IDs of some famous statues whose metadata can be downloaded from Wikidata using the `acquire_wikidata_metadata.py` based on the properties in the `config_wikidata.yaml` file. |
| `statues.csv` | CSV data file for statues data showing all of the identifiers generated by the custom wikibase API after the data were uploaded |
| `statues_added_ready.csv` | CSV data file for statues data ready for upload to the custom wikibase after adding the data downloaded from Wikidata |
| `statues_downloaded.csv` | CSV data file resulting from the Wikidata download by the `acquire_wikidata_metadata.py` based on the `config_wikidata.yaml` configuration data and the list of Q IDs in `qids.csv` |
| `statues_raw.csv` | Statue of Liberty metadata added manually to the headers generated in `hstatues.csv` by the `convert_config_to_metadata_schema.py` script, ready to be written to the custom wikibase API. |
| [elements](https://github.com/HeardLibrary/linked-data/tree/master/wikibase/vanderbot/elements) | directory containing data that can be used for a test upload of chemical elements to a custom wikibase |
| [states](https://github.com/HeardLibrary/linked-data/tree/master/wikibase/vanderbot/states) | directory containing data that can be used for a test upload of state capitals and states to a custom wikibase |

----
Revised 2023-02-09
