{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VanderBot (departments)\n",
    "\n",
    "The scripts in this notebook are part of the development of VanderBot, a system to write information about Vanderbilt University researchers and their works to Wikidata.  \n",
    "\n",
    "This is a side project geared towards getting information about Vanderbilt departments into Wikidata.  It is a hack of https://github.com/HeardLibrary/linked-data/blob/master/publications/process_department.ipynb\n",
    "\n",
    "This code is freely available under a CC0 license. Steve Baskauf 2019-12-16  \n",
    "\n",
    "For more information, see [this page](https://github.com/HeardLibrary/linked-data/tree/master/publications).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Code\n",
    "\n",
    "This code block includes import statements, function definitions, and declarations of variables that are common to the rest of the script. It needs to be run once before the other code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steve-bootcamp\\AppData\\Roaming\\Python\\Python37\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "# For a particular processing round, set a short name for the department here.\n",
    "# This name is used to generate a set of unique processing files for that department.\n",
    "deptShortName = 'physics'\n",
    "testEmployer = 'Vanderbilt University' # to test against Wikidata employer property\n",
    "employerQId = 'Q29052' # Vanderbilt University\n",
    "deathDateLimit = '2000' # any date deaths before this date will be assumed to not be a match\n",
    "\n",
    "# NOTE: eventually need to test against all affiliations in cases of faculty with multiple appointments\n",
    "\n",
    "# ***********************************\n",
    "# NOTE: the script fails if there is a current item in Wikidata that has the same values for both label and description. \n",
    "# A check needs to be run for this !!!\n",
    "# ***********************************\n",
    "\n",
    "# The default labels and descriptions can either be a column in the table or set as a constant. \n",
    "# If it's a column, the value is the column header.  If it's a constant, the value is the string to assign as the value.\n",
    "deptSettings = {\n",
    "    'bsci': {\n",
    "        'categories': ['primary-training-faculty', 'research-and-teaching-faculty', 'secondary-faculty', 'postdoc-fellows', 'emeriti'],\n",
    "        'baseUrl': 'https://as.vanderbilt.edu/biosci/people/index.php?group=',\n",
    "        'departmentSearchString': 'Biological Sciences',\n",
    "        'departmentQId': 'Q78041310',\n",
    "        'testAuthorAffiliation': 'Biological Sciences Vanderbilt',\n",
    "        \"labels\": \n",
    "            {\n",
    "                \"source\": \"column\",\n",
    "                \"value\": \"name\"\n",
    "            },\n",
    "        \"descriptions\": \n",
    "            {\n",
    "                \"source\": \"constant\",\n",
    "                \"value\": \"biology researcher\"\n",
    "            }\n",
    "    },\n",
    "    'physics': {\n",
    "        'categories': ['faculty', 'emeritus-faculty', 'academic-research-staff'],\n",
    "        'baseUrl': 'https://as.vanderbilt.edu/physics/people/index.php?group=',\n",
    "        'departmentSearchString': 'Physics Astronomy',\n",
    "        'departmentQId': 'Q78779260',\n",
    "        'testAuthorAffiliation': 'Physics Astronomy Vanderbilt',\n",
    "        \"labels\": \n",
    "            {\n",
    "                \"source\": \"column\",\n",
    "                \"value\": \"name\"\n",
    "            },\n",
    "        \"descriptions\": \n",
    "            {\n",
    "                \"source\": \"constant\",\n",
    "                \"value\": \"physics/astronomy researcher\"\n",
    "            }\n",
    "    }\n",
    "}\n",
    "\n",
    "wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "degreeList = [\n",
    "    {'string': 'Ph.D.', 'value': 'Ph.D.'},\n",
    "    {'string': 'PhD', 'value': 'Ph.D.'},\n",
    "    {'string': 'D.Phil.', 'value': 'D.Phil.'},\n",
    "    {'string': 'J.D.', 'value': 'J.D.'}\n",
    "     ]\n",
    "\n",
    "# NCBI identification requirements:\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'VanderBot' # give your application a name here\n",
    "\n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generateHeaderDictionary(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/0.8 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def writeCsv(fileName, array):\n",
    "    fileObject = open(fileName, 'w', newline='', encoding='utf-8')\n",
    "    writerObject = csv.writer(fileObject)\n",
    "    for row in array:\n",
    "        writerObject.writerow(row)\n",
    "    fileObject.close()\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def writeDictsToCsv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='') as csvFileObject:\n",
    "        writer = csv.DictWriter(csvFileObject, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def readDict(filename):\n",
    "    fileObject = open(filename, 'r', newline='', encoding='utf-8')\n",
    "    dictObject = csv.DictReader(fileObject)\n",
    "    array = []\n",
    "    for row in dictObject:\n",
    "        array.append(row)\n",
    "    fileObject.close()\n",
    "    return array\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extractQNumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts a local name from an IRI, specify the list item number for the last piece separated by slash\n",
    "def extractFromIri(iri, numberPieces):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[numberPieces]\n",
    "\n",
    "# see https://www.wikidata.org/wiki/Property:P21 for values\n",
    "def decodeSexOrGender(code):\n",
    "    code = code.lower()\n",
    "    if code == 'm':\n",
    "        qId = 'Q6581097'\n",
    "    elif code == 'f':\n",
    "        qId = 'Q6581072'\n",
    "    elif code == 'i':\n",
    "        qId = 'Q1097630'\n",
    "    elif code == 'tf':\n",
    "        qId = 'Q1052281'\n",
    "    elif code == 'tm':\n",
    "        qId = 'Q2449503'\n",
    "    else:\n",
    "        qId = ''\n",
    "    return qId\n",
    "\n",
    "# query for a single variable that's an item named 'item'\n",
    "# returns a list of results\n",
    "def searchWikidataForQIdByOrcid(orcid):\n",
    "    query = '''\n",
    "select distinct ?item where {\n",
    "  ?item wdt:P496 \"''' + employees[employeeIndex]['orcid'] + '''\".\n",
    "  }\n",
    "'''\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append(qNumber)\n",
    "    except:\n",
    "        results = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# returns a list of employer strings for the item with Wikidata ID qId; P108 is employer\n",
    "def searchWikidataEmployer(qId):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?employer where {\n",
    "        wd:'''+ qId + ''' wdt:P108 ?employerId.\n",
    "        ?employerId rdfs:label ?employer.\n",
    "        FILTER(lang(?employer) = 'en')\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                resultsList.append(statement['employer']['value'])\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "# returns a list of value Q IDs of the property propertyId for the item with Wikidata ID qId\n",
    "def searchWikidataSingleProperty(qId, propertyId, valueType):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?object where {\n",
    "        wd:'''+ qId + ''' wdt:''' + propertyId + ''' ?object.\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                if valueType == 'item':\n",
    "                    resultValue = extractQNumber(statement['object']['value'])\n",
    "                else:\n",
    "                    resultValue = statement['object']['value']\n",
    "                resultsList.append(resultValue)\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "def searchOrcidAtWikidata(qIds):\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    # NOTE: the MINUS clause removes properties in the http://www.wikidata.org/prop/reference/value/ namespace\n",
    "    # leaving only those in the http://www.wikidata.org/prop/reference/ namespace (i.e. the direct literal values)\n",
    "    query = '''\n",
    "select distinct ?id ?statement ?orcid ?reference ?refProp ?refVal where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id p:P496 ?statement.\n",
    "  ?statement ps:P496 ?orcid.\n",
    "  optional {\n",
    "        ?statement prov:wasDerivedFrom ?reference.\n",
    "        ?reference ?refProp ?refVal.\n",
    "        MINUS {\n",
    "            ?reference ?refProp ?refVal. \n",
    "            FILTER(contains(str(?refProp), \"value/\"))\n",
    "        }\n",
    "    }\n",
    "  }'''\n",
    "    #print(query)\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    statements = data['results']['bindings']\n",
    "    # ********** NOTE: need to deal with case where there are more than one reference per statement\n",
    "    # This will result in several statements with the same qNumeber, orcid, and referenceHash\n",
    "    for statement in statements:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(statement['id']['value'], 4)\n",
    "        # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "        noDomain = extractFromIri(statement['statement']['value'], 5)\n",
    "        # need to remove the qNumber that's appended in front of the UUID\n",
    "        statementUuid = noDomain.partition(qNumber + '-')[2]\n",
    "        orcid = statement['orcid']['value']\n",
    "        if 'reference' in statement:\n",
    "            # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "            referenceHash = extractFromIri(statement['reference']['value'], 4)\n",
    "        else:\n",
    "            referenceHash = ''\n",
    "        if 'refProp' in statement:\n",
    "            # remove pr: 'http://www.wikidata.org/prop/reference/'\n",
    "            referenceProperty = extractFromIri(statement['refProp']['value'], 5)\n",
    "        else:\n",
    "            referenceProperty = ''\n",
    "        if 'refVal' in statement:\n",
    "            referenceValue = statement['refVal']['value']\n",
    "            # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "            if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                referenceValue = referenceValue.split('T')[0]\n",
    "        else:\n",
    "            referenceValue = ''\n",
    "        results.append({'qId': qNumber, 'statementUuid': statementUuid, 'orcid': orcid, 'referenceHash': referenceHash, 'referenceProperty': referenceProperty, 'referenceValue': referenceValue})\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "def searchHumansAtWikidata(qIds):\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    # NOTE: instanceOf human is one of the statement that Wikidata does not care about references for\n",
    "    # So we will ignore them here\n",
    "    query = '''\n",
    "select distinct ?id ?statement where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:Q5.\n",
    "  }'''\n",
    "    #print(query)\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    statements = data['results']['bindings']\n",
    "    for statement in statements:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(statement['id']['value'], 4)\n",
    "        # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "        noDomain = extractFromIri(statement['statement']['value'], 5)\n",
    "        # need to remove the qNumber that's appended in front of the UUID\n",
    "        statementUuid = noDomain.partition(qNumber + '-')[2]\n",
    "        results.append({'qId': qNumber, 'statementUuid': statementUuid})\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "def checkOrcid(orcid):\n",
    "    namespace = 'https://orcid.org/'\n",
    "    endpointUrl = namespace + orcid\n",
    "    acceptMediaType = 'application/ld+json'\n",
    "    r = requests.get(endpointUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    code = r.status_code\n",
    "    #print(r.text)\n",
    "    data = r.json()\n",
    "    response = {'code': code, 'data': data}\n",
    "    if response['code'] != 200:\n",
    "        print('Attempt to dereference ORCID resulted in HTTP response code ', response['code'])\n",
    "        data['orcidReferenceValue'] = ''\n",
    "    else:\n",
    "        print('Successfully retrieved')\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "    # delay a quarter second to avoid hitting the API too rapidly\n",
    "    sleep(0.25)\n",
    "    return(wholeDateZ)\n",
    "\n",
    "# if the value passed is '' then the value will be retrieved.  Otherwise, the value is used to screen.\n",
    "def searchStatementAtWikidata(qIds, prop, value, refPropList):\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?statement '\n",
    "    # if no value was specified, find the value\n",
    "    if value == '':\n",
    "        query += '?statementValue '\n",
    "    if len(refPropList) != 0:\n",
    "        query += '?reference '\n",
    "    for refPropIndex in range(0, len(refPropList)):\n",
    "        query += '?refVal' + str(refPropIndex) + ' '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id p:'''+ prop + ''' ?statement.\n",
    "  ?statement ps:'''+ prop\n",
    "    \n",
    "    if value == '':\n",
    "        query += ' ?statementValue.'\n",
    "    else:\n",
    "        query += ' wd:' + value + '.'\n",
    "\n",
    "    if len(refPropList) != 0:\n",
    "        query += '''\n",
    "  optional {\n",
    "    ?statement prov:wasDerivedFrom ?reference.'''\n",
    "        for refPropIndex in range(0, len(refPropList)):\n",
    "            query +='''\n",
    "    ?reference pr:''' + refPropList[refPropIndex] + ''' ?refVal''' + str(refPropIndex) + '''.'''\n",
    "        query +='''\n",
    "        }'''\n",
    "    query +='''\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    # ********** NOTE: need to deal with case where there are more than one reference per result\n",
    "    # This will result in several results with the same qNumeber, orcid, and referenceHash\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "        noDomain = extractFromIri(result['statement']['value'], 5)\n",
    "        # need to remove the qNumber that's appended in front of the UUID\n",
    "        statementUuid = noDomain.partition(qNumber + '-')[2]\n",
    "        # if no value was specified, get the value that was found in the search\n",
    "        if value == '':\n",
    "            statementValue = result['statementValue']['value']\n",
    "        if len(refPropList) != 0:\n",
    "            if 'reference' in result:\n",
    "                # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "                referenceHash = extractFromIri(result['reference']['value'], 4)\n",
    "            else:\n",
    "                referenceHash = ''\n",
    "            referenceValues = []\n",
    "            for refPropIndex in range(0, len(refPropList)):\n",
    "                if 'refVal' + str(refPropIndex) in result:\n",
    "                    refVal = result['refVal' + str(refPropIndex)]['value']\n",
    "                    # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "                    #if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                    #    referenceValue = referenceValue.split('T')[0]\n",
    "                else:\n",
    "                    refVal = ''\n",
    "                referenceValues.append(refVal)\n",
    "        resultsDict = {'qId': qNumber, 'statementUuid': statementUuid}\n",
    "        # if no value was specified, get the value that was found in the search\n",
    "        if value == '':\n",
    "            resultsDict['statementValue'] = statementValue\n",
    "        if len(refPropList) != 0:\n",
    "            resultsDict['referenceHash'] = referenceHash\n",
    "            resultsDict['referenceValues'] = referenceValues\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    \n",
    "    return returnValue\n",
    "\n",
    "# search for any of the \"label\" types: label, alias, description\n",
    "def searchLabelsDescriptionsAtWikidata(qIds, labelType, language):\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    if labelType == 'label':\n",
    "        predicate = 'rdfs:label'\n",
    "    elif labelType == 'alias':\n",
    "        predicate = 'skos:altLabel'\n",
    "    elif labelType == 'description':\n",
    "        predicate = 'schema:description'\n",
    "    else:\n",
    "        predicate = 'rdfs:label'        \n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?string '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id '''+ predicate + ''' ?string.\n",
    "  filter(lang(?string)=\"''' + language + '''\")\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        string = result['string']['value']\n",
    "        resultsDict = {'qId': qNumber, 'string': string}\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    \n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Engineering departments web page\n",
    "\n",
    "Scrapes the department names and homepage URLs at https://engineering.vanderbilt.edu/academics/departments-and-programs.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biomedical Engineering\n",
      "https://engineering.vanderbilt.edu/bme/index.php\n",
      "Chemical and Biomolecular Engineering\n",
      "https://engineering.vanderbilt.edu/chbe/index.php\n",
      "Civil and Environmental Engineering\n",
      "https://engineering.vanderbilt.edu/cee/index.php\n",
      "Electrical Engineering and Computer Science\n",
      "https://engineering.vanderbilt.edu/eecs/index.php\n",
      "Mechanical Engineering\n",
      "https://engineering.vanderbilt.edu/me/index.php\n",
      "Division of General Engineering\n",
      "https://engineering.vanderbilt.edu/ge/index.php\n",
      "Interdisciplinary Materials Science Program\n",
      "https://engineering.vanderbilt.edu/materials-science/\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "outputTable = [['name', 'labelEn', 'alias', 'parentUnitReferenceRetrieved', 'officialWebsite']]\n",
    "\n",
    "acceptMediaType = 'text/html'\n",
    "url = 'https://engineering.vanderbilt.edu/academics/departments-and-programs.php'\n",
    "response = requests.get(url, headers = generateHeaderDictionary(acceptMediaType))\n",
    "soupObject = BeautifulSoup(response.text,features=\"html5lib\")\n",
    "\n",
    "# get the first table from the page\n",
    "listObject = soupObject.find_all('article')[0]\n",
    "\n",
    "# get all of the list items\n",
    "departmentItems = listObject.find_all('p')\n",
    "\n",
    "for department in departmentItems:\n",
    "    # the name is the text inside the 'a' element\n",
    "    nameElement = department.find('a')\n",
    "    name = nameElement.text.strip()\n",
    "    print(name)\n",
    "    url = department.a['href']\n",
    "    if url[0] =='/':\n",
    "        url = 'https://engineering.vanderbilt.edu' + url\n",
    "    print(url)\n",
    "    labelEn = 'Vanderbilt Department of ' + name\n",
    "    alias = json.dumps(['Vanderbilt ' + name + ' Department'])\n",
    "    \n",
    "    wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "    wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "\n",
    "    \n",
    "    outputTable.append([name, labelEn, alias, wholeDateZ, url])            \n",
    "\n",
    "fileName = 'engineering.csv'\n",
    "writeCsv(fileName, outputTable)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add description\n",
    "\n",
    "Manually dereference each website and copy and paste some description.\n",
    "\n",
    "Also check that the labelEn is actually what they use. Replace the autogenerated labelEn and move the name to alias if needed.\n",
    "Add column for wikidataId."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download various statements and references, then generate write file\n",
    "\n",
    "NOTE: between the previous step and this one, the description column was added.\n",
    "\n",
    "The following script was hacked from the researcher script with no attempt to change variable names.  \"employees\" refers to departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "filename = 'engineering.csv'\n",
    "employees = readDict(filename)\n",
    "\n",
    "# create a list of the departments that have Wikidata qIDs\n",
    "qIds = []\n",
    "for employee in employees:\n",
    "    if employee['wikidataId'] != '':\n",
    "        qIds.append(employee['wikidataId'])\n",
    "\n",
    "# get data already in Wikidata about parent organization\n",
    "prop = 'P749' # parent organization\n",
    "value = 'Q7914459' # Vanderbilt University School of Engineering\n",
    "refProps = ['P854', 'P813'] # source URL, retrieved\n",
    "wikidataEmployerData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "#print(json.dumps(wikidataEmployerData, indent=2))\n",
    "\n",
    "# match people with employment data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matchedStatement = False\n",
    "    for wikidataEmployerDataIndex in range(0, len(wikidataEmployerData)):\n",
    "        if wikidataEmployerData[wikidataEmployerDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matchedStatement = True\n",
    "            employees[employeeIndex]['parentUnitStatementUuid'] = wikidataEmployerData[wikidataEmployerDataIndex]['statementUuid']\n",
    "            employees[employeeIndex]['parentUnitReferenceHash'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceHash']\n",
    "            # if there is a referenceHash then record the values for the two reference properties: P813, P854'; retrieved, source URL\n",
    "            if employees[employeeIndex]['parentUnitReferenceHash']!= '':\n",
    "                # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                employees[employeeIndex]['parentUnitReferenceSourceUrl'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0]\n",
    "                employees[employeeIndex]['parentUnitReferenceRetrieved'] = '+' + wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][1]\n",
    "        \n",
    "    employees[employeeIndex]['parentUnit'] = value\n",
    "    if not matchedStatement:  # only generate the metadata if there isn't already a statement\n",
    "        employees[employeeIndex]['parentUnitReferenceSourceUrl'] = 'https://engineering.vanderbilt.edu/academics/departments-and-programs.php'\n",
    "        # leave the value in the parentUnitReferenceRetrieved column the same\n",
    "        #employees[employeeIndex]['employerReferenceRetrieved'] = wholeDateZ\n",
    "\n",
    "# get data already in Wikidata about official department web page\n",
    "prop = 'P856' # official website\n",
    "refProps = ['P813'] # retrieved\n",
    "wikidataEmployerData = searchStatementAtWikidata(qIds, prop, '', refProps)\n",
    "#print(json.dumps(wikidataEmployerData, indent=2))\n",
    "\n",
    "# match people with official website data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matchedStatement = False\n",
    "    matchedReference = False\n",
    "    for wikidataEmployerDataIndex in range(0, len(wikidataEmployerData)):\n",
    "        if wikidataEmployerData[wikidataEmployerDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matchedStatement = True\n",
    "            employees[employeeIndex]['officialWebsiteStatementUuid'] = wikidataEmployerData[wikidataEmployerDataIndex]['statementUuid']\n",
    "            employees[employeeIndex]['officialWebsite'] = wikidataEmployerData[wikidataEmployerDataIndex]['statementValue']\n",
    "            employees[employeeIndex]['officialWebsiteReferenceHash'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceHash']\n",
    "            # if there is a referenceHash then record the values for the reference properties: P813; retrieved\n",
    "            if employees[employeeIndex]['officialWebsiteReferenceHash']!= '':\n",
    "                # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                employees[employeeIndex]['officialWebsiteReferenceRetrieved'] = '+' + wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0]\n",
    "\n",
    "    if not matchedStatement:  # only generate the metadata if there isn't already a statement\n",
    "        # use existing officialWebsite value\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "        employees[employeeIndex]['officialWebsiteReferenceRetrieved'] = wholeDateZ\n",
    "\n",
    "# get data already in Wikidata about type\n",
    "prop = 'P31' # instance of\n",
    "value = 'Q2467461' # academic department\n",
    "refProps = ['P854', 'P813'] # source URL, retrieved\n",
    "wikidataEmployerData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "#print(json.dumps(wikidataEmployerData, indent=2))\n",
    "\n",
    "# match people with instance of data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matchedStatement = False\n",
    "    matchedReference = False\n",
    "    for wikidataEmployerDataIndex in range(0, len(wikidataEmployerData)):\n",
    "        if wikidataEmployerData[wikidataEmployerDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matchedStatement = True\n",
    "            employees[employeeIndex]['instanceOfStatementUuid'] = wikidataEmployerData[wikidataEmployerDataIndex]['statementUuid']\n",
    "            employees[employeeIndex]['instanceOfReferenceHash'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceHash']\n",
    "            # if there is a referenceHash then record the values for the two reference properties: P813, P854'; retrieved, source URL\n",
    "            if employees[employeeIndex]['instanceOfReferenceHash']!= '':\n",
    "                # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                if wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0] == employees[employeeIndex]['officialWebsite']:\n",
    "                    matchedReference = True\n",
    "                    employees[employeeIndex]['instanceOfReferenceSourceUrl'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0]\n",
    "                    employees[employeeIndex]['instanceOfReferenceRetrieved'] = '+' + wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][1]\n",
    "            # stop checking if there is an exact match to the reference URL. Otherwise keep looping.\n",
    "            # if there is a later reference that matches ours, it will overwrite any previous reference data\n",
    "            # otherwise, the existing (different) reference data will be retained\n",
    "            if matchedReference:\n",
    "                break\n",
    "\n",
    "    employees[employeeIndex]['instanceOf'] = value\n",
    "    if not matchedStatement:  # only generate the metadata if there isn't already a statement\n",
    "        # use the departmental websit as the reference URL\n",
    "        employees[employeeIndex]['instanceOfReferenceSourceUrl'] = employees[employeeIndex]['officialWebsite']\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        employees[employeeIndex]['instanceOfReferenceRetrieved'] = wholeDateZ        \n",
    "        \n",
    "# get all of the English language labels for the employees that are already in Wikidata\n",
    "labelType = 'label'\n",
    "language = 'en'\n",
    "wikidataLabels = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "\n",
    "# Match people with their labels; only overwrite if there is already an item online.  Otherwise use what's in the table\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    for wikidataLabelIndex in range(0, len(wikidataLabels)):\n",
    "        if wikidataLabels[wikidataLabelIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            employees[employeeIndex]['labelEn'] = wikidataLabels[wikidataLabelIndex]['string']\n",
    "\n",
    "# get all of the English language descriptions for the employees that are already in Wikidata\n",
    "labelType = 'description'\n",
    "language = 'en'\n",
    "wikidataDescriptions = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "\n",
    "# Match people with their descriptions\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    for wikidataDescriptionIndex in range(0, len(wikidataDescriptions)):\n",
    "        if wikidataDescriptions[wikidataDescriptionIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['description'] = wikidataDescriptions[wikidataDescriptionIndex]['string']\n",
    "\n",
    "# Get all of the aliases already at Wikidata for employees.  \n",
    "# Since there can be multiple aliases, they are stored as a list structure.\n",
    "# The writing script can handle multiple languages, but here we are only dealing with English ones.\n",
    "\n",
    "# retrieve the aliases in that language that already exist in Wikidata and match them with table rows\n",
    "labelType = 'alias'\n",
    "language = 'en'\n",
    "aliasesAtWikidata = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "for entityIndex in range(0, len(employees)):\n",
    "    personAliasList = []\n",
    "    if employees[entityIndex]['wikidataId'] != '':  # don't look for the label at Wikidata if the item doesn't yet exist\n",
    "        for wikiLabel in aliasesAtWikidata:\n",
    "            if employees[entityIndex]['wikidataId'] == wikiLabel['qId']:\n",
    "                personAliasList.append(wikiLabel['string'])\n",
    "        employees[entityIndex]['alias'] = json.dumps(personAliasList)\n",
    "    # if the item doesn't exist, don't mess with the existing alias dictionary that's in the table\n",
    "\n",
    "# write the file\n",
    "filename = 'engineering-to-write.csv'\n",
    "fieldnames = ['wikidataId', 'name', 'labelEn', 'alias', 'description', 'parentUnitStatementUuid', 'parentUnit', 'parentUnitReferenceHash', 'parentUnitReferenceSourceUrl', 'parentUnitReferenceRetrieved', 'officialWebsiteStatementUuid', 'officialWebsite', 'officialWebsiteReferenceHash', 'officialWebsiteReferenceRetrieved', 'instanceOfStatementUuid', 'instanceOf', 'instanceOfReferenceHash', 'instanceOfReferenceSourceUrl', 'instanceOfReferenceRetrieved']\n",
    "writeDictsToCsv(employees, filename, fieldnames)\n",
    "\n",
    "print()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
