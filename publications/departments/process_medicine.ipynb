{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VanderBot (departments)\n",
    "\n",
    "The scripts in this notebook are part of the development of VanderBot, a system to write information about Vanderbilt University researchers and their works to Wikidata.  \n",
    "\n",
    "This is a side project geared towards getting information about Vanderbilt departments into Wikidata.  It is a hack of https://github.com/HeardLibrary/linked-data/blob/master/publications/process_department.ipynb\n",
    "\n",
    "This code is freely available under a CC0 license. Steve Baskauf 2019-12-16  \n",
    "\n",
    "For more information, see [this page](https://github.com/HeardLibrary/linked-data/tree/master/publications).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Code\n",
    "\n",
    "This code block includes import statements, function definitions, and declarations of variables that are common to the rest of the script. It needs to be run once before the other code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "# For a particular processing round, set a short name for the department here.\n",
    "# This name is used to generate a set of unique processing files for that department.\n",
    "deptShortName = 'physics'\n",
    "testEmployer = 'Vanderbilt University' # to test against Wikidata employer property\n",
    "employerQId = 'Q29052' # Vanderbilt University\n",
    "deathDateLimit = '2000' # any date deaths before this date will be assumed to not be a match\n",
    "\n",
    "# NOTE: eventually need to test against all affiliations in cases of faculty with multiple appointments\n",
    "\n",
    "# ***********************************\n",
    "# NOTE: the script fails if there is a current item in Wikidata that has the same values for both label and description. \n",
    "# A check needs to be run for this !!!\n",
    "# ***********************************\n",
    "\n",
    "# The default labels and descriptions can either be a column in the table or set as a constant. \n",
    "# If it's a column, the value is the column header.  If it's a constant, the value is the string to assign as the value.\n",
    "deptSettings = {\n",
    "    'bsci': {\n",
    "        'categories': ['primary-training-faculty', 'research-and-teaching-faculty', 'secondary-faculty', 'postdoc-fellows', 'emeriti'],\n",
    "        'baseUrl': 'https://as.vanderbilt.edu/biosci/people/index.php?group=',\n",
    "        'departmentSearchString': 'Biological Sciences',\n",
    "        'departmentQId': 'Q78041310',\n",
    "        'testAuthorAffiliation': 'Biological Sciences Vanderbilt',\n",
    "        \"labels\": \n",
    "            {\n",
    "                \"source\": \"column\",\n",
    "                \"value\": \"name\"\n",
    "            },\n",
    "        \"descriptions\": \n",
    "            {\n",
    "                \"source\": \"constant\",\n",
    "                \"value\": \"biology researcher\"\n",
    "            }\n",
    "    },\n",
    "    'physics': {\n",
    "        'categories': ['faculty', 'emeritus-faculty', 'academic-research-staff'],\n",
    "        'baseUrl': 'https://as.vanderbilt.edu/physics/people/index.php?group=',\n",
    "        'departmentSearchString': 'Physics Astronomy',\n",
    "        'departmentQId': 'Q78779260',\n",
    "        'testAuthorAffiliation': 'Physics Astronomy Vanderbilt',\n",
    "        \"labels\": \n",
    "            {\n",
    "                \"source\": \"column\",\n",
    "                \"value\": \"name\"\n",
    "            },\n",
    "        \"descriptions\": \n",
    "            {\n",
    "                \"source\": \"constant\",\n",
    "                \"value\": \"physics/astronomy researcher\"\n",
    "            }\n",
    "    }\n",
    "}\n",
    "\n",
    "wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "degreeList = [\n",
    "    {'string': 'Ph.D.', 'value': 'Ph.D.'},\n",
    "    {'string': 'PhD', 'value': 'Ph.D.'},\n",
    "    {'string': 'D.Phil.', 'value': 'D.Phil.'},\n",
    "    {'string': 'J.D.', 'value': 'J.D.'}\n",
    "     ]\n",
    "\n",
    "# NCBI identification requirements:\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'VanderBot' # give your application a name here\n",
    "\n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generateHeaderDictionary(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/0.9 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def writeCsv(fileName, array):\n",
    "    fileObject = open(fileName, 'w', newline='', encoding='utf-8')\n",
    "    writerObject = csv.writer(fileObject)\n",
    "    for row in array:\n",
    "        writerObject.writerow(row)\n",
    "    fileObject.close()\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def writeDictsToCsv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='') as csvFileObject:\n",
    "        writer = csv.DictWriter(csvFileObject, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def readDict(filename):\n",
    "    fileObject = open(filename, 'r', newline='', encoding='utf-8')\n",
    "    dictObject = csv.DictReader(fileObject)\n",
    "    array = []\n",
    "    for row in dictObject:\n",
    "        array.append(row)\n",
    "    fileObject.close()\n",
    "    return array\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extractQNumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts a local name from an IRI, specify the list item number for the last piece separated by slash\n",
    "def extractFromIri(iri, numberPieces):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[numberPieces]\n",
    "\n",
    "# see https://www.wikidata.org/wiki/Property:P21 for values\n",
    "def decodeSexOrGender(code):\n",
    "    code = code.lower()\n",
    "    if code == 'm':\n",
    "        qId = 'Q6581097'\n",
    "    elif code == 'f':\n",
    "        qId = 'Q6581072'\n",
    "    elif code == 'i':\n",
    "        qId = 'Q1097630'\n",
    "    elif code == 'tf':\n",
    "        qId = 'Q1052281'\n",
    "    elif code == 'tm':\n",
    "        qId = 'Q2449503'\n",
    "    else:\n",
    "        qId = ''\n",
    "    return qId\n",
    "\n",
    "# query for a single variable that's an item named 'item'\n",
    "# returns a list of results\n",
    "def searchWikidataForQIdByOrcid(orcid):\n",
    "    query = '''\n",
    "select distinct ?item where {\n",
    "  ?item wdt:P496 \"''' + employees[employeeIndex]['orcid'] + '''\".\n",
    "  }\n",
    "'''\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append(qNumber)\n",
    "    except:\n",
    "        results = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# returns a list of employer strings for the item with Wikidata ID qId; P108 is employer\n",
    "def searchWikidataEmployer(qId):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?employer where {\n",
    "        wd:'''+ qId + ''' wdt:P108 ?employerId.\n",
    "        ?employerId rdfs:label ?employer.\n",
    "        FILTER(lang(?employer) = 'en')\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                resultsList.append(statement['employer']['value'])\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "# returns a list of value Q IDs of the property propertyId for the item with Wikidata ID qId\n",
    "def searchWikidataSingleProperty(qId, propertyId, valueType):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?object where {\n",
    "        wd:'''+ qId + ''' wdt:''' + propertyId + ''' ?object.\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                if valueType == 'item':\n",
    "                    resultValue = extractQNumber(statement['object']['value'])\n",
    "                else:\n",
    "                    resultValue = statement['object']['value']\n",
    "                resultsList.append(resultValue)\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "def searchOrcidAtWikidata(qIds):\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    # NOTE: the MINUS clause removes properties in the http://www.wikidata.org/prop/reference/value/ namespace\n",
    "    # leaving only those in the http://www.wikidata.org/prop/reference/ namespace (i.e. the direct literal values)\n",
    "    query = '''\n",
    "select distinct ?id ?statement ?orcid ?reference ?refProp ?refVal where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id p:P496 ?statement.\n",
    "  ?statement ps:P496 ?orcid.\n",
    "  optional {\n",
    "        ?statement prov:wasDerivedFrom ?reference.\n",
    "        ?reference ?refProp ?refVal.\n",
    "        MINUS {\n",
    "            ?reference ?refProp ?refVal. \n",
    "            FILTER(contains(str(?refProp), \"value/\"))\n",
    "        }\n",
    "    }\n",
    "  }'''\n",
    "    #print(query)\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    statements = data['results']['bindings']\n",
    "    # ********** NOTE: need to deal with case where there are more than one reference per statement\n",
    "    # This will result in several statements with the same qNumeber, orcid, and referenceHash\n",
    "    for statement in statements:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(statement['id']['value'], 4)\n",
    "        # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "        noDomain = extractFromIri(statement['statement']['value'], 5)\n",
    "        # need to remove the qNumber that's appended in front of the UUID\n",
    "        statementUuid = noDomain.partition(qNumber + '-')[2]\n",
    "        orcid = statement['orcid']['value']\n",
    "        if 'reference' in statement:\n",
    "            # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "            referenceHash = extractFromIri(statement['reference']['value'], 4)\n",
    "        else:\n",
    "            referenceHash = ''\n",
    "        if 'refProp' in statement:\n",
    "            # remove pr: 'http://www.wikidata.org/prop/reference/'\n",
    "            referenceProperty = extractFromIri(statement['refProp']['value'], 5)\n",
    "        else:\n",
    "            referenceProperty = ''\n",
    "        if 'refVal' in statement:\n",
    "            referenceValue = statement['refVal']['value']\n",
    "            # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "            if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                referenceValue = referenceValue.split('T')[0]\n",
    "        else:\n",
    "            referenceValue = ''\n",
    "        results.append({'qId': qNumber, 'statementUuid': statementUuid, 'orcid': orcid, 'referenceHash': referenceHash, 'referenceProperty': referenceProperty, 'referenceValue': referenceValue})\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "def searchHumansAtWikidata(qIds):\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    # NOTE: instanceOf human is one of the statement that Wikidata does not care about references for\n",
    "    # So we will ignore them here\n",
    "    query = '''\n",
    "select distinct ?id ?statement where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:Q5.\n",
    "  }'''\n",
    "    #print(query)\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    statements = data['results']['bindings']\n",
    "    for statement in statements:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(statement['id']['value'], 4)\n",
    "        # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "        noDomain = extractFromIri(statement['statement']['value'], 5)\n",
    "        # need to remove the qNumber that's appended in front of the UUID\n",
    "        statementUuid = noDomain.partition(qNumber + '-')[2]\n",
    "        results.append({'qId': qNumber, 'statementUuid': statementUuid})\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "def checkOrcid(orcid):\n",
    "    namespace = 'https://orcid.org/'\n",
    "    endpointUrl = namespace + orcid\n",
    "    acceptMediaType = 'application/ld+json'\n",
    "    r = requests.get(endpointUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    code = r.status_code\n",
    "    #print(r.text)\n",
    "    data = r.json()\n",
    "    response = {'code': code, 'data': data}\n",
    "    if response['code'] != 200:\n",
    "        print('Attempt to dereference ORCID resulted in HTTP response code ', response['code'])\n",
    "        data['orcidReferenceValue'] = ''\n",
    "    else:\n",
    "        print('Successfully retrieved')\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "    # delay a quarter second to avoid hitting the API too rapidly\n",
    "    sleep(0.25)\n",
    "    return(wholeDateZ)\n",
    "\n",
    "# if the value passed is '' then the value will be retrieved.  Otherwise, the value is used to screen.\n",
    "def searchStatementAtWikidata(qIds, prop, value, refPropList):\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?statement '\n",
    "    # if no value was specified, find the value\n",
    "    if value == '':\n",
    "        query += '?statementValue '\n",
    "    if len(refPropList) != 0:\n",
    "        query += '?reference '\n",
    "    for refPropIndex in range(0, len(refPropList)):\n",
    "        query += '?refVal' + str(refPropIndex) + ' '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id p:'''+ prop + ''' ?statement.\n",
    "  ?statement ps:'''+ prop\n",
    "    \n",
    "    if value == '':\n",
    "        query += ' ?statementValue.'\n",
    "    else:\n",
    "        query += ' wd:' + value + '.'\n",
    "\n",
    "    if len(refPropList) != 0:\n",
    "        query += '''\n",
    "  optional {\n",
    "    ?statement prov:wasDerivedFrom ?reference.'''\n",
    "        for refPropIndex in range(0, len(refPropList)):\n",
    "            query +='''\n",
    "    ?reference pr:''' + refPropList[refPropIndex] + ''' ?refVal''' + str(refPropIndex) + '''.'''\n",
    "        query +='''\n",
    "        }'''\n",
    "    query +='''\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    # ********** NOTE: need to deal with case where there are more than one reference per result\n",
    "    # This will result in several results with the same qNumeber, orcid, and referenceHash\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "        noDomain = extractFromIri(result['statement']['value'], 5)\n",
    "        # need to remove the qNumber that's appended in front of the UUID\n",
    "        statementUuid = noDomain.partition(qNumber + '-')[2]\n",
    "        # if no value was specified, get the value that was found in the search\n",
    "        if value == '':\n",
    "            statementValue = result['statementValue']['value']\n",
    "        if len(refPropList) != 0:\n",
    "            if 'reference' in result:\n",
    "                # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "                referenceHash = extractFromIri(result['reference']['value'], 4)\n",
    "            else:\n",
    "                referenceHash = ''\n",
    "            referenceValues = []\n",
    "            for refPropIndex in range(0, len(refPropList)):\n",
    "                if 'refVal' + str(refPropIndex) in result:\n",
    "                    refVal = result['refVal' + str(refPropIndex)]['value']\n",
    "                    # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "                    #if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                    #    referenceValue = referenceValue.split('T')[0]\n",
    "                else:\n",
    "                    refVal = ''\n",
    "                referenceValues.append(refVal)\n",
    "        resultsDict = {'qId': qNumber, 'statementUuid': statementUuid}\n",
    "        # if no value was specified, get the value that was found in the search\n",
    "        if value == '':\n",
    "            resultsDict['statementValue'] = statementValue\n",
    "        if len(refPropList) != 0:\n",
    "            resultsDict['referenceHash'] = referenceHash\n",
    "            resultsDict['referenceValues'] = referenceValues\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    \n",
    "    return returnValue\n",
    "\n",
    "# search for any of the \"label\" types: label, alias, description\n",
    "def searchLabelsDescriptionsAtWikidata(qIds, labelType, language):\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    if labelType == 'label':\n",
    "        predicate = 'rdfs:label'\n",
    "    elif labelType == 'alias':\n",
    "        predicate = 'skos:altLabel'\n",
    "    elif labelType == 'description':\n",
    "        predicate = 'schema:description'\n",
    "    else:\n",
    "        predicate = 'rdfs:label'        \n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?string '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id '''+ predicate + ''' ?string.\n",
    "  filter(lang(?string)=\"''' + language + '''\")\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        string = result['string']['value']\n",
    "        resultsDict = {'qId': qNumber, 'string': string}\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    \n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Medicine departments faculty directory web pages\n",
    "\n",
    "Scrapes the faculty, department names, and homepage URLs at https://wag.app.vanderbilt.edu/PublicPage/Faculty/ViewAll The HTML provides PickLetter URLs for each letter of the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_uppercase\n",
    "outputTable = [['name', 'givenName', 'surname', 'degrees', 'rank', 'department', 'url', 'date']]\n",
    "\n",
    "for letter in ascii_uppercase:\n",
    "#if 1==1:\n",
    "    #letter = 'Q'\n",
    "    print(letter)\n",
    "    acceptMediaType = 'text/html'\n",
    "    url = 'https://wag.app.vanderbilt.edu//PublicPage/Faculty/PickLetter?letter=' + letter\n",
    "    response = requests.get(url, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    soupObject = BeautifulSoup(response.text,features=\"html5lib\")\n",
    "\n",
    "    # get the first table from the page\n",
    "    tableObject = soupObject.find_all('tbody')[0]\n",
    "\n",
    "    facultyItems = tableObject.find_all('tr')\n",
    "\n",
    "    for personRecord in facultyItems:\n",
    "        column = personRecord.find_all('td')\n",
    "        localUrl = column[0].find('a')\n",
    "        url = 'https://wag.app.vanderbilt.edu' + localUrl.get('href')\n",
    "        nameLastFirst = column[1].text.strip()\n",
    "        nameParts = nameLastFirst.split(',')\n",
    "        firstName = nameParts[1].strip()\n",
    "        lastName = nameParts[0].strip()\n",
    "        name = firstName + ' ' + lastName\n",
    "        degrees = column[2].text.strip()\n",
    "        title = column[3].text.strip()\n",
    "        department = column[4].text.strip()\n",
    "        #print(name, degrees, title, department, url)    \n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "\n",
    "\n",
    "        outputTable.append([name, firstName, lastName, degrees, title, department, url, wholeDateZ])            \n",
    "\n",
    "    fileName = 'medicine-faculty.csv'\n",
    "    writeCsv(fileName, outputTable)\n",
    "    sleep(0.25)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create departmental CSV\n",
    "\n",
    "Remove duplicates to get department list.\n",
    "\n",
    "Manually dereference each website and copy and paste some description.\n",
    "\n",
    "Also check that the labelEn is actually what they use. Manually created fields to match the Engineering departments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the script is done, I manually added a column named \"officialWebsiteLanguageQualifier\" and put the value \"Q1860\" (for English) in each row. This should just get added to the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
