{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import xml.etree.ElementTree as et\n",
    "from time import sleep\n",
    "import urllib\n",
    "\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'testApiScript' # give your application a name here\n",
    "\n",
    "wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "acceptMediaType = 'application/json'\n",
    "userAgentHeader = 'BaskaufScraper/0.1 (mailto:steve.baskauf@vanderbilt.edu)'\n",
    "requestHeaderDictionary = {\n",
    "    'Accept' : acceptMediaType,\n",
    "    'User-Agent': userAgentHeader\n",
    "    }\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsDict\n",
    "\n",
    "# returns a list of employer strings for the item with Wikidata ID qId;,P108 is employer\n",
    "def searchWikidataEmployer(qId):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?employer where {\n",
    "        wd:'''+ qId + ''' wdt:P108 ?employerId.\n",
    "        ?employerId rdfs:label ?employer.\n",
    "        FILTER(lang(?employer) = 'en')\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                resultsList.append(statement['employer']['value'])\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "# returns a list of results of articles by person with Wikidata ID qId\n",
    "def searchWikidataArticle(qId):\n",
    "    resultsList = []\n",
    "    # P50 is \"author\"; P698 is the PubMed ID of the article; P356 is the DOI of the article\n",
    "    query = '''select distinct ?title ?doi ?pmid where {\n",
    "      ?article wdt:P50 wd:''' + qId + '''.\n",
    "      optional {\n",
    "          ?article rdfs:label ?title.\n",
    "          FILTER(lang(?title) = 'en')\n",
    "          }\n",
    "      optional {?article wdt:P698 ?pmid.}\n",
    "      optional {?article wdt:P356 ?doi.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            if 'title' in statement:\n",
    "                title = statement['title']['value']\n",
    "                print('title=',title)\n",
    "            else:\n",
    "                title = ''\n",
    "            if 'pmid' in statement:\n",
    "                pmid = statement['pmid']['value']\n",
    "            else:\n",
    "                pmid = ''\n",
    "            if 'doi' in statement:\n",
    "                doi = statement['doi']['value']\n",
    "            else:\n",
    "                doi = ''\n",
    "            resultsList.append({'title': title, 'pmid': pmid, 'doi': doi})\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "def retrievePubMedData(pmid):\n",
    "    fetchUrl = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    paramDict = {\n",
    "        'tool': toolName, \n",
    "        'email': emailAddress,\n",
    "        'db': 'pubmed', \n",
    "         #'retmode': 'xml', \n",
    "        'rettype': 'abstract', \n",
    "        'id': pmid\n",
    "    }\n",
    "    response = requests.get(fetchUrl, params=paramDict)\n",
    "    #print(response.url)\n",
    "    pubData = response.text  # the response text is XML\n",
    "    #print(pubData)  # uncomment this line to see the XML\n",
    "\n",
    "    # process the returned XML, see https://docs.python.org/2/library/xml.etree.elementtree.html\n",
    "    root = et.fromstring(pubData)\n",
    "    try:\n",
    "        title = root.findall('.//ArticleTitle')[0].text\n",
    "    except:\n",
    "        title = ''\n",
    "    try:\n",
    "        print(title)\n",
    "    except:\n",
    "        print('')\n",
    "    names = root.findall('.//Author')\n",
    "    affiliations = []\n",
    "    for name in names:\n",
    "        try:\n",
    "            affiliation = name.find('./AffiliationInfo/Affiliation').text\n",
    "        except:\n",
    "            affiliation = ''\n",
    "        try:\n",
    "            lastName = name.find('./LastName').text\n",
    "        except:\n",
    "            lastName = ''\n",
    "        try:\n",
    "            foreName = name.find('./ForeName').text\n",
    "        except:\n",
    "            foreName = ''\n",
    "              \n",
    "        #print(lastName)\n",
    "        #print(affiliation)\n",
    "        affiliations.append({'affiliation': affiliation, 'surname': lastName, 'forename': foreName})\n",
    "    #print()\n",
    "\n",
    "    # See https://www.ncbi.nlm.nih.gov/books/NBK25497/ for usage guidelines. \n",
    "    # An API key is required for more than 3 requests per second.\n",
    "    sleep(0.5) # wait half a second before hitting the API again to avoid getting blocked\n",
    "    return affiliations\n",
    "\n",
    "def retrieveCrossRefDoi(doi):\n",
    "    authorList = []\n",
    "    crossRefEndpointUrl = 'https://api.crossref.org/works/'\n",
    "    encodedDoi = urllib.parse.quote(doi)\n",
    "    searchUrl = crossRefEndpointUrl + encodedDoi\n",
    "    response = requests.get(searchUrl, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = response.json()\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        if 'author' in data['message']:\n",
    "            authors = data['message']['author']\n",
    "            for author in authors:\n",
    "                authorDict = {}\n",
    "                if 'ORCID' in author:\n",
    "                    authorDict['orcid'] = author['ORCID']\n",
    "                else:\n",
    "                    authorDict['orcid'] = ''\n",
    "                if 'given' in author:\n",
    "                    authorDict['givenName'] = author['given']\n",
    "                else:\n",
    "                    authorDict['givenName'] = ''\n",
    "                if 'family' in author:\n",
    "                    authorDict['familyName'] = author['family']\n",
    "                else:\n",
    "                    authorDict['familyName'] = ''\n",
    "                affiliationList = []\n",
    "                if 'affiliation' in author:\n",
    "                    for affiliation in author['affiliation']:\n",
    "                        affiliationList.append(affiliation['name'])\n",
    "                # if there aren't any affiliations, the list will remain empty\n",
    "                authorDict['affiliation'] = affiliationList\n",
    "                authorList.append(authorDict)\n",
    "    except:\n",
    "        authorList = [data]\n",
    "    return authorList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieveCrossRefDoi('10.1111/jzo.12413'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qIds = [\"Q21503132\", \"Q45530486\", \"Q45579795\", \"Q45579952\", \"Q45580596\", \"Q45631936\", \"Q56480357\", \"Q57416670\", \"Q57550074\", \"Q59553435\", \"Q70150244\"]\n",
    "testString = 'Biological Sciences Vanderbilt'\n",
    "testAuthor = 'Peng Xu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qId in qIds:\n",
    "    result = searchWikidataArticle(qId)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if len(result) > 0:\n",
    "    if result[0]['pmid'] != '':\n",
    "        pubMedAuthors = retrievePubMedData(result[0])\n",
    "        print(pubMedAuthors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author in pubMedAuthors:\n",
    "    nameTestRatio = fuzz.token_set_ratio(author['surname'], testAuthor)\n",
    "    print(nameTestRatio, author['surname'])\n",
    "    if nameTestRatio >= 90:\n",
    "        setRatio = fuzz.token_set_ratio(testString, author['affiliation'])\n",
    "        print(setRatio, author['affiliation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qIds = [\"Q21503132\", \"Q45530486\", \"Q45579795\", \"Q45579952\", \"Q45580596\", \"Q45631936\", \"Q56480357\", \"Q57416670\", \"Q57550074\", \"Q59553435\", \"Q70150244\"]\n",
    "#qIds = [\"Q16910840\", \"Q64091655\", \"Q66741850\", \"Q67221376\"]\n",
    "qIds = [\"Q64091698\"]\n",
    "testString = 'Biological Sciences Vanderbilt'\n",
    "testEmployer = 'Vanderbilt University'\n",
    "#testAuthor = 'Peng Xu'\n",
    "#testAuthor = 'Thomas Clements'\n",
    "testAuthor = 'Larisa DeSantis'\n",
    "#testOrcid = '0000-0001-7103-3692'\n",
    "testOrcid = ''\n",
    "testOrcid = ''\n",
    "\n",
    "print('Checking identities for ', testAuthor)\n",
    "if testOrcid == '':\n",
    "    print('(no ORCID)')\n",
    "else:\n",
    "    print('ORCID: ', testOrcid)\n",
    "print()\n",
    "for qIdIndex in range(0, len(qIds)):\n",
    "    print(qIdIndex, 'Wikidata ID: ', qIds[qIdIndex])\n",
    "    descriptors = searchWikidataDescription(qIds[qIdIndex])\n",
    "    employers = searchWikidataEmployer(qIds[qIdIndex])\n",
    "    #print(descriptors)\n",
    "    if descriptors != {}:\n",
    "        if descriptors['description'] != '':\n",
    "            print('description: ', descriptors['description'])\n",
    "        for occupation in descriptors['occupation']:\n",
    "            print('occupation: ', occupation)\n",
    "        for employer in employers:\n",
    "            print('employer: ', employer)\n",
    "        if descriptors['orcid'] != '':\n",
    "            if testOrcid == '':\n",
    "                # **** NOTE: if the person has an ORCID, it may be possible to find articles via ORCID\n",
    "                # that aren't linked in Wikidata. Not sure if this happens often enough to handle it\n",
    "                print('ORCID: ', descriptors['orcid'])\n",
    "            else:\n",
    "                # This should always be true if the SPARQL query for ORCID was already done\n",
    "                if testOrcid != descriptors['orcid']:\n",
    "                    print('*** NOT the same person; ORCID ' + descriptors['orcid'] + ' does not match.')\n",
    "                    break # don't continue the loop (look up references) since it's definitely not a match\n",
    "                else:\n",
    "                    print('*** An ORCID match! How did it get missed in the earlier SPARQL query?')\n",
    "                    break\n",
    "    else:\n",
    "        print('No description or occupation given.')\n",
    "    \n",
    "    result = searchWikidataArticle(qIds[qIdIndex])\n",
    "    if len(result) == 0:\n",
    "        print('No articles authored by that person')\n",
    "    else:\n",
    "        foundMatch = False\n",
    "        for article in result:\n",
    "            print('Checking article: ', article['title'])\n",
    "            if article['pmid'] == '':\n",
    "                print('No PubMed ID')\n",
    "            else:\n",
    "                print('Checking authors in PubMed article: ', article['pmid'])\n",
    "                pubMedAuthors = retrievePubMedData(article['pmid'])\n",
    "                for author in pubMedAuthors:\n",
    "                    nameTestRatio = fuzz.token_set_ratio(author['surname'], testAuthor)\n",
    "                    print(nameTestRatio, author['surname'])\n",
    "                    if nameTestRatio >= 90:\n",
    "                        if author['affiliation'] != '': \n",
    "                            setRatio = fuzz.token_set_ratio(testString, author['affiliation'])\n",
    "                            print('Affiliation test: ', setRatio, author['affiliation'])\n",
    "                            if setRatio >= 90:\n",
    "                                foundMatch = True\n",
    "                        else:\n",
    "                            break # give up on this article because no affiliation string\n",
    "            # Don't look up the DOI if it's already found a match with PubMed\n",
    "            if foundMatch:\n",
    "                break # stop checking articles after one has matched\n",
    "            else:\n",
    "                if article['doi'] == '':\n",
    "                    print('No DOI')\n",
    "                else:\n",
    "                    print('Checking authors in DOI article: ', article['doi'])\n",
    "                    doiAuthors = retrieveCrossRefDoi(article['doi'])\n",
    "                    for author in doiAuthors:\n",
    "                        nameTestRatio = fuzz.token_set_ratio(author['familyName'], testAuthor)\n",
    "                        print(nameTestRatio, author['familyName'])\n",
    "                        if nameTestRatio >= 90:\n",
    "                            if author['orcid'] != '':\n",
    "                                if testOrcid == '':\n",
    "                                    print('ORCID from article: ', author['orcid'])\n",
    "                                else:\n",
    "                                    if testOrcid != author['orcid']:\n",
    "                                        print('*** NOT the same person; ORCID ' + author['orcid'] + ' does not match.')\n",
    "                                        break # don't continue the loop (look up references) since it's definitely not a match\n",
    "                                    else:\n",
    "                                        print('*** An ORCID match!')\n",
    "                                        foundMatch = True\n",
    "                                        break\n",
    "\n",
    "\n",
    "                            if len(author['affiliation']) > 0:\n",
    "                                for affiliation in author['affiliation']:\n",
    "                                    setRatio = fuzz.token_set_ratio(testString, affiliation)\n",
    "                                    print('Affiliation test: ', setRatio, affiliation)\n",
    "                                    if setRatio >= 90:\n",
    "                                        foundMatch = True\n",
    "                            #else:\n",
    "                            #    break # give up on this article because no affiliation string\n",
    "        if foundMatch:\n",
    "            print('***', qId, ' has a match.')\n",
    "        else:\n",
    "            print('No match found.')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
