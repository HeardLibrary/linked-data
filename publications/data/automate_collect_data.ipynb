{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, configuration, functions, etc.\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from time import sleep\n",
    "import csv\n",
    "import io\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from github import Github\n",
    "\n",
    "# the access token should be generated for read/write access to public repos\n",
    "# see https://developer.github.com/v3/auth/#working-with-two-factor-authentication\n",
    "# see https://github.com/settings/tokens/new\n",
    "# select public_repo\n",
    "\n",
    "# reference on PyGithub: https://pygithub.readthedocs.io/en/latest/github_objects/Repository.html\n",
    "# reference on GitHub API: https://developer.github.com/v3/guides/getting-started/\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "githubUsername = ''  # set to empty string if using a token (for 2FA)\n",
    "organizationName = 'baskaufs'\n",
    "organization_is_user = True\n",
    "repoName = 'test'\n",
    "credDirectory = 'home' # set to 'home' if the credential is in the home directory, otherwise working directory\n",
    "pathToDirectory = ''\n",
    "\n",
    "# -----------------\n",
    "# utility functions\n",
    "# -----------------\n",
    "\n",
    "# NOTE: change the user_agent_header string to something appropriate for your project\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderDataBot/0.1 (https://github.com/HeardLibrary/linked-data/tree/master/publications/data; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# read from a CSV file into a list of dictionaries (representing a table)\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        table = []\n",
    "        for row in dict_object:\n",
    "            table.append(row)\n",
    "    return table\n",
    "\n",
    "# read from a CSV file in GitHub into a list of dictionaries (representing a table)\n",
    "def read_dicts_from_github_csv(organizationName, repoName, pathToDirectory, filename):\n",
    "    path = pathToDirectory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organizationName + '/' + repoName + '/master/' + path)\n",
    "    file_text = r.text.split('\\n')\n",
    "    file_rows = csv.DictReader(file_text)\n",
    "    table = []\n",
    "    for row in file_rows:\n",
    "        table.append(row)\n",
    "    return table\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_string(table, fieldnames):\n",
    "    output = io.StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in table:\n",
    "        writer.writerow(row)\n",
    "    return output.getvalue()\n",
    "# -----------------\n",
    "# functions for interacting with APIs\n",
    "# -----------------\n",
    "\n",
    "# This function sends a query to a SPARQL endpoint and returns a single value.\n",
    "# For the Wikidata SPARQL endpoint, it extracts \"single_value\" from the query.\n",
    "def get_single_value(query, endpoint_url):\n",
    "    r = requests.get(endpoint_url, params={'query' : query}, headers = generate_header_dictionary(accept_media_type))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        #print(json.dumps(data, indent=2))\n",
    "        \n",
    "        # Extract value from response JSON depending on the API type\n",
    "        if endpoint_url == 'https://query.wikidata.org/sparql':\n",
    "            value = data['results']['bindings'][0]['single_value']['value']\n",
    "    except:\n",
    "        value = [r.text]\n",
    "    # delay to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return value\n",
    "\n",
    "# This function sends a query to the XTools Edit Counter and returns a single value.\n",
    "def get_xtools_edit_counts(username, project, namespace):\n",
    "    query_url = 'https://xtools.wmflabs.org/api/user/simple_editcount/' + project + '/' + username + '/' + namespace\n",
    "    #query_url = 'https://xtools.wmflabs.org/api/user/simple_editcount/' + project + '/' + username + '/' + namespace + '/' + start_date + '/' + end_date\n",
    "    r = requests.get(query_url, headers = generate_header_dictionary(accept_media_type))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        #print(json.dumps(data, indent=2))\n",
    "        \n",
    "        value = data['live_edit_count']\n",
    "    except:\n",
    "        value = [r.text]\n",
    "    # delay to avoid hitting the API to rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return value\n",
    "\n",
    "# -----------------\n",
    "# functions for interacting with GitHub\n",
    "# -----------------\n",
    "\n",
    "# value of directory should be either 'home' or 'working'\n",
    "def loadCredential(filename, directory):\n",
    "    cred = ''\n",
    "    # to change the script to look for the credential in the working directory, change the value of home to empty string\n",
    "    if directory == 'home':\n",
    "        home = str(Path.home()) #gets path to home directory; supposed to work for Win and Mac\n",
    "        credentialPath = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credentialPath = filename\n",
    "    try:\n",
    "        with open(credentialPath, 'rt', encoding='utf-8') as fileObject:\n",
    "            cred = fileObject.read()\n",
    "    except:\n",
    "        print(filename + ' file not found - is it in your ' + directory + ' directory?')\n",
    "        exit()\n",
    "    return(cred)\n",
    "\n",
    "# pass in an empty string for organizationName to use an individual account\n",
    "# pass in an empty string for githubUsername to use a token instead of username login\n",
    "def loginGetRepo(repoName, githubUsername, organizationName, organization_is_user, credDirectory):\n",
    "    if githubUsername == '':\n",
    "        token = loadCredential('test_token.txt', credDirectory)\n",
    "        g = Github(login_or_token = token)\n",
    "    else:\n",
    "        pwd = loadCredential('pwd.txt', credDirectory)\n",
    "        g = Github(githubUsername, pwd)\n",
    "    \n",
    "    if organization_is_user:\n",
    "        # this option accesses a user's repo instead of an organizational one\n",
    "        # In this case, the value of organizationName is not used.\n",
    "        user = g.get_user()\n",
    "        repo = user.get_repo(repoName)\n",
    "    else:\n",
    "        # this option creates an instance of a repo in an organization\n",
    "        # to which the token creator has push access\n",
    "        organization = g.get_organization(organizationName)\n",
    "        repo = organization.get_repo(repoName)\n",
    "    return(repo)\n",
    "\n",
    "def getUserList(repo):\n",
    "    personList = []\n",
    "    people = repo.get_collaborators()\n",
    "    for person in people:\n",
    "        personList.append(person.login)\n",
    "    return personList\n",
    "\n",
    "def getFileSha(account, repo, filePath):\n",
    "    # get the data about the file to get its blob SHA\n",
    "    r = requests.get('https://api.github.com/repos/' + account + '/' + repo + '/contents/' + filePath)\n",
    "    fileData = r.json()\n",
    "    try:\n",
    "        sha = fileData['sha']\n",
    "    except:\n",
    "        # if the file doesn't already exist on GitHub, no sha will be returned\n",
    "        sha = ''\n",
    "    return sha\n",
    "\n",
    "# use this function to update an existing text file\n",
    "def updateFile(account, repoName, pathToDirectory, filename, content):\n",
    "    path = pathToDirectory + filename\n",
    "    commitMessage = 'Update ' + filename + ' file via API'\n",
    "    sha = getFileSha(account, repoName, path)\n",
    "    if sha == '':\n",
    "        response = repo.create_file(path, commitMessage, content)\n",
    "    else:\n",
    "        response = repo.update_file(path, commitMessage, content, sha)\n",
    "    return response\n",
    "\n",
    "# -----------------\n",
    "# top-level functions for acquiring the main datasets\n",
    "# -----------------\n",
    "\n",
    "# Retrieves the total contributions for all of the participants in the VandyCite project\n",
    "# If it fails due to timeout or some other error, the table remains unchanged\n",
    "# Returns a raw CSV string\n",
    "def get_vandycite_contribution_counts(table):\n",
    "    # Get username list\n",
    "    vandycite_user_list = []\n",
    "    user_dicts = read_dicts_from_csv('vandycite_users.csv')\n",
    "    for dict in user_dicts:\n",
    "        vandycite_user_list.append(dict['username'])\n",
    "\n",
    "    # Retrieve data from XTools Edit Counter API\n",
    "    project = 'wikidata'\n",
    "    namespace = '0' # 0 is the main namespace\n",
    "\n",
    "    fieldnames = ['date'] + vandycite_user_list\n",
    "    today = generate_utc_date()\n",
    "    row_dict = {'date': today}\n",
    "\n",
    "    for username in vandycite_user_list:\n",
    "        print(username)\n",
    "        tries = 0\n",
    "        success = False\n",
    "        # try to acquire the data for an hour\n",
    "        while (success == False) and (tries < 12):\n",
    "            try:\n",
    "                count = get_xtools_edit_counts(username, project, namespace)\n",
    "                success = True\n",
    "                row_dict[username] = count\n",
    "            except:\n",
    "                tries += 1\n",
    "                sleep(300) # wait 5 minutes and try again\n",
    "    if success:\n",
    "        table.append(row_dict)\n",
    "\n",
    "    return write_dicts_to_string(table, fieldnames)\n",
    "\n",
    "# Runs all of the queries that retrieve a single value for the whole university\n",
    "# If it fails due to timeout or some other error, the table remains unchanged\n",
    "# Returns a raw CSV string\n",
    "def get_vu_counts(table):\n",
    "    all_vu_query_list = [\n",
    "        {'name': 'vu_total',\n",
    "        'query': '''\n",
    "        select (count(distinct ?person) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_men',\n",
    "        'query': '''\n",
    "        select (count(distinct ?man) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?man wdt:P1416 ?unit.\n",
    "          ?man wdt:P21 wd:Q6581097.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_women',\n",
    "        'query': '''\n",
    "        select (count(distinct ?woman) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?woman wdt:P1416 ?unit.\n",
    "          ?woman wdt:P21 wd:Q6581072.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_orcid',\n",
    "        'query': '''\n",
    "        select (count(distinct ?person) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          ?person wdt:P496 ?orcid.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_works',\n",
    "        'query': '''\n",
    "        select (count(distinct ?work) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          ?work wdt:P50 ?person.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_men_works',\n",
    "        'query': '''\n",
    "        select (count(distinct ?work) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?man wdt:P1416 ?unit.\n",
    "          ?man wdt:P21 wd:Q6581097.\n",
    "          ?work wdt:P50 ?man.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_women_works',\n",
    "        'query': '''\n",
    "        select (count(distinct ?work) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?woman wdt:P1416 ?unit.\n",
    "          ?woman wdt:P21 wd:Q6581072.\n",
    "          ?work wdt:P50 ?woman.\n",
    "          }\n",
    "        '''},\n",
    "    ]\n",
    "    #print(json.dumps(all_vu_query_list, indent=2))\n",
    "\n",
    "    # Retrieve data from Wikidata Query Service\n",
    "    endpoint_url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "    fieldnames = ['date']\n",
    "    today = generate_utc_date()\n",
    "    row_dict = {'date': today}\n",
    "\n",
    "    for query_dict in all_vu_query_list:\n",
    "        query_name = query_dict['name']\n",
    "        print(query_name)\n",
    "        fieldnames.append(query_name)\n",
    "        tries = 0\n",
    "        success = False\n",
    "\n",
    "        # try to acquire the data for an hour\n",
    "        while (success == False) and (tries < 12):\n",
    "            try:\n",
    "                count = get_single_value(query_dict['query'], endpoint_url)\n",
    "                success = True\n",
    "                row_dict[query_name] = count\n",
    "            except:\n",
    "                tries += 1\n",
    "                sleep(300) # wait 5 minutes and try again\n",
    "    if success:\n",
    "        table.append(row_dict)\n",
    "\n",
    "    return write_dicts_to_string(table, fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time checked: 2020-07-20T20:33:23.570305\n",
      "vu_total\n",
      "vu_men\n",
      "vu_women\n",
      "vu_orcid\n",
      "vu_works\n",
      "vu_men_works\n",
      "vu_women_works\n",
      "Item counts: {'commit': Commit(sha=\"755b7c874f514819e24b8e7a2de5e306987c15b1\"), 'content': ContentFile(path=\"vandycite_item_data.csv\")}\n",
      "Clifford_Anderson\n",
      "Baskaufs\n",
      "Fmlester\n",
      "Ramonavromero\n",
      "Talinum\n",
      "Celiaswalker\n",
      "CatonMA2\n",
      "Gridersd\n",
      "JeffBTaylor\n",
      "Marjans74\n",
      "Charlotte_Y._Lew\n",
      "KukanaLuika\n",
      "VanderBot\n",
      "Contributions {'commit': Commit(sha=\"abd311319da0075114dec06b8c37dbbd8bd478ad\"), 'content': ContentFile(path=\"vandycite_edit_data.csv\")}\n"
     ]
    }
   ],
   "source": [
    "print('Time checked:', datetime.datetime.utcnow().isoformat())\n",
    "# log into the GitHub API and create a repo instance\n",
    "repo = loginGetRepo(repoName, githubUsername, organizationName, organization_is_user, credDirectory)\n",
    "\n",
    "# Record today's Wikidata Vanderbilt-wide counts for items\n",
    "\n",
    "# Retrieve old copy of data from GitHub\n",
    "filename = 'vandycite_item_data.csv'\n",
    "table = read_dicts_from_github_csv(organizationName, repoName, pathToDirectory, filename)\n",
    "# Query the Wikidata Query Service to get today's data\n",
    "rawCsvText = get_vu_counts(table)\n",
    "# Write edited data back to GitHub via its API\n",
    "response = updateFile(organizationName, repoName, pathToDirectory, filename, rawCsvText)\n",
    "print('Item counts:', response)\n",
    "print()\n",
    "\n",
    "# Record today's counts of contrubutions to Wikidata by VandyCite members\n",
    "filename = 'vandycite_edit_data.csv'\n",
    "table = read_dicts_from_github_csv(organizationName, repoName, pathToDirectory, filename)\n",
    "rawCsvText = get_vandycite_contribution_counts(table)\n",
    "response = updateFile(organizationName, repoName, pathToDirectory, filename, rawCsvText)\n",
    "print('Contributions', response)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
