{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, configuration, functions, etc.\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from time import sleep\n",
    "import csv\n",
    "import io\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from github import Github\n",
    "\n",
    "# the access token should be generated for read/write access to public repos\n",
    "# see https://developer.github.com/v3/auth/#working-with-two-factor-authentication\n",
    "# see https://github.com/settings/tokens/new\n",
    "# select public_repo\n",
    "\n",
    "# reference on PyGithub: https://pygithub.readthedocs.io/en/latest/github_objects/Repository.html\n",
    "# reference on GitHub API: https://developer.github.com/v3/guides/getting-started/\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "github_username = ''  # set to empty string if using a token (for 2FA)\n",
    "organization_name = 'heardlibrary'\n",
    "organization_is_user = False\n",
    "repo_name = 'linked-data'\n",
    "cred_directory = 'home' # set to 'home' if the credential is in the home directory, otherwise working directory\n",
    "path_to_directory = 'publications/data/'\n",
    "\n",
    "# -----------------\n",
    "# utility functions\n",
    "# -----------------\n",
    "\n",
    "# NOTE: change the user_agent_header string to something appropriate for your project\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderDataBot/0.1 (https://github.com/HeardLibrary/linked-data/tree/master/publications/data; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# read from a CSV file into a list of dictionaries (representing a table)\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        table = []\n",
    "        for row in dict_object:\n",
    "            table.append(row)\n",
    "    return table\n",
    "\n",
    "# read raw string from a file in GitHub\n",
    "def read_string_from_github_file(organization_name, repo_name, path_to_directory, filename):\n",
    "    path = path_to_directory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organization_name + '/' + repo_name + '/master/' + path)\n",
    "    return r.text\n",
    "\n",
    "# read from a CSV file in GitHub into a list of dictionaries (representing a table)\n",
    "def read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, filename):\n",
    "    path = path_to_directory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organization_name + '/' + repo_name + '/master/' + path)\n",
    "    file_text = r.text.split('\\n')\n",
    "    file_rows = csv.DictReader(file_text)\n",
    "    table = []\n",
    "    for row in file_rows:\n",
    "        table.append(row)\n",
    "    return table\n",
    "\n",
    "# read from a CSV file in GitHub into a list of lists (representing a table)\n",
    "def read_lists_from_github_csv(organization_name, repo_name, path_to_directory, filename):\n",
    "    path = path_to_directory + filename\n",
    "    r = requests.get('https://raw.githubusercontent.com/' + organization_name + '/' + repo_name + '/master/' + path)\n",
    "    file_text = r.text.split('\\n')\n",
    "    # remove any trailing newlines\n",
    "    if file_text[len(file_text)-1] == '':\n",
    "        file_text = file_text[0:len(file_text)-1]\n",
    "    file_rows = csv.reader(file_text)\n",
    "    table = []\n",
    "    for row in file_rows:\n",
    "        table.append(row)\n",
    "    return table\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_string(table, fieldnames):\n",
    "    output = io.StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in table:\n",
    "        writer.writerow(row)\n",
    "    return output.getvalue()\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def write_lists_to_string(table):\n",
    "    output = io.StringIO()\n",
    "    writer = csv.writer(output)\n",
    "    for row in table:\n",
    "        writer.writerow(row)\n",
    "    return output.getvalue()\n",
    "\n",
    "# -----------------\n",
    "# functions for interacting with APIs\n",
    "# -----------------\n",
    "\n",
    "# This function sends a query to a SPARQL endpoint and returns a single value.\n",
    "# For the Wikidata SPARQL endpoint, it extracts \"single_value\" from the query.\n",
    "def get_single_value(query, endpoint_url):\n",
    "    r = requests.get(endpoint_url, params={'query' : query}, headers = generate_header_dictionary(accept_media_type))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        #print(json.dumps(data, indent=2))\n",
    "        \n",
    "        # Extract value from response JSON depending on the API type\n",
    "        if endpoint_url == 'https://query.wikidata.org/sparql':\n",
    "            value = data['results']['bindings'][0]['single_value']['value']\n",
    "    except:\n",
    "        value = [r.text]\n",
    "    # delay to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return value\n",
    "\n",
    "# This function sends a query to the Wikidata SPARQL endpoint that searches for\n",
    "# counts related to all subsidiary units of Vanderbilt. The function returns a list of\n",
    "# dictionaries with the Q ID and count for each unit.\n",
    "def get_unit_counts(query):\n",
    "    table = []\n",
    "    endpoint_url = 'https://query.wikidata.org/sparql'\n",
    "    accept_media_type = 'application/json'\n",
    "    r = requests.get(endpoint_url, params={'query' : query}, headers = generate_header_dictionary(accept_media_type))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            unit_iri = statement['unit']['value']\n",
    "            unit_qnumber = extract_qnumber(unit_iri)\n",
    "            count = statement['count']['value']\n",
    "            table.append({'unit': unit_qnumber, 'count': count})\n",
    "    except:\n",
    "        table = [r.text]\n",
    "    # delay to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return table\n",
    "\n",
    "# This function sends a query to the XTools Edit Counter and returns a single value.\n",
    "def get_xtools_edit_counts(username, project, namespace):\n",
    "    query_url = 'https://xtools.wmflabs.org/api/user/simple_editcount/' + project + '/' + username + '/' + namespace\n",
    "    #query_url = 'https://xtools.wmflabs.org/api/user/simple_editcount/' + project + '/' + username + '/' + namespace + '/' + start_date + '/' + end_date\n",
    "    r = requests.get(query_url, headers = generate_header_dictionary(accept_media_type))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        #print(json.dumps(data, indent=2))\n",
    "        \n",
    "        value = data['live_edit_count']\n",
    "    except:\n",
    "        value = [r.text]\n",
    "    # delay to avoid hitting the API to rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return value\n",
    "\n",
    "# -----------------\n",
    "# functions for interacting with GitHub\n",
    "# -----------------\n",
    "\n",
    "# value of directory should be either 'home' or 'working'\n",
    "def load_credential(filename, directory):\n",
    "    cred = ''\n",
    "    # to change the script to look for the credential in the working directory, change the value of home to empty string\n",
    "    if directory == 'home':\n",
    "        home = str(Path.home()) #gets path to home directory; supposed to work for Win and Mac\n",
    "        credential_path = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credential_path = filename\n",
    "    try:\n",
    "        with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "            cred = file_object.read()\n",
    "    except:\n",
    "        print(filename + ' file not found - is it in your ' + directory + ' directory?')\n",
    "        exit()\n",
    "    return(cred)\n",
    "\n",
    "# pass in an empty string for organization_name to use an individual account\n",
    "# pass in an empty string for github_username to use a token instead of username login\n",
    "def login_get_repo(repo_name, github_username, organization_name, organization_is_user, cred_directory):\n",
    "    if github_username == '':\n",
    "        token = load_credential('linked-data_github_token.txt', cred_directory)\n",
    "        g = Github(login_or_token = token)\n",
    "    else:\n",
    "        pwd = load_credential('pwd.txt', cred_directory)\n",
    "        g = Github(github_username, pwd)\n",
    "    \n",
    "    if organization_is_user:\n",
    "        # this option accesses a user's repo instead of an organizational one\n",
    "        # In this case, the value of organization_name is not used.\n",
    "        user = g.get_user()\n",
    "        repo = user.get_repo(repo_name)\n",
    "    else:\n",
    "        # this option creates an instance of a repo in an organization\n",
    "        # to which the token creator has push access\n",
    "        organization = g.get_organization(organization_name)\n",
    "        repo = organization.get_repo(repo_name)\n",
    "    return(repo)\n",
    "\n",
    "def get_user_list(repo):\n",
    "    person_list = []\n",
    "    people = repo.get_collaborators()\n",
    "    for person in people:\n",
    "        person_list.append(person.login)\n",
    "    return person_list\n",
    "\n",
    "def get_file_sha(account, repo, file_path):\n",
    "    # get the data about the file to get its blob SHA\n",
    "    r = requests.get('https://api.github.com/repos/' + account + '/' + repo + '/contents/' + file_path)\n",
    "    file_data = r.json()\n",
    "    try:\n",
    "        sha = file_data['sha']\n",
    "    except:\n",
    "        # if the file doesn't already exist on GitHub, no sha will be returned\n",
    "        sha = ''\n",
    "    return sha\n",
    "\n",
    "# use this function to update an existing text file\n",
    "def update_file(account, repo_name, path_to_directory, filename, content):\n",
    "    path = path_to_directory + filename\n",
    "    commit_message = 'Update ' + filename + ' file via API'\n",
    "    sha = get_file_sha(account, repo_name, path)\n",
    "    if sha == '':\n",
    "        response = repo.create_file(path, commit_message, content)\n",
    "    else:\n",
    "        response = repo.update_file(path, commit_message, content, sha)\n",
    "    return response\n",
    "\n",
    "# -----------------\n",
    "# top-level functions for acquiring the main datasets\n",
    "# -----------------\n",
    "\n",
    "# Retrieves the total contributions for all of the participants in the VandyCite project\n",
    "# If it fails due to timeout or some other error, the table remains unchanged\n",
    "# Returns a raw CSV string\n",
    "def get_vandycite_contribution_counts(organization_name, repo_name, path_to_directory, table):\n",
    "    # Get username list\n",
    "    vandycite_user_list = []\n",
    "    user_dicts = read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, 'vandycite_users.csv')\n",
    "    for dict in user_dicts:\n",
    "        vandycite_user_list.append(dict['username'])\n",
    "\n",
    "    # Retrieve data from XTools Edit Counter API\n",
    "    project = 'wikidata'\n",
    "    namespace = '0' # 0 is the main namespace\n",
    "\n",
    "    fieldnames = ['date'] + vandycite_user_list\n",
    "    today = generate_utc_date()\n",
    "    row_dict = {'date': today}\n",
    "\n",
    "    total = 0\n",
    "    for username in vandycite_user_list:\n",
    "        print(username)\n",
    "        tries = 0\n",
    "        success = False\n",
    "        # try to acquire the data for an hour\n",
    "        while (success == False) and (tries < 12):\n",
    "            try:\n",
    "                count = get_xtools_edit_counts(username, project, namespace)\n",
    "                success = True\n",
    "                row_dict[username] = count\n",
    "                total += int(count)\n",
    "            except:\n",
    "                tries += 1\n",
    "                sleep(300) # wait 5 minutes and try again\n",
    "    row_dict['total'] = str(total)\n",
    "    \n",
    "    if success:\n",
    "        table.append(row_dict)\n",
    "\n",
    "    return write_dicts_to_string(table, fieldnames)\n",
    "\n",
    "# Runs all of the queries that retrieve a single value for the whole university\n",
    "# If it fails due to timeout or some other error, the table remains unchanged\n",
    "# Returns a raw CSV string\n",
    "def get_vu_counts(table):\n",
    "    all_vu_query_list = [\n",
    "        {'name': 'vu_total',\n",
    "        'query': '''\n",
    "        select (count(distinct ?person) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_men',\n",
    "        'query': '''\n",
    "        select (count(distinct ?man) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?man wdt:P1416 ?unit.\n",
    "          ?man wdt:P21 wd:Q6581097.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_women',\n",
    "        'query': '''\n",
    "        select (count(distinct ?woman) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?woman wdt:P1416 ?unit.\n",
    "          ?woman wdt:P21 wd:Q6581072.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_orcid',\n",
    "        'query': '''\n",
    "        select (count(distinct ?person) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          ?person wdt:P496 ?orcid.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_works',\n",
    "        'query': '''\n",
    "        select (count(distinct ?work) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          ?work wdt:P50 ?person.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_men_works',\n",
    "        'query': '''\n",
    "        select (count(distinct ?work) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?man wdt:P1416 ?unit.\n",
    "          ?man wdt:P21 wd:Q6581097.\n",
    "          ?work wdt:P50 ?man.\n",
    "          }\n",
    "        '''},\n",
    "        {'name': 'vu_women_works',\n",
    "        'query': '''\n",
    "        select (count(distinct ?work) as ?single_value)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?woman wdt:P1416 ?unit.\n",
    "          ?woman wdt:P21 wd:Q6581072.\n",
    "          ?work wdt:P50 ?woman.\n",
    "          }\n",
    "        '''},\n",
    "    ]\n",
    "    #print(json.dumps(all_vu_query_list, indent=2))\n",
    "\n",
    "    # Retrieve data from Wikidata Query Service\n",
    "    endpoint_url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "    fieldnames = ['date']\n",
    "    today = generate_utc_date()\n",
    "    row_dict = {'date': today}\n",
    "\n",
    "    for query_dict in all_vu_query_list:\n",
    "        query_name = query_dict['name']\n",
    "        print(query_name)\n",
    "        fieldnames.append(query_name)\n",
    "        tries = 0\n",
    "        success = False\n",
    "\n",
    "        # try to acquire the data for an hour\n",
    "        while (success == False) and (tries < 12):\n",
    "            try:\n",
    "                count = get_single_value(query_dict['query'], endpoint_url)\n",
    "                success = True\n",
    "                row_dict[query_name] = count\n",
    "            except:\n",
    "                tries += 1\n",
    "                sleep(300) # wait 5 minutes and try again\n",
    "    if success:\n",
    "        table.append(row_dict)\n",
    "\n",
    "    return write_dicts_to_string(table, fieldnames)\n",
    "\n",
    "def get_unit_affiliation_queries():\n",
    "    units_query_list = [\n",
    "        {'name': 'units_total',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?person) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''},\n",
    "        {'name': 'units_women',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?woman) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?woman wdt:P1416 ?unit.\n",
    "          ?woman wdt:P21 wd:Q6581072.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''},\n",
    "        {'name': 'units_men',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?man) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?man wdt:P1416 ?unit.\n",
    "          ?man wdt:P21 wd:Q6581097.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''},\n",
    "        {'name': 'units_orcid',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?person) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          ?person wdt:P496 ?orcid.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''},\n",
    "        {'name': 'units_works',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?work) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?person wdt:P1416 ?unit.\n",
    "          ?work wdt:P50 ?person.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''},\n",
    "        {'name': 'units_works_men',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?work) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?man wdt:P1416 ?unit.\n",
    "          ?man wdt:P21 wd:Q6581097.\n",
    "          ?work wdt:P50 ?man.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''},\n",
    "        {'name': 'units_works_women',\n",
    "        'query': '''\n",
    "        select ?unit (count(distinct ?work) as ?count)  where {\n",
    "          ?unit wdt:P749+ wd:Q29052.\n",
    "          ?woman wdt:P1416 ?unit.\n",
    "          ?woman wdt:P21 wd:Q6581072.\n",
    "          ?work wdt:P50 ?woman.\n",
    "          }\n",
    "        group by ?unit\n",
    "        '''}\n",
    "    ]\n",
    "    return units_query_list\n",
    "\n",
    "# This retrieves counts by unit for a particular query type, then appends the results\n",
    "# for all of the units as a new row in the table.\n",
    "# NOTE: unlike the other functions, the table here is a list of lists, not list of dicts.\n",
    "# The first column must be the date.\n",
    "def add_query_to_unit_table(table, query):\n",
    "    date = generate_utc_date()\n",
    "    tries = 0\n",
    "    success = False\n",
    "\n",
    "    # try to acquire the data for an hour\n",
    "    while (success == False) and (tries < 12):\n",
    "        try:\n",
    "            dictionary = get_unit_counts(query)\n",
    "            success = True\n",
    "            row_list = [date]\n",
    "            for header in table[0][1:len(table[0])]: # skip the first item (date)\n",
    "                found = False\n",
    "                for count in dictionary:\n",
    "                    if count['unit'] == header:\n",
    "                        found = True\n",
    "                        row_list.append(count['count'])\n",
    "                if not found:\n",
    "                    row_list.append('0')\n",
    "        except:\n",
    "            tries += 1\n",
    "            sleep(300) # wait 5 minutes and try again\n",
    "    if success:\n",
    "        table.append(row_list)\n",
    "    return write_lists_to_string(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time checked: 2020-07-23T01:20:52.292060\n",
      "Date last run: 2020-07-23\n",
      "UTC date now is: 2020-07-23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "while True: # infinite loop\n",
    "    print('Time checked:', datetime.datetime.utcnow().isoformat())\n",
    "\n",
    "    date_last_run = read_string_from_github_file(organization_name, repo_name, path_to_directory, 'last_run.txt')\n",
    "    print('Date last run:', date_last_run)\n",
    "\n",
    "    date_now_utc = generate_utc_date()\n",
    "    print('UTC date now is:', date_now_utc)\n",
    "\n",
    "    if date_now_utc > date_last_run:\n",
    "        # log into the GitHub API and create a repo instance\n",
    "        repo = login_get_repo(repo_name, github_username, organization_name, organization_is_user, cred_directory)\n",
    "\n",
    "        # Record today's Wikidata Vanderbilt-wide counts for items\n",
    "        print('Item counts (university-wide):')\n",
    "\n",
    "        # Retrieve old copy of data from GitHub\n",
    "        filename = 'vandycite_item_data.csv'\n",
    "        table = read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, filename)\n",
    "        # Query the Wikidata Query Service to get today's data\n",
    "        rawCsvText = get_vu_counts(table)\n",
    "        # Write edited data back to GitHub via its API\n",
    "        response = update_file(organization_name, repo_name, path_to_directory, filename, rawCsvText)\n",
    "        print(response)\n",
    "        print()\n",
    "\n",
    "        # Record today's Wikidata item counts by Vanderbilt subsidiary unit\n",
    "        print('Item counts by unit:')\n",
    "        queries = get_unit_affiliation_queries()\n",
    "\n",
    "        for query_dict in queries:\n",
    "            print(query_dict['name'])\n",
    "            filename = query_dict['name'] + '.csv'\n",
    "            table = read_lists_from_github_csv(organization_name, repo_name, path_to_directory, filename)\n",
    "            rawCsvText = add_query_to_unit_table(table, query_dict['query'])\n",
    "            #print(rawCsvText)\n",
    "            response = update_file(organization_name, repo_name, path_to_directory, filename, rawCsvText)\n",
    "            print(response)\n",
    "        print()\n",
    "\n",
    "        # Record today's counts of contrubutions to Wikidata by VandyCite members\n",
    "        print('Contributions')\n",
    "        filename = 'vandycite_edit_data.csv'\n",
    "        table = read_dicts_from_github_csv(organization_name, repo_name, path_to_directory, filename)\n",
    "        rawCsvText = get_vandycite_contribution_counts(organization_name, repo_name, path_to_directory, table)\n",
    "        response = update_file(organization_name, repo_name, path_to_directory, filename, rawCsvText)\n",
    "        print(response)\n",
    "\n",
    "        # Update the date last run\n",
    "        response = update_file(organization_name, repo_name, path_to_directory, 'last_run.txt', generate_utc_date() )\n",
    "        print('done')\n",
    "    print()\n",
    "    # wait an hour before checking again\n",
    "    sleep(3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
