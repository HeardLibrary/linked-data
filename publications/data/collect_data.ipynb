{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, configuration, functions, etc.\n",
    "# Run this every time you use the script\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from time import sleep\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "sparqlSleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "\n",
    "def generateHeaderDictionary(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/1.0 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "def generate_utc_date():\n",
    "    wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "    return dateZ\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def writeListsToCsv(fileName, array):\n",
    "    with open(fileName, 'w', newline='', encoding='utf-8') as fileObject:\n",
    "        writerObject = csv.writer(fileObject)\n",
    "        for row in array:\n",
    "            writerObject.writerow(row)\n",
    "\n",
    "\n",
    "def read_lists_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        reader_object = csv.reader(file_object)\n",
    "        list_of_lists = []\n",
    "        for row_list in reader_object:\n",
    "            list_of_lists.append(row_list)\n",
    "    return list_of_lists\n",
    "\n",
    "\n",
    "            # write a list of dictionaries to a CSV file\n",
    "def writeDictsToCsv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvFileObject:\n",
    "        writer = csv.DictWriter(csvFileObject, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "            # read from a CSV file into a list of dictionaries\n",
    "def readDict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as fileObject:\n",
    "        dictObject = csv.DictReader(fileObject)\n",
    "        array = []\n",
    "        for row in dictObject:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "def get_vu_counts(query):\n",
    "    wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        #print(json.dumps(data, indent=2))\n",
    "        count = data['results']['bindings'][0]['count']['value']\n",
    "    except:\n",
    "        count = [r.text]\n",
    "    # delay to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(sparqlSleep)\n",
    "    return count\n",
    "\n",
    "def get_unit_counts(query):\n",
    "    table = []\n",
    "    wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            unit_iri = statement['unit']['value']\n",
    "            unit_qnumber = extract_qnumber(unit_iri)\n",
    "            count = statement['count']['value']\n",
    "            table.append({'unit': unit_qnumber, 'count': count})\n",
    "    except:\n",
    "        table = [r.text]\n",
    "    # delay to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(sparqlSleep)\n",
    "    return table\n",
    "\n",
    "def add_query_to_vu_table(filename, query):\n",
    "    table = read_lists_from_csv(filename)\n",
    "    #print(table)\n",
    "\n",
    "    count = get_vu_counts(query)\n",
    "    #print(count)\n",
    "    \n",
    "    date = generate_utc_date()\n",
    "    row_list = [date]\n",
    "    row_list.append(count)\n",
    "    #print(row_list)\n",
    "    table.append(row_list)\n",
    "    #print(table)\n",
    "\n",
    "    writeListsToCsv(filename, table)\n",
    "\n",
    "def add_query_to_unit_table(filename, query):\n",
    "    table = read_lists_from_csv(filename)\n",
    "    #print(table)\n",
    "\n",
    "    dictionary = get_unit_counts(query)\n",
    "    #print(json.dumps(dictionary, indent=2))\n",
    "    \n",
    "    date = generate_utc_date()\n",
    "    row_list = [date]\n",
    "    for header in table[0][1:len(table[0])]: # skip the first item (date)\n",
    "        found = False\n",
    "        for count in dictionary:\n",
    "            if count['unit'] == header:\n",
    "                found = True\n",
    "                row_list.append(count['count'])\n",
    "        if not found:\n",
    "            row_list.append('0')\n",
    "    #print(row_list)\n",
    "    table.append(row_list)\n",
    "\n",
    "    writeListsToCsv(filename, table)\n",
    "    \n",
    "def run_all_queries():\n",
    "    # -----------------------------\n",
    "    # Queries for all of Vanderbilt\n",
    "    # -----------------------------\n",
    "\n",
    "    # query to get the total number of persons affiliated with Vanderbilt units\n",
    "    query = '''\n",
    "    select (count(distinct ?person) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?person wdt:P1416 ?unit.\n",
    "      }\n",
    "    '''\n",
    "    filename = 'vu_total.csv'\n",
    "    add_query_to_vu_table(filename, query)\n",
    "    print(filename)\n",
    "\n",
    "    # query to get the total number of men affiliated with Vanderbilt units\n",
    "    query = '''\n",
    "    select (count(distinct ?man) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?man wdt:P1416 ?unit.\n",
    "      ?man wdt:P21 wd:Q6581097.\n",
    "      }\n",
    "    '''\n",
    "    filename = 'vu_men.csv'\n",
    "    add_query_to_vu_table(filename, query)\n",
    "    print(filename)\n",
    "\n",
    "    # query to get the total number of women affiliated with Vanderbilt units\n",
    "    query = '''\n",
    "    select (count(distinct ?woman) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?woman wdt:P1416 ?unit.\n",
    "      ?woman wdt:P21 wd:Q6581072.\n",
    "      }\n",
    "    '''\n",
    "    filename = 'vu_women.csv'\n",
    "    add_query_to_vu_table(filename, query)\n",
    "    print(filename)\n",
    "\n",
    "    # query to count number of people with ORCIDs\n",
    "    query = '''\n",
    "    select (count(distinct ?person) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?person wdt:P1416 ?unit.\n",
    "      ?person wdt:P496 ?orcid.\n",
    "      }\n",
    "    '''\n",
    "    filename = 'vu_orcid.csv'\n",
    "    add_query_to_vu_table(filename, query)\n",
    "    print(filename)\n",
    "    \n",
    "    # query to get the total number works authored by anyone affiliated with Vanderbilt units\n",
    "    query = '''\n",
    "    select (count(distinct ?work) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?person wdt:P1416 ?unit.\n",
    "      ?work wdt:P50 ?person.\n",
    "      }\n",
    "    '''\n",
    "    filename = 'vu_works.csv'\n",
    "    add_query_to_vu_table(filename, query)\n",
    "    print(filename)\n",
    "\n",
    "    # query to get the total number works authored by men affiliated with Vanderbilt units\n",
    "    query = '''\n",
    "    select (count(distinct ?work) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?man wdt:P1416 ?unit.\n",
    "      ?man wdt:P21 wd:Q6581097.\n",
    "      ?work wdt:P50 ?man.\n",
    "      }\n",
    "    '''\n",
    "    filename = 'vu_men_works.csv'\n",
    "    add_query_to_vu_table(filename, query)\n",
    "    print(filename)\n",
    "\n",
    "    # query to get the total number works authored by women affiliated with Vanderbilt units\n",
    "    query = '''\n",
    "    select (count(distinct ?work) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?woman wdt:P1416 ?unit.\n",
    "      ?woman wdt:P21 wd:Q6581072.\n",
    "      ?work wdt:P50 ?woman.\n",
    "      }\n",
    "    '''\n",
    "    filename = 'vu_women_works.csv'\n",
    "    add_query_to_vu_table(filename, query)\n",
    "    print(filename)\n",
    "\n",
    "\n",
    "\n",
    "    # ------------------------\n",
    "    # Query by Vanderbilt unit\n",
    "    # ------------------------\n",
    "\n",
    "    # query to get the total number of persons affiliated with each unit\n",
    "    query = '''\n",
    "    select ?unit (count(distinct ?person) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?person wdt:P1416 ?unit.\n",
    "      }\n",
    "    group by ?unit\n",
    "    '''\n",
    "    filename = 'units_total.csv'\n",
    "    add_query_to_unit_table(filename, query)\n",
    "    print(filename)\n",
    "\n",
    "    # query to get the total number of women affiliated with each unit\n",
    "    query = '''\n",
    "    select ?unit (count(distinct ?woman) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?woman wdt:P1416 ?unit.\n",
    "      ?woman wdt:P21 wd:Q6581072.\n",
    "      }\n",
    "    group by ?unit\n",
    "    '''\n",
    "    filename = 'units_women.csv'\n",
    "    add_query_to_unit_table(filename, query)\n",
    "    print(filename)\n",
    "\n",
    "    # query to get the total number of men affiliated with each unit\n",
    "    query = '''\n",
    "    select ?unit (count(distinct ?man) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?man wdt:P1416 ?unit.\n",
    "      ?man wdt:P21 wd:Q6581097.\n",
    "      }\n",
    "    group by ?unit\n",
    "    '''\n",
    "    filename = 'units_men.csv'\n",
    "    add_query_to_unit_table(filename, query)\n",
    "    print(filename)\n",
    "\n",
    "    # query to get the total number of people having ORCIDs affiliated with each unit\n",
    "    query = '''\n",
    "    select ?unit (count(distinct ?person) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?person wdt:P1416 ?unit.\n",
    "      ?person wdt:P496 ?orcid.\n",
    "      }\n",
    "    group by ?unit\n",
    "    '''\n",
    "    filename = 'units_orcid.csv'\n",
    "    add_query_to_unit_table(filename, query)\n",
    "    print(filename)\n",
    "\n",
    "    # query to get the total number of authored works associated with each unit\n",
    "    query = '''\n",
    "    select ?unit (count(distinct ?work) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?person wdt:P1416 ?unit.\n",
    "      ?work wdt:P50 ?person.\n",
    "      }\n",
    "    group by ?unit\n",
    "    '''\n",
    "    filename = 'units_works.csv'\n",
    "    add_query_to_unit_table(filename, query)\n",
    "    print(filename)\n",
    "\n",
    "    # query to get the total number of authored works by men affiliated with each unit\n",
    "    query = '''\n",
    "    select ?unit (count(distinct ?work) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?man wdt:P1416 ?unit.\n",
    "      ?man wdt:P21 wd:Q6581097.\n",
    "      ?work wdt:P50 ?man.\n",
    "      }\n",
    "    group by ?unit\n",
    "    '''\n",
    "    filename = 'units_works_men.csv'\n",
    "    add_query_to_unit_table(filename, query)\n",
    "    print(filename)\n",
    "\n",
    "    # query to get the total number of authored works by women affiliated with each unit\n",
    "    query = '''\n",
    "    select ?unit (count(distinct ?work) as ?count)  where {\n",
    "      ?unit wdt:P749+ wd:Q29052.\n",
    "      ?woman wdt:P1416 ?unit.\n",
    "      ?woman wdt:P21 wd:Q6581072.\n",
    "      ?work wdt:P50 ?woman.\n",
    "      }\n",
    "    group by ?unit\n",
    "    '''\n",
    "    filename = 'units_works_women.csv'\n",
    "    add_query_to_unit_table(filename, query)\n",
    "    print(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# ---------------\n",
    "# Run this once the first time you set up\n",
    "# !!! Running it again will wipe out all of your data !!!\n",
    "# ---------------\n",
    "\n",
    "# Get the Wikidata IDs and names for Vanderbilt Units\n",
    "\n",
    "# create a string for the query\n",
    "query = '''\n",
    "select ?label ?unit ?parent where {\n",
    "  ?unit wdt:P749+ wd:Q29052.  # Q29052 is Vanderbilt\n",
    "  ?unit wdt:P749 ?parent.\n",
    "  ?unit rdfs:label ?label.\n",
    "  filter(lang(?label) = 'en')\n",
    "  }\n",
    "order by ?label\n",
    "'''\n",
    "\n",
    "#print(query)\n",
    "\n",
    "unit_table = []\n",
    "wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "acceptMediaType = 'application/json'\n",
    "r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers = generateHeaderDictionary(acceptMediaType))\n",
    "try:\n",
    "    data = r.json()\n",
    "    statements = data['results']['bindings']\n",
    "    for statement in statements:\n",
    "        unit_iri = statement['unit']['value']\n",
    "        unit_qnumber = extract_qnumber(unit_iri)\n",
    "        parent_iri = statement['parent']['value']\n",
    "        parent_qnumber = extract_qnumber(parent_iri)\n",
    "        unit_label = statement['label']['value']\n",
    "        unit_table.append({'unit': unit_qnumber, 'label': unit_label, 'parent': parent_qnumber})\n",
    "except:\n",
    "    unit_table = [r.text]\n",
    "# delay a quarter second to avoid hitting the SPARQL endpoint to rapidly\n",
    "#sleep(sparqlSleep)\n",
    "\n",
    "#print(json.dumps(unit_table, indent=2))\n",
    "\n",
    "writeDictsToCsv(unit_table, 'vanderbilt_units.csv', ['unit', 'label', 'parent'])\n",
    "\n",
    "\n",
    "# create blank files for institution-wide data\n",
    "\n",
    "unit_files = ['vu_total.csv', 'vu_men.csv', 'vu_works.csv', 'vu_women.csv', 'vu_orcid.csv', 'vu_men_works.csv', 'vu_women_works.csv']\n",
    "\n",
    "for file_name in unit_files:\n",
    "    header_row = ['date', 'count']\n",
    "    header_table = [header_row]\n",
    "    writeListsToCsv(file_name, header_table)\n",
    "\n",
    "# create blank files for units data\n",
    "\n",
    "unit_files = ['units_total.csv', 'units_women.csv', 'units_men.csv', 'units_orcid.csv', 'units_works.csv', 'units_works_men.csv', 'units_works_women.csv']\n",
    "\n",
    "for file_name in unit_files:\n",
    "    header_row = ['date']\n",
    "    for unit in unit_table:\n",
    "        header_row.append(unit['unit'])\n",
    "    header_table = [header_row]\n",
    "    writeListsToCsv(file_name, header_table)\n",
    "    \n",
    "# Write date when last run\n",
    "with open('last_run.txt', 'wt', encoding='utf-8') as fileObject:\n",
    "    fileObject.write(generate_utc_date())\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time checked: 2020-05-05T23:38:14.765565\n",
      "Date last run: 2020-05-04\n",
      "UTC date now is: 2020-05-05\n",
      "vu_total.csv\n",
      "vu_men.csv\n",
      "vu_women.csv\n",
      "vu_orcid.csv\n",
      "vu_works.csv\n",
      "vu_men_works.csv\n",
      "vu_women_works.csv\n",
      "units_total.csv\n",
      "units_women.csv\n",
      "units_men.csv\n",
      "units_orcid.csv\n",
      "units_works.csv\n",
      "units_works_men.csv\n",
      "units_works_women.csv\n",
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Script to actually collect the data\n",
    "\n",
    "#data = readDict('vanderbilt_units.csv')\n",
    "#print(json.dumps(data,indent=2))\n",
    "\n",
    "while True: # infinite loop\n",
    "    print('Time checked:', datetime.datetime.utcnow().isoformat())\n",
    "    with open('last_run.txt', 'rt', encoding='utf-8') as fileObject:\n",
    "        date_last_run = fileObject.read()\n",
    "    print('Date last run:', date_last_run)\n",
    "\n",
    "    date_now_utc = generate_utc_date()\n",
    "    print('UTC date now is:', date_now_utc)\n",
    "\n",
    "    if date_now_utc > date_last_run:\n",
    "        run_all_queries()\n",
    "\n",
    "        # Update the date last run\n",
    "        with open('last_run.txt', 'wt', encoding='utf-8') as fileObject:\n",
    "            fileObject.write(generate_utc_date())\n",
    "\n",
    "        print('done')\n",
    "    print()\n",
    "\n",
    "    # wait an hour before checking again\n",
    "    sleep(3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
