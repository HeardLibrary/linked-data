{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VanderBot\n",
    "\n",
    "The scripts in this notebook are part of the development of VanderBot, a system to write information about Vanderbilt University researchers and their works to Wikidata.  \n",
    "\n",
    "This code is freely available under a CC0 license. Steve Baskauf 2019-12-16\n",
    "\n",
    "VanderBot 0.8 is under development and subject to continual change. At this point, it's too new to have any stable releases.  \n",
    "\n",
    "For more information, see [this page](https://github.com/HeardLibrary/linked-data/tree/master/publications).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Code\n",
    "\n",
    "This code block includes import statements, function definitions, and declarations of variables that are common to the rest of the script. It needs to be run once before the other code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "# For a particular processing round, set a short name for the department here.\n",
    "# This name is used to generate a set of unique processing files for that department.\n",
    "deptShortName = 'physics'\n",
    "testEmployer = 'Vanderbilt University' # to test against Wikidata employer property\n",
    "employerQId = 'Q29052' # Vanderbilt University\n",
    "deathDateLimit = '2000' # any date deaths before this date will be assumed to not be a match\n",
    "\n",
    "# NOTE: eventually need to test against all affiliations in cases of faculty with multiple appointments\n",
    "\n",
    "# ***********************************\n",
    "# NOTE: the script fails if there is a current item in Wikidata that has the same values for both label and description. \n",
    "# A check needs to be run for this !!!\n",
    "# ***********************************\n",
    "\n",
    "# The default labels and descriptions can either be a column in the table or set as a constant. \n",
    "# If it's a column, the value is the column header.  If it's a constant, the value is the string to assign as the value.\n",
    "deptSettings = {\n",
    "    'bsci': {\n",
    "        'categories': ['primary-training-faculty', 'research-and-teaching-faculty', 'secondary-faculty', 'postdoc-fellows', 'emeriti'],\n",
    "        'baseUrl': 'https://as.vanderbilt.edu/biosci/people/index.php?group=',\n",
    "        'departmentSearchString': 'Biological Sciences',\n",
    "        'departmentQId': 'Q78041310',\n",
    "        'testAuthorAffiliation': 'Biological Sciences Vanderbilt',\n",
    "        \"labels\": \n",
    "            {\n",
    "                \"source\": \"column\",\n",
    "                \"value\": \"name\"\n",
    "            },\n",
    "        \"descriptions\": \n",
    "            {\n",
    "                \"source\": \"constant\",\n",
    "                \"value\": \"biology researcher\"\n",
    "            }\n",
    "    },\n",
    "    'physics': {\n",
    "        'categories': ['faculty', 'emeritus-faculty', 'academic-research-staff'],\n",
    "        'baseUrl': 'https://as.vanderbilt.edu/physics/people/index.php?group=',\n",
    "        'departmentSearchString': 'Physics Astronomy',\n",
    "        'departmentQId': 'Q78779260',\n",
    "        'testAuthorAffiliation': 'Physics Astronomy Vanderbilt',\n",
    "        \"labels\": \n",
    "            {\n",
    "                \"source\": \"column\",\n",
    "                \"value\": \"name\"\n",
    "            },\n",
    "        \"descriptions\": \n",
    "            {\n",
    "                \"source\": \"constant\",\n",
    "                \"value\": \"physics/astronomy researcher\"\n",
    "            }\n",
    "    }\n",
    "}\n",
    "\n",
    "wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "degreeList = [\n",
    "    {'string': 'Ph.D.', 'value': 'Ph.D.'},\n",
    "    {'string': 'PhD', 'value': 'Ph.D.'},\n",
    "    {'string': 'D.Phil.', 'value': 'D.Phil.'},\n",
    "    {'string': 'J.D.', 'value': 'J.D.'}\n",
    "     ]\n",
    "\n",
    "# NCBI identification requirements:\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'VanderBot' # give your application a name here\n",
    "\n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generateHeaderDictionary(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/0.8 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def writeCsv(fileName, array):\n",
    "    fileObject = open(fileName, 'w', newline='', encoding='utf-8')\n",
    "    writerObject = csv.writer(fileObject)\n",
    "    for row in array:\n",
    "        writerObject.writerow(row)\n",
    "    fileObject.close()\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def writeDictsToCsv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='') as csvFileObject:\n",
    "        writer = csv.DictWriter(csvFileObject, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def readDict(filename):\n",
    "    fileObject = open(filename, 'r', newline='', encoding='utf-8')\n",
    "    dictObject = csv.DictReader(fileObject)\n",
    "    array = []\n",
    "    for row in dictObject:\n",
    "        array.append(row)\n",
    "    fileObject.close()\n",
    "    return array\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extractQNumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts a local name from an IRI, specify the list item number for the last piece separated by slash\n",
    "def extractFromIri(iri, numberPieces):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[numberPieces]\n",
    "\n",
    "# see https://www.wikidata.org/wiki/Property:P21 for values\n",
    "def decodeSexOrGender(code):\n",
    "    code = code.lower()\n",
    "    if code == 'm':\n",
    "        qId = 'Q6581097'\n",
    "    elif code == 'f':\n",
    "        qId = 'Q6581072'\n",
    "    elif code == 'i':\n",
    "        qId = 'Q1097630'\n",
    "    elif code == 'tf':\n",
    "        qId = 'Q1052281'\n",
    "    elif code == 'tm':\n",
    "        qId = 'Q2449503'\n",
    "    else:\n",
    "        qId = ''\n",
    "    return qId\n",
    "\n",
    "# query for a single variable that's an item named 'item'\n",
    "# returns a list of results\n",
    "def searchWikidataForQIdByOrcid(orcid):\n",
    "    query = '''\n",
    "select distinct ?item where {\n",
    "  ?item wdt:P496 \"''' + employees[employeeIndex]['orcid'] + '''\".\n",
    "  }\n",
    "'''\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append(qNumber)\n",
    "    except:\n",
    "        results = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# returns a list of employer strings for the item with Wikidata ID qId; P108 is employer\n",
    "def searchWikidataEmployer(qId):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?employer where {\n",
    "        wd:'''+ qId + ''' wdt:P108 ?employerId.\n",
    "        ?employerId rdfs:label ?employer.\n",
    "        FILTER(lang(?employer) = 'en')\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                resultsList.append(statement['employer']['value'])\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "# returns a list of value Q IDs of the property propertyId for the item with Wikidata ID qId\n",
    "def searchWikidataSingleProperty(qId, propertyId, valueType):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?object where {\n",
    "        wd:'''+ qId + ''' wdt:''' + propertyId + ''' ?object.\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                if valueType == 'item':\n",
    "                    resultValue = extractQNumber(statement['object']['value'])\n",
    "                else:\n",
    "                    resultValue = statement['object']['value']\n",
    "                resultsList.append(resultValue)\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "def searchOrcidAtWikidata(qIds):\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    # NOTE: the MINUS clause removes properties in the http://www.wikidata.org/prop/reference/value/ namespace\n",
    "    # leaving only those in the http://www.wikidata.org/prop/reference/ namespace (i.e. the direct literal values)\n",
    "    query = '''\n",
    "select distinct ?id ?statement ?orcid ?reference ?refProp ?refVal where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id p:P496 ?statement.\n",
    "  ?statement ps:P496 ?orcid.\n",
    "  optional {\n",
    "        ?statement prov:wasDerivedFrom ?reference.\n",
    "        ?reference ?refProp ?refVal.\n",
    "        MINUS {\n",
    "            ?reference ?refProp ?refVal. \n",
    "            FILTER(contains(str(?refProp), \"value/\"))\n",
    "        }\n",
    "    }\n",
    "  }'''\n",
    "    #print(query)\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    statements = data['results']['bindings']\n",
    "    # ********** NOTE: need to deal with case where there are more than one reference per statement\n",
    "    # This will result in several statements with the same qNumeber, orcid, and referenceHash\n",
    "    for statement in statements:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(statement['id']['value'], 4)\n",
    "        # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "        noDomain = extractFromIri(statement['statement']['value'], 5)\n",
    "        # need to remove the qNumber that's appended in front of the UUID\n",
    "        statementUuid = noDomain.partition(qNumber + '-')[2]\n",
    "        orcid = statement['orcid']['value']\n",
    "        if 'reference' in statement:\n",
    "            # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "            referenceHash = extractFromIri(statement['reference']['value'], 4)\n",
    "        else:\n",
    "            referenceHash = ''\n",
    "        if 'refProp' in statement:\n",
    "            # remove pr: 'http://www.wikidata.org/prop/reference/'\n",
    "            referenceProperty = extractFromIri(statement['refProp']['value'], 5)\n",
    "        else:\n",
    "            referenceProperty = ''\n",
    "        if 'refVal' in statement:\n",
    "            referenceValue = statement['refVal']['value']\n",
    "            # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "            if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                referenceValue = referenceValue.split('T')[0]\n",
    "        else:\n",
    "            referenceValue = ''\n",
    "        results.append({'qId': qNumber, 'statementUuid': statementUuid, 'orcid': orcid, 'referenceHash': referenceHash, 'referenceProperty': referenceProperty, 'referenceValue': referenceValue})\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "def searchHumansAtWikidata(qIds):\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    # NOTE: instanceOf human is one of the statement that Wikidata does not care about references for\n",
    "    # So we will ignore them here\n",
    "    query = '''\n",
    "select distinct ?id ?statement where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:Q5.\n",
    "  }'''\n",
    "    #print(query)\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    statements = data['results']['bindings']\n",
    "    for statement in statements:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(statement['id']['value'], 4)\n",
    "        # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "        noDomain = extractFromIri(statement['statement']['value'], 5)\n",
    "        # need to remove the qNumber that's appended in front of the UUID\n",
    "        statementUuid = noDomain.partition(qNumber + '-')[2]\n",
    "        results.append({'qId': qNumber, 'statementUuid': statementUuid})\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "def checkOrcid(orcid):\n",
    "    namespace = 'https://orcid.org/'\n",
    "    endpointUrl = namespace + orcid\n",
    "    acceptMediaType = 'application/ld+json'\n",
    "    r = requests.get(endpointUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    code = r.status_code\n",
    "    #print(r.text)\n",
    "    data = r.json()\n",
    "    response = {'code': code, 'data': data}\n",
    "    if response['code'] != 200:\n",
    "        print('Attempt to dereference ORCID resulted in HTTP response code ', response['code'])\n",
    "        data['orcidReferenceValue'] = ''\n",
    "    else:\n",
    "        print('Successfully retrieved')\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "    # delay a quarter second to avoid hitting the API too rapidly\n",
    "    sleep(0.25)\n",
    "    return(wholeDateZ)\n",
    "\n",
    "# if the value passed is '' then the value will be retrieved.  Otherwise, the value is used to screen.\n",
    "def searchStatementAtWikidata(qIds, prop, value, refPropList):\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?statement '\n",
    "    # if no value was specified, find the value\n",
    "    if value == '':\n",
    "        query += '?statementValue '\n",
    "    if len(refPropList) != 0:\n",
    "        query += '?reference '\n",
    "    for refPropIndex in range(0, len(refPropList)):\n",
    "        query += '?refVal' + str(refPropIndex) + ' '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id p:'''+ prop + ''' ?statement.\n",
    "  ?statement ps:'''+ prop\n",
    "    \n",
    "    if value == '':\n",
    "        query += ' ?statementValue.'\n",
    "    else:\n",
    "        query += ' wd:' + value + '.'\n",
    "\n",
    "    if len(refPropList) != 0:\n",
    "        query += '''\n",
    "  optional {\n",
    "    ?statement prov:wasDerivedFrom ?reference.'''\n",
    "        for refPropIndex in range(0, len(refPropList)):\n",
    "            query +='''\n",
    "    ?reference pr:''' + refPropList[refPropIndex] + ''' ?refVal''' + str(refPropIndex) + '''.'''\n",
    "        query +='''\n",
    "        }'''\n",
    "    query +='''\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    # ********** NOTE: need to deal with case where there are more than one reference per result\n",
    "    # This will result in several results with the same qNumeber, orcid, and referenceHash\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "        noDomain = extractFromIri(result['statement']['value'], 5)\n",
    "        # need to remove the qNumber that's appended in front of the UUID\n",
    "        statementUuid = noDomain.partition(qNumber + '-')[2]\n",
    "        # if no value was specified, get the value that was found in the search\n",
    "        if value == '':\n",
    "            statementValue = result['statementValue']['value']\n",
    "        if len(refPropList) != 0:\n",
    "            if 'reference' in result:\n",
    "                # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "                referenceHash = extractFromIri(result['reference']['value'], 4)\n",
    "            else:\n",
    "                referenceHash = ''\n",
    "            referenceValues = []\n",
    "            for refPropIndex in range(0, len(refPropList)):\n",
    "                if 'refVal' + str(refPropIndex) in result:\n",
    "                    refVal = result['refVal' + str(refPropIndex)]['value']\n",
    "                    # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "                    #if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                    #    referenceValue = referenceValue.split('T')[0]\n",
    "                else:\n",
    "                    refVal = ''\n",
    "                referenceValues.append(refVal)\n",
    "        resultsDict = {'qId': qNumber, 'statementUuid': statementUuid}\n",
    "        # if no value was specified, get the value that was found in the search\n",
    "        if value == '':\n",
    "            resultsDict['statementValue'] = statementValue\n",
    "        if len(refPropList) != 0:\n",
    "            resultsDict['referenceHash'] = referenceHash\n",
    "            resultsDict['referenceValues'] = referenceValues\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    \n",
    "    return returnValue\n",
    "\n",
    "# search for any of the \"label\" types: label, alias, description\n",
    "def searchLabelsDescriptionsAtWikidata(qIds, labelType, language):\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    if labelType == 'label':\n",
    "        predicate = 'rdfs:label'\n",
    "    elif labelType == 'alias':\n",
    "        predicate = 'skos:altLabel'\n",
    "    elif labelType == 'description':\n",
    "        predicate = 'schema:description'\n",
    "    else:\n",
    "        predicate = 'rdfs:label'        \n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?string '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id '''+ predicate + ''' ?string.\n",
    "  filter(lang(?string)=\"''' + language + '''\")\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        string = result['string']['value']\n",
    "        resultsDict = {'qId': qNumber, 'string': string}\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    \n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query ORCID for Vanderbilt University people\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/orcid/orcid-get-json.ipynb\n",
    "\n",
    "Retrieves results 100 at a time, then processes them by extracting desired information.  NOTE: takes hours to run.\n",
    "\n",
    "Saves results in a file and the alternative names in a second file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [['orcid', 'givenNames', 'familyName', 'startDate', 'endDate', 'department', 'organization']]\n",
    "otherNameList = [['orcid', 'altName']]\n",
    "\n",
    "# use the API to search for people associated with Vanderbilt University\n",
    "# First search is for only one record, just to get the number of hits found\n",
    "searchUri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start=1&rows=1'\n",
    "acceptMediaType = 'application/json'\n",
    "response = requests.get(searchUri, headers = generateHeaderDictionary(acceptMediaType))\n",
    "data = response.json()\n",
    "numberResults = data[\"num-found\"]\n",
    "#print(data[\"num-found\"])\n",
    "numberPages = math.floor(numberResults/100)\n",
    "#print(numberPages)\n",
    "remainder = numberResults - 100*numberPages\n",
    "#print(remainder)\n",
    "\n",
    "for pageCount in range(0, numberPages+1):  # the remainder will be caught when pageCount = numberPages\n",
    "    print('page: ', pageCount)\n",
    "    searchUri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start='+str(pageCount*100+1)\n",
    "    response = requests.get(searchUri, headers={'Accept' : 'application/json'})\n",
    "    print(response.url)\n",
    "    data = response.json()\n",
    "    orcidsDictsList = data['result']\n",
    "\n",
    "    # extract the identifier strings from the data structure\n",
    "    orcids = []\n",
    "    for orcidDict in orcidsDictsList:\n",
    "        dictionary = {'id': orcidDict['orcid-identifier']['path'], 'iri': orcidDict['orcid-identifier']['uri']}\n",
    "        orcids.append(dictionary)\n",
    "\n",
    "    for orchidIndex in range(0, len(orcids)):\n",
    "        response = requests.get(orcids[orchidIndex]['iri'], headers={'Accept' : 'application/json'})\n",
    "        data = response.json()\n",
    "\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        orcidId = data['orcid-identifier']['path']\n",
    "        #print(orcidId)\n",
    "        # if there isn't a name, then go on to the next ORCID\n",
    "        if not data['person']['name']:\n",
    "            continue\n",
    "        if data['person']['name']['given-names']:  \n",
    "            givenNames = data['person']['name']['given-names']['value']\n",
    "        else:\n",
    "            givenNames = ''\n",
    "        if data['person']['name']['family-name']:\n",
    "            familyName = data['person']['name']['family-name']['value']\n",
    "        else:\n",
    "            familyName = ''\n",
    "        #print(givenNames, ' ', familyName)\n",
    "        otherNames = data['person']['other-names']['other-name']\n",
    "        for otherName in otherNames:\n",
    "            #print(otherName['content'])\n",
    "            otherNameList.append([orcidId, otherName['content']])\n",
    "\n",
    "        affiliations = data['activities-summary']['employments']['affiliation-group']\n",
    "        #print(json.dumps(affiliations, indent = 2))\n",
    "        for affiliation in affiliations:\n",
    "            summaries = affiliation['summaries']\n",
    "            #print(summaries)\n",
    "            #print()\n",
    "            for summary in summaries:\n",
    "                employment = summary['employment-summary']\n",
    "                #print(json.dumps(employment, indent = 2))\n",
    "                startDate = ''\n",
    "                if employment['start-date']:\n",
    "                    if employment['start-date']['year']:\n",
    "                        startDate += employment['start-date']['year']['value']\n",
    "                        startMonth = employment['start-date']['month']\n",
    "                        if startMonth:\n",
    "                            startDate += '-' + startMonth['value']\n",
    "                            startDay = employment['start-date']['day']\n",
    "                            if startDay:\n",
    "                                startDate += '-' + startDay['value']\n",
    "                #print('start date: ', startDate)\n",
    "                endDate = ''\n",
    "                if employment['end-date']:\n",
    "                    if employment['end-date']['year']:\n",
    "                        endDate += employment['end-date']['year']['value']\n",
    "                        endMonth = employment['end-date']['month']\n",
    "                        if endMonth:\n",
    "                            endDate += '-' + endMonth['value']\n",
    "                            endDay = employment['end-date']['day']\n",
    "                            if endDay:\n",
    "                                endDate += '-' + endDay['value']\n",
    "                #print('end date: ', endDate)\n",
    "                department = employment['department-name']\n",
    "                # if there is no value for department, set it to empty string\n",
    "                if not department:\n",
    "                    department = ''\n",
    "                #print(department)\n",
    "                if employment['organization']:\n",
    "                    organization = employment['organization']['name']\n",
    "                #print(organization)\n",
    "                if 'Vanderbilt University' in organization:\n",
    "                    print(orcidId, givenNames, familyName, startDate, endDate, department, organization)\n",
    "                    table.append([orcidId, givenNames, familyName, startDate, endDate, department, organization])\n",
    "                #print(table)\n",
    "        sleep(.25)\n",
    "\n",
    "print()\n",
    "print('Done')\n",
    "fileName = 'orcid_data.csv'\n",
    "writeCsv(fileName, table)\n",
    "fileName = 'orcid_other_names.csv'\n",
    "writeCsv(fileName, otherNameList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape departmental website\n",
    "\n",
    "script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/scrape-bsci.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputTable = [['name', 'degree', 'role', 'category']]\n",
    "categories = deptSettings[deptShortName]['categories']\n",
    "\n",
    "acceptMediaType = 'text/html'\n",
    "for category in categories:\n",
    "    url = deptSettings[deptShortName]['baseUrl'] + category\n",
    "    response = requests.get(url, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    soupObject = BeautifulSoup(response.text,features=\"html5lib\")\n",
    "\n",
    "    # get the first table from the page\n",
    "    tableObject = soupObject.find_all('table')[0]\n",
    "    \n",
    "    # get the rows from the table\n",
    "    rowObjectsList = tableObject.find_all('tr')\n",
    "    for rowObject in rowObjectsList:\n",
    "        # get the cells from each row\n",
    "        cellObjectsList = rowObject.find_all('td')\n",
    "        # picture is in cell 0, name and title is in cell 1\n",
    "        nameCell = cellObjectsList[1]\n",
    "        # the name part is bolded\n",
    "        name = nameCell('strong')[0].text\n",
    "\n",
    "        # check to see if the name has already been added to the list (some depts put people on two category lists)\n",
    "        found = False\n",
    "        for person in outputTable:  # not worrying about the header row, which shouldn't match any name\n",
    "            if person[0] == name:\n",
    "                found = True\n",
    "                break  # quit looking for the person\n",
    "        if not found:  # only finish extracting and saving data if there isn't a match\n",
    "            # separate degrees from names\n",
    "            degree = ''\n",
    "            for testDegree in degreeList:\n",
    "                if testDegree['string'] in name:\n",
    "                    name = name.partition(', ' + testDegree['string'])[0]\n",
    "                    # correct any malformed strings\n",
    "                    degree = testDegree['value']\n",
    "\n",
    "            try:\n",
    "                # process the roles text\n",
    "                dirtyText  = str(nameCell)\n",
    "                # get rid of trailing td tag\n",
    "                nameCellText = dirtyText.split('</td>')[0]\n",
    "                cellLines = nameCellText.split('<br/>')\n",
    "                roles = []\n",
    "                for lineIndex in range(1, len(cellLines)):\n",
    "                    roleDict = {}\n",
    "                    if ' of ' in cellLines[lineIndex]:\n",
    "                        pieces = cellLines[lineIndex].split(' of ')\n",
    "                        roleDict['title'] = pieces[0]\n",
    "                        roleDict['department'] = pieces[1]\n",
    "                        roles.append(roleDict)\n",
    "                    elif ' in ' in cellLines[lineIndex]:\n",
    "                        pieces = cellLines[lineIndex].split(' in ')\n",
    "                        roleDict['title'] = pieces[0]\n",
    "                        roleDict['department'] = pieces[1]\n",
    "                        roles.append(roleDict)\n",
    "                    else:\n",
    "                        roleDict['title'] = cellLines[lineIndex]\n",
    "                        roleDict['department'] = ''\n",
    "                        roles.append(roleDict)\n",
    "                    if ', Emeritus' in roleDict['department']:\n",
    "                        roleDict['department'] = roleDict['department'].split(', Emeritus')[0]\n",
    "                        roleDict['title'] = 'Emeritus ' + roleDict['title']\n",
    "                rolesJson = json.dumps(roles)\n",
    "\n",
    "            except:\n",
    "                rolesJson = ''\n",
    "            outputTable.append([name, degree, rolesJson, category])            \n",
    "\n",
    "fileName = deptShortName + '-employees.csv'\n",
    "writeCsv(fileName, outputTable)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match departmental people with ORCID results\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/orcid/match_bsci_orcid.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = deptShortName + '-employees.csv'\n",
    "employees = readDict(filename)\n",
    "\n",
    "filename = 'orcid_data.csv'\n",
    "orcidData = readDict(filename)\n",
    "\n",
    "testRatio = 90\n",
    "departmentTestRatio = 90\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for row in orcidData:\n",
    "        name = row['givenNames'] + ' ' + row['familyName']\n",
    "        #ratio = fuzz.ratio(name, employees[employeeIndex][0])\n",
    "        #partialRatio = fuzz.partial_ratio(name, employees[employeeIndex][0])\n",
    "        #sortRatio = fuzz.token_sort_ratio(name, employees[employeeIndex][0])\n",
    "        \n",
    "        output = ''\n",
    "        # the set ratio seems to do the best job of matching\n",
    "        setRatio = fuzz.token_set_ratio(name, employees[employeeIndex]['name'])\n",
    "        if setRatio >= testRatio:\n",
    "            output = str(setRatio) + ' ' + name + ' / ' + employees[employeeIndex]['name']\n",
    "            \n",
    "            if row['department'] == '':\n",
    "                output += \" WARNING: no department given in ORCID\"\n",
    "            else:\n",
    "                # carry out a secondary test to see if any of the departments listed in the BSCI page\n",
    "                # are a good match to the department given in the ORCID record\n",
    "                \n",
    "                # expand the role JSON into a list of dictionaries\n",
    "                roleDict = json.loads(employees[employeeIndex]['role'])\n",
    "                departmentMatch = False\n",
    "                for department in roleDict:\n",
    "                    setRatio = fuzz.token_set_ratio(deptSettings[deptShortName]['departmentSearchString'], row['department'])\n",
    "                    if setRatio > departmentTestRatio:\n",
    "                        departmentMatch = True\n",
    "                        output += ' ' + str(setRatio) + ' ' + row['department']\n",
    "                if not departmentMatch:\n",
    "                    output += ' WARNING: ' + row['department'] + ' less than ' + str(departmentTestRatio) + '% match to any dept.'\n",
    "            print(output)\n",
    "            matched = True\n",
    "            foundOrcid = row['orcid']\n",
    "            # We only care about the first good match to an ORCID record, kill the loop after that\n",
    "            break\n",
    "    if matched:\n",
    "        employees[employeeIndex]['orcid'] = foundOrcid\n",
    "    else:\n",
    "        employees[employeeIndex]['orcid'] = ''\n",
    "        \n",
    "filename = deptShortName + '-employees-with-orcid.csv'\n",
    "fieldnames = ['name', 'degree', 'category', 'orcid', 'role']\n",
    "writeDictsToCsv(employees, filename, fieldnames)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Vanderbilt people data from Wikidata\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/download-vanderbilt-people.py\n",
    "\n",
    "Performs a SPARQL query to find people in Wikidata that are employed at Vanderbilt.  Returns name, description, start date, end date, and ORCID if it has them.  Output to vanderbilt_wikidata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''select distinct  ?person ?name ?orcid ?startDate ?endDate ?description where {\n",
    "  ?person p:P108 ?statement.\n",
    "  ?statement ps:P108  wd:Q29052.\n",
    "  optional{\n",
    "    ?person rdfs:label ?name.\n",
    "    FILTER(lang(?name)=\"en\")\n",
    "    }\n",
    "  optional{?statement pq:P580 ?startDate.}\n",
    "  optional{?statement pq:P582 ?endDate.}\n",
    "  optional{?person wdt:P496 ?orcid.}\n",
    "  optional{\n",
    "    ?person schema:description ?description.\n",
    "    FILTER(lang(?description)=\"en\")\n",
    "          }\n",
    "  }'''\n",
    "\n",
    "# The endpoint defaults to returning XML, so the Accept: header is required\n",
    "r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers={'Accept' : 'application/json'})\n",
    "\n",
    "data = r.json()\n",
    "print(json.dumps(data,indent = 2))\n",
    "\n",
    "table = [['wikidataIri', 'name', 'description', 'startDate', 'endDate', 'orcid']]\n",
    "items = data['results']['bindings']\n",
    "for item in items:\n",
    "    wikidataIri = item['person']['value']\n",
    "    name = ''\n",
    "    if 'name' in item:\n",
    "        name = item['name']['value']\n",
    "    description = ''\n",
    "    if 'description' in item:\n",
    "        description = item['description']['value']\n",
    "    startDate = ''\n",
    "    if 'startDate' in item:\n",
    "        startDate = item['startDate']['value']\n",
    "    endDate = ''\n",
    "    if 'endDate' in item:\n",
    "        endDate = item['endDate']['value']\n",
    "    orcid = ''\n",
    "    if 'orcid' in item:\n",
    "        orcid = item['orcid']['value']\n",
    "    table.append([wikidataIri, name, description, startDate, endDate, orcid])\n",
    "    \n",
    "fileName = 'vanderbilt_wikidata.csv'\n",
    "writeCsv(fileName, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Vanderbilt people's altLabels from Wikidata\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/download-vanderbilt-people-altlabels.py\n",
    "\n",
    "Similar to previous query, except when there is a match, it downloads the altLabels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''select distinct  ?person ?altLabel where {\n",
    "  ?person p:P108 ?statement.\n",
    "  ?statement ps:P108  wd:Q29052.\n",
    "  ?person skos:altLabel ?altLabel.\n",
    "  FILTER(lang(?altLabel)=\"en\")\n",
    "}'''\n",
    "\n",
    "# The endpoint defaults to returning XML, so the Accept: header is required\n",
    "r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers={'Accept' : 'application/json'})\n",
    "\n",
    "data = r.json()\n",
    "print(json.dumps(data,indent = 2))\n",
    "\n",
    "table = [['wikidataIri', 'altLabel']]\n",
    "items = data['results']['bindings']\n",
    "for item in items:\n",
    "    wikidataIri = item['person']['value']\n",
    "    altLabel = ''\n",
    "    if 'altLabel' in item:\n",
    "        altLabel = item['altLabel']['value']\n",
    "    table.append([wikidataIri, altLabel])\n",
    "    \n",
    "fileName = 'vanderbilt_wikidata_altlabels.csv'\n",
    "writeCsv(fileName, table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match employees to Wikidata\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/match_bsci_wikidata.ipynb\n",
    "\n",
    "Attempts to match records of people Wikidata knows to work at Vanderbilt with departmental people by matching their ORCIDs, then name strings. If there isn't a match with the downloaded Wikidata records, for employees with ORCIDs, the script attempts to find them in Wikidata by directly doing a SPARQL search for their ORCID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = deptShortName + '-employees-with-orcid.csv'\n",
    "employees = readDict(filename)\n",
    "\n",
    "filename = 'vanderbilt_wikidata.csv'\n",
    "wikidataData = readDict(filename)\n",
    "\n",
    "testRatio = 90\n",
    "departmentTestRatio = 90\n",
    "\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    # 0=unmatched\n",
    "    # 1=matched with ORCID in both sources\n",
    "    # 2=ORCID in BSCI but name match to Wikidata (no ORCID)\n",
    "    # 3=no ORCID in BSCI but name match to Wikidata (with ORCID); could happen if affiliation isn't matched in ORCID\n",
    "    # 4=no ORCID in BSCI but name match to Wikidata (no ORCID)\n",
    "    # 5=ORCID in BSCI and found via SPARQL ORCID search (likely non-VU affiliated in Wikidata)\n",
    "    # 6=ORCID in BSCI and found via SPARQL name search (non-VU affiliated without ORCID)\n",
    "    # 7=no name match\n",
    "    # 8=ORCID in BSCI, error in SPARQL ORCID search\n",
    "    # 9=no ORCID in BSCI, error in SPARQL name search\n",
    "    # 10=affiliation match in article\n",
    "    # 11=match by human choice after looking at entity data\n",
    "    \n",
    "    matchStatus = 0\n",
    "    for row in wikidataData:\n",
    "        # We know the employee has an ORCID, so try to match it\n",
    "        if employees[employeeIndex]['orcid'] != '':\n",
    "            # There's a match, hooray!\n",
    "            if employees[employeeIndex]['orcid'] == row['orcid']:\n",
    "                print('orcid match: ', row['name'] + ' ' + row['orcid'])\n",
    "                matchStatus = 1\n",
    "                employees[employeeIndex]['wikidataId'] = extractQNumber(row['wikidataIri'])\n",
    "            # No ORCID match - see if the name matches\n",
    "            else:\n",
    "                setRatio = fuzz.token_set_ratio(row['name'], employees[employeeIndex]['name'])\n",
    "                if setRatio >= testRatio:\n",
    "                    print('name match: ', str(setRatio) + ' ' + row['name'] + ' / ' + employees[employeeIndex]['name'] + ' department:' + employees[employeeIndex]['orcid'])\n",
    "                    matchStatus = 2\n",
    "                    employees[employeeIndex]['wikidataId'] = extractQNumber(row['wikidataIri'])\n",
    "        # As far as we know, the employee doesn't have an ORCID, so try to match the name\n",
    "        else:\n",
    "            setRatio = fuzz.token_set_ratio(row['name'], employees[employeeIndex]['name'])\n",
    "            # We get a name match \n",
    "            if setRatio >= testRatio:\n",
    "                # For some reason, Wikidata has the ORCID, so grab it\n",
    "                if row['orcid'] != '':\n",
    "                    print('name match: ', str(setRatio) + ' ' + row['name'] + ' / ' + employees[employeeIndex]['name'] + ' ORCID:' + row['orcid'])\n",
    "                    employees[employeeIndex]['orcid'] = row['orcid']\n",
    "                    matchStatus = 3\n",
    "                # Wikidata doesn't have an ORCID\n",
    "                else:\n",
    "                    print('name match: ', str(setRatio) + ' ' + row['name'] + ' / ' + employees[employeeIndex]['name'] + ' WD description: ' + row['description'])\n",
    "                    matchStatus = 4\n",
    "                employees[employeeIndex]['wikidataId'] = extractQNumber(row['wikidataIri'])\n",
    "        # We've gone all the way through the without finding a match\n",
    "\n",
    "    # Do a last ditch attempt to try to find the person in Wikidata by doing a SPARQL search for their ORCID\n",
    "    if matchStatus == 0:\n",
    "        if employees[employeeIndex]['orcid'] != '':\n",
    "            results = searchWikidataForQIdByOrcid(employees[employeeIndex]['orcid'])\n",
    "            if len(results) > 0:\n",
    "                print('SPARQL ORCID search: ', employees[employeeIndex]['name'], results)\n",
    "                if len(results) == 1:\n",
    "                    # if search fails and return an error message\n",
    "                    if len(results[0]) > 15:\n",
    "                        matchStatus = 8\n",
    "                        print('Error message in ORCID search')\n",
    "                    else:\n",
    "                        matchStatus = 5\n",
    "                        employees[employeeIndex]['wikidataId'] = results[0]\n",
    "                else:\n",
    "                    print('ERROR: multiple results for same ORCID')\n",
    "    # after every possible matching method, record the matchStatus\n",
    "    employees[employeeIndex]['wikidataStatus'] = str(matchStatus)\n",
    "#print(employees)\n",
    "\n",
    "filename = deptShortName + '-employees-with-wikidata.csv'\n",
    "fieldnames = ['wikidataId', 'name', 'degree', 'category', 'orcid', 'wikidataStatus', 'role']\n",
    "writeDictsToCsv(employees, filename, fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosscheck people against publications\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/crosscheck-publications.ipynb\n",
    "\n",
    "Checks possible Wikidata records against publications in CrossRef and PubMed to see if the author metadata will disambiguate the Wikidata record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptMediaType = 'application/json'\n",
    "requestHeaderDictionary = generateHeaderDictionary(acceptMediaType)\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        initials.append(piece[0:1])\n",
    "    \n",
    "    # NOTE: currently doesn't handle \", Jr.\", \"III\", etc.\n",
    "    \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsDict\n",
    "\n",
    "# returns a list of results of articles by person with Wikidata ID qId\n",
    "def searchWikidataArticle(qId):\n",
    "    resultsList = []\n",
    "    # P50 is \"author\"; P698 is the PubMed ID of the article; P356 is the DOI of the article\n",
    "    query = '''select distinct ?title ?doi ?pmid where {\n",
    "      ?article wdt:P50 wd:''' + qId + '''.\n",
    "      optional {\n",
    "          ?article rdfs:label ?title.\n",
    "          FILTER(lang(?title) = 'en')\n",
    "          }\n",
    "      optional {?article wdt:P698 ?pmid.}\n",
    "      optional {?article wdt:P356 ?doi.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            if 'title' in statement:\n",
    "                title = statement['title']['value']\n",
    "                #print('title=',title)\n",
    "            else:\n",
    "                title = ''\n",
    "            if 'pmid' in statement:\n",
    "                pmid = statement['pmid']['value']\n",
    "            else:\n",
    "                pmid = ''\n",
    "            if 'doi' in statement:\n",
    "                doi = statement['doi']['value']\n",
    "            else:\n",
    "                doi = ''\n",
    "            resultsList.append({'title': title, 'pmid': pmid, 'doi': doi})\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "def retrievePubMedData(pmid):\n",
    "    fetchUrl = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    paramDict = {\n",
    "        'tool': toolName, \n",
    "        'email': emailAddress,\n",
    "        'db': 'pubmed', \n",
    "         #'retmode': 'xml', \n",
    "        'rettype': 'abstract', \n",
    "        'id': pmid\n",
    "    }\n",
    "    response = requests.get(fetchUrl, params=paramDict)\n",
    "    #print(response.url)\n",
    "    pubData = response.text  # the response text is XML\n",
    "    #print(pubData)  # uncomment this line to see the XML\n",
    "\n",
    "    # process the returned XML, see https://docs.python.org/2/library/xml.etree.elementtree.html\n",
    "    root = et.fromstring(pubData)\n",
    "    try:\n",
    "        title = root.findall('.//ArticleTitle')[0].text\n",
    "    except:\n",
    "        title = ''\n",
    "    names = root.findall('.//Author')\n",
    "    affiliations = []\n",
    "    for name in names:\n",
    "        try:\n",
    "            affiliation = name.find('./AffiliationInfo/Affiliation').text\n",
    "        except:\n",
    "            affiliation = ''\n",
    "        try:\n",
    "            lastName = name.find('./LastName').text\n",
    "        except:\n",
    "            lastName = ''\n",
    "        try:\n",
    "            foreName = name.find('./ForeName').text\n",
    "        except:\n",
    "            foreName = ''\n",
    "        try:\n",
    "            idField = name.find('./Identifier')\n",
    "            if idField.get('Source') == 'ORCID':\n",
    "                orcid = idField.text\n",
    "            else:\n",
    "                orcid = ''\n",
    "        except:\n",
    "            orcid = ''\n",
    "              \n",
    "        #print(lastName)\n",
    "        #print(affiliation)\n",
    "        affiliations.append({'affiliation': affiliation, 'surname': lastName, 'forename': foreName, 'orcid': orcid})\n",
    "    #print()\n",
    "\n",
    "    # See https://www.ncbi.nlm.nih.gov/books/NBK25497/ for usage guidelines. \n",
    "    # An API key is required for more than 3 requests per second.\n",
    "    sleep(0.5) # wait half a second before hitting the API again to avoid getting blocked\n",
    "    return affiliations\n",
    "\n",
    "def retrieveCrossRefDoi(doi):\n",
    "    authorList = []\n",
    "    crossRefEndpointUrl = 'https://api.crossref.org/works/'\n",
    "    encodedDoi = urllib.parse.quote(doi)\n",
    "    searchUrl = crossRefEndpointUrl + encodedDoi\n",
    "    acceptMediaType = 'application/json'\n",
    "    response = requests.get(searchUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    try:\n",
    "        data = response.json()\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        if 'author' in data['message']:\n",
    "            authors = data['message']['author']\n",
    "            for author in authors:\n",
    "                authorDict = {}\n",
    "                if 'ORCID' in author:\n",
    "                    authorDict['orcid'] = author['ORCID']\n",
    "                else:\n",
    "                    authorDict['orcid'] = ''\n",
    "                if 'given' in author:\n",
    "                    authorDict['givenName'] = author['given']\n",
    "                else:\n",
    "                    authorDict['givenName'] = ''\n",
    "                if 'family' in author:\n",
    "                    authorDict['familyName'] = author['family']\n",
    "                else:\n",
    "                    authorDict['familyName'] = ''\n",
    "                affiliationList = []\n",
    "                if 'affiliation' in author:\n",
    "                    for affiliation in author['affiliation']:\n",
    "                        affiliationList.append(affiliation['name'])\n",
    "                # if there aren't any affiliations, the list will remain empty\n",
    "                authorDict['affiliation'] = affiliationList\n",
    "                authorList.append(authorDict)\n",
    "    except:\n",
    "        authorList = [data]\n",
    "    return authorList\n",
    "\n",
    "# ***** BODY OF SEARCH\n",
    "filename = deptShortName + '-employees-with-wikidata.csv'\n",
    "bsciEmployees = readDict(filename)\n",
    "\n",
    "for employeeIndex in range(0, len(bsciEmployees)):\n",
    "#for employeeIndex in range(0, 1): # just do one person for testing\n",
    "    # perform search only for people who weren't already matched\n",
    "    if bsciEmployees[employeeIndex]['wikidataStatus'] == '0':\n",
    "        matchStatus = 0\n",
    "        print('--------------------------')\n",
    "        results = searchNameAtWikidata(bsciEmployees[employeeIndex]['name'])\n",
    "        if len(results) == 0:\n",
    "            print('No Wikidata name match: ', bsciEmployees[employeeIndex]['name'])\n",
    "            matchStatus = 7\n",
    "            print()\n",
    "        else:\n",
    "            print('SPARQL name search: ', bsciEmployees[employeeIndex]['name'])\n",
    "            if len(results) == 1:\n",
    "                if 'error' in results[0]:\n",
    "                    matchStatus = 9\n",
    "                    print('Error message in name search:', results[0]['error'])\n",
    "                    break # discontinue processing this person\n",
    "\n",
    "            qIds = []\n",
    "            nameVariants = []\n",
    "            potentialOrcid = []\n",
    "            for result in results:\n",
    "                qIds.append(result['qId'])\n",
    "                nameVariants.append(result['name'])\n",
    "            \n",
    "            testAuthor = bsciEmployees[employeeIndex]['name']\n",
    "            testOrcid = bsciEmployees[employeeIndex]['orcid']\n",
    "\n",
    "            if testOrcid == '':\n",
    "                print('(no ORCID)')\n",
    "            else:\n",
    "                print('ORCID: ', testOrcid)\n",
    "            print()\n",
    "            \n",
    "            foundMatch = False # start the flag with the person not being matched\n",
    "            for qIdIndex in range(0, len(qIds)):\n",
    "                potentialOrcid.append('') # default to no ORCID found for that person\n",
    "                print()\n",
    "                print(qIdIndex, 'Wikidata ID: ', qIds[qIdIndex], ' Name variant: ', nameVariants[qIdIndex], ' ', 'https://www.wikidata.org/wiki/' + qIds[qIdIndex])\n",
    "                wdClassList = searchWikidataSingleProperty(qIds[qIdIndex], 'P31', 'item')\n",
    "                # if there is a class property, check if it's a human\n",
    "                if len(wdClassList) != 0:\n",
    "                    # if it's not a human\n",
    "                    if wdClassList[0] != 'Q5':\n",
    "                        print('This item is not a human!')\n",
    "                        break\n",
    "                        \n",
    "                # check for a death date\n",
    "                deathDateList = searchWikidataSingleProperty(qIds[qIdIndex], 'P570', 'string')\n",
    "                if len(deathDateList) == 0:\n",
    "                    print('No death date given.')\n",
    "                else:\n",
    "                    deathDate = deathDateList[0][0:10] # all dates are converted to xsd:dateTime and will have a y-m-d date\n",
    "                    if deathDate < deathDateLimit:\n",
    "                        # if the person died a long time ago, don't retrieve other stuff\n",
    "                        print('This person died in ', deathDate)\n",
    "                        break\n",
    "                    else:\n",
    "                        # if the person died recently, we still might be interested in them so keep going\n",
    "                        print('This person died in ', deathDate)\n",
    "\n",
    "                descriptors = searchWikidataDescription(qIds[qIdIndex])\n",
    "                employers = searchWikidataEmployer(qIds[qIdIndex])\n",
    "                #print(descriptors)\n",
    "                if descriptors != {}:\n",
    "                    if descriptors['description'] != '':\n",
    "                        print('description: ', descriptors['description'])\n",
    "                    for occupation in descriptors['occupation']:\n",
    "                        print('occupation: ', occupation)\n",
    "                    for employer in employers:\n",
    "                        print('employer: ', employer)\n",
    "                    if descriptors['orcid'] != '':\n",
    "                        if testOrcid == '':\n",
    "                            # **** NOTE: if the person has an ORCID, it may be possible to find articles via ORCID\n",
    "                            # that aren't linked in Wikidata. Not sure if this happens often enough to handle it\n",
    "                            print('ORCID: ', descriptors['orcid'])\n",
    "                            potentialOrcid[qIdIndex] = descriptors['orcid']\n",
    "                        else:\n",
    "                            # This should always be true if the SPARQL query for ORCID was already done\n",
    "                            if testOrcid != descriptors['orcid']:\n",
    "                                print('*** NOT the same person; ORCID ' + descriptors['orcid'] + ' does not match.')\n",
    "                                break # don't continue the loop (look up references) since it's definitely not a match\n",
    "                            else:\n",
    "                                print('*** An ORCID match! How did it get missed in the earlier SPARQL query?')\n",
    "                                break\n",
    "                else:\n",
    "                    print('No description or occupation given.')\n",
    "\n",
    "                result = searchWikidataArticle(qIds[qIdIndex])\n",
    "                if len(result) == 0:\n",
    "                    print('No articles authored by that person')\n",
    "                else:\n",
    "                    articleCount = 0\n",
    "                    for article in result:\n",
    "                        print()\n",
    "                        print('Checking article: ', article['title'])\n",
    "                        if article['pmid'] == '':\n",
    "                            print('No PubMed ID')\n",
    "                        else:\n",
    "                            print('Checking authors in PubMed article: ', article['pmid'])\n",
    "                            pubMedAuthors = retrievePubMedData(article['pmid'])\n",
    "                            #print(pubMedAuthors)\n",
    "                            for author in pubMedAuthors:\n",
    "                                nameTestRatio = fuzz.token_set_ratio(author['surname'], testAuthor)\n",
    "                                #print(nameTestRatio, author['surname'])\n",
    "                                if nameTestRatio >= 90:\n",
    "                                    # if the PubMed metadata gives an ORCID for the matched person, record it unless \n",
    "                                    # the ORCID has already been gotten from the Wikidata record\n",
    "                                    if author['orcid'] != '':\n",
    "                                        if testOrcid == '':\n",
    "                                            print('ORCID from article: ', author['orcid'])\n",
    "                                            if potentialOrcid[qIdIndex] == '':\n",
    "                                                potentialOrcid[qIdIndex] = author['orcid']\n",
    "                                        else:\n",
    "                                            if testOrcid != author['orcid']:\n",
    "                                                print('*** NOT the same person; ORCID ' + author['orcid'] + ' does not match.')\n",
    "                                                break # don't continue the loop (look up authors) since it's definitely not a match\n",
    "                                            else:\n",
    "                                                print('*** An ORCID match!')\n",
    "                                                foundMatch = True\n",
    "                                                matchStatus = 6\n",
    "                                                break # don't continue the loop (look up authors) since it's an ORCID match\n",
    "\n",
    "                                    if author['affiliation'] != '': \n",
    "                                        setRatio = fuzz.token_set_ratio(deptSettings[deptShortName]['testAuthorAffiliation'], author['affiliation'])\n",
    "                                        print('Affiliation test: ', setRatio, author['affiliation'])\n",
    "                                        if setRatio >= 90:\n",
    "                                            foundMatch = True\n",
    "                                            matchStatus = 10\n",
    "                                            break # don't continue the loop (look up authors) since it's an affiliation match\n",
    "                                    else:\n",
    "                                        break # give up on this article because no affiliation string\n",
    "                        # Don't look up the DOI if it's already found a match with PubMed\n",
    "                        if foundMatch:\n",
    "                            break # stop checking articles after a PubMed one has matched\n",
    "                        else:\n",
    "                            if article['doi'] == '':\n",
    "                                print('No DOI')\n",
    "                            else:\n",
    "                                print('Checking authors in DOI article: ', article['doi'])\n",
    "                                doiAuthors = retrieveCrossRefDoi(article['doi'])\n",
    "                                for author in doiAuthors:\n",
    "                                    nameTestRatio = fuzz.token_set_ratio(author['familyName'], testAuthor)\n",
    "                                    #print(nameTestRatio, author['familyName'])\n",
    "                                    if nameTestRatio >= 90:\n",
    "                                        if author['orcid'] != '':\n",
    "                                            if testOrcid == '':\n",
    "                                                # DOI records the entire ORCID URI, not just the ID number\n",
    "                                                # so pull the last 19 characters from the string\n",
    "                                                print('ORCID from article: ', author['orcid'][-19:])\n",
    "                                                # only add the ORCID from article if there isn't already one,\n",
    "                                                # for example, one gotten from the Wikidata record itself\n",
    "                                                if potentialOrcid[qIdIndex] == '':\n",
    "                                                    potentialOrcid[qIdIndex] = author['orcid'][-19:]\n",
    "                                            else:\n",
    "                                                if testOrcid != author['orcid']:\n",
    "                                                    print('*** NOT the same person; ORCID ' + author['orcid'] + ' does not match.')\n",
    "                                                    break # don't continue the loop (look up authors) since it's definitely not a match\n",
    "                                                else:\n",
    "                                                    print('*** An ORCID match!')\n",
    "                                                    foundMatch = True\n",
    "                                                    matchStatus = 6\n",
    "                                                    break # don't continue the loop (look up authors) since it's an ORCID match\n",
    "\n",
    "\n",
    "                                        if len(author['affiliation']) > 0:\n",
    "                                            for affiliation in author['affiliation']:\n",
    "                                                setRatio = fuzz.token_set_ratio(deptSettings[deptShortName]['testAuthorAffiliation'], affiliation)\n",
    "                                                print('Affiliation test: ', setRatio, affiliation)\n",
    "                                                if setRatio >= 90:\n",
    "                                                    foundMatch = True\n",
    "                                                    matchStatus = 10\n",
    "                                                    break # don't continue the loop (look up authors) since it's an affiliation match\n",
    "                                        else:\n",
    "                                            break # give up on this article because no affiliation string\n",
    "                            if foundMatch:\n",
    "                                break # stop checking articles after a DOI one has matched\n",
    "                        articleCount += 1\n",
    "                        if articleCount > 10:\n",
    "                            checkMore = input('There are more than 10 articles. Press Enter to skip the rest or enter anything to get the rest.')\n",
    "                            if checkMore == '':\n",
    "                                break\n",
    "                    if foundMatch:\n",
    "                        print('***', qIds[qIdIndex], ' is a match.')\n",
    "                        print()\n",
    "                        bsciEmployees[employeeIndex]['wikidataId'] = qIds[qIdIndex]\n",
    "                        bsciEmployees[employeeIndex]['orcid'] = potentialOrcid[qIdIndex]\n",
    "                        break # quit checking Q IDs since the person was matched\n",
    "                    else:\n",
    "                        print('No match found.')\n",
    "                print('Employee: ', bsciEmployees[employeeIndex]['name'], ' vs. name variant: ', nameVariants[qIdIndex])\n",
    "                print()\n",
    "            if not foundMatch:\n",
    "                choiceString = input('Enter the number of the matched entity, or press Enter/return if none match: ')\n",
    "                if choiceString == '':\n",
    "                    matchStatus = 7\n",
    "                else:\n",
    "                    # NOTE: there is no error trapping here for mis-entry !!!\n",
    "                    choice = int(choiceString)\n",
    "                    matchStatus = 11\n",
    "                    bsciEmployees[employeeIndex]['wikidataId'] = qIds[choice]\n",
    "                    # write a discovered ORCID only if the person didn't already have one\n",
    "                    if (potentialOrcid[choice] != '') and (bsciEmployees[employeeIndex]['orcid'] == ''):\n",
    "                        bsciEmployees[employeeIndex]['orcid'] = potentialOrcid[choice]\n",
    "                print()\n",
    "                \n",
    "        # record the final match status\n",
    "        bsciEmployees[employeeIndex]['wikidataStatus'] = str(matchStatus)\n",
    "    \n",
    "    # write the file after each person is checked in case the user crashes the script\n",
    "    filename = deptShortName + '-employees-curated.csv'\n",
    "    fieldnames = ['wikidataId', 'name', 'degree', 'category', 'orcid', 'wikidataStatus', 'role']\n",
    "    writeDictsToCsv(bsciEmployees, filename, fieldnames)\n",
    "\n",
    "print()\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download various statements and references, then generate write file\n",
    "\n",
    "NOTE: between the previous step and this one, one can add a gender/sex column to the table that will be processed if it exists.  Column header: 'gender'.  Allowed values (from Wikidata): m=male, f=female, i=intersex, tf=transgender female, tm=transgender male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = deptShortName + '-employees-curated.csv'\n",
    "employees = readDict(filename)\n",
    "\n",
    "# create a list of the employees who have Wikidata qIDs\n",
    "qIds = []\n",
    "for employee in employees:\n",
    "    if employee['wikidataId'] != '':\n",
    "        qIds.append(employee['wikidataId'])\n",
    "\n",
    "# get all of the ORCID data that is already in Wikidata\n",
    "prop = 'P496' # ORCID iD\n",
    "value = '' # since no value is passed, the search will retrieve the value\n",
    "refProps = ['P813'] # retrieved\n",
    "wikidataOrcidData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "#print(json.dumps(wikidataOrcidData, indent=2))\n",
    "\n",
    "# match people who have ORCIDs with ORCID data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataOrcidDataIndex in range(0, len(wikidataOrcidData)):\n",
    "        if wikidataOrcidData[wikidataOrcidDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            if employees[employeeIndex]['orcid'] != wikidataOrcidData[wikidataOrcidDataIndex]['statementValue']:\n",
    "                print('Non-matching ORCID for ', employees[employeeIndex]['name'])\n",
    "            # if there is a match, record whatever data was retrieved\n",
    "            else:\n",
    "                employees[employeeIndex]['orcidStatementUuid'] = wikidataOrcidData[wikidataOrcidDataIndex]['statementUuid']\n",
    "                employees[employeeIndex]['orcidReferenceHash'] = wikidataOrcidData[wikidataOrcidDataIndex]['referenceHash']\n",
    "                # if there is no referenceHash then try to dereference the ORCID\n",
    "                if employees[employeeIndex]['orcidReferenceHash']== '':\n",
    "                    # if there is a match, check whether the ORCID record can be retrieved\n",
    "                    print('Checking ORCID for Wikidata matched: ', employees[employeeIndex]['name'])\n",
    "                    # returned value is the current date if successful; empty string if not\n",
    "                    employees[employeeIndex]['orcidReferenceValue'] = checkOrcid(employees[employeeIndex]['orcid'])\n",
    "                # if there is an existing reference, record the value for the first reference property (only one ref property)\n",
    "                else:\n",
    "                    print('Already an ORCID reference for: ', employees[employeeIndex]['name'])\n",
    "                    # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                    employees[employeeIndex]['orcidReferenceValue'] = '+' + wikidataOrcidData[wikidataOrcidDataIndex]['referenceValues'][0]\n",
    "            # stop checking at the first match.\n",
    "            break\n",
    "    # if the person doesn't match with those whose ORCIDs came back from the query...\n",
    "    if not matched:\n",
    "        # check for access if they have an ORCID (not present in Wikidata)\n",
    "        if employees[employeeIndex]['orcid'] != '':\n",
    "            print('Checking ORCID for unmatched: ', employees[employeeIndex]['name'])\n",
    "            # the function returns the current date (to use as the retrieved date) if the ORCID is found, otherwise empty string\n",
    "            employees[employeeIndex]['orcidReferenceValue'] = checkOrcid(employees[employeeIndex]['orcid'])\n",
    "\n",
    "# get data already in Wikidata about people employed at Vanderbilt\n",
    "prop = 'P108' # employer\n",
    "refProps = ['P854', 'P813'] # source URL, retrieved\n",
    "wikidataEmployerData = searchStatementAtWikidata(qIds, prop, employerQId, refProps)\n",
    "#print(json.dumps(wikidataEmployerData, indent=2))\n",
    "\n",
    "# match people with employment data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matchedStatement = False\n",
    "    matchedReference = False\n",
    "    for wikidataEmployerDataIndex in range(0, len(wikidataEmployerData)):\n",
    "        if wikidataEmployerData[wikidataEmployerDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matchedStatement = True\n",
    "            employees[employeeIndex]['employerStatementUuid'] = wikidataEmployerData[wikidataEmployerDataIndex]['statementUuid']\n",
    "            employees[employeeIndex]['employerReferenceHash'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceHash']\n",
    "            # if there is a referenceHash then record the values for the two reference properties: P813, P854'; retrieved, source URL\n",
    "            if employees[employeeIndex]['employerReferenceHash']!= '':\n",
    "                # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                employees[employeeIndex]['employerReferenceSourceUrl'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0]\n",
    "                if wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0] == deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']:\n",
    "                    matchedReference = True\n",
    "                employees[employeeIndex]['employerReferenceRetrieved'] = '+' + wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][1]\n",
    "            # stop checking if there is an exact match to the reference URL. Otherwise keep looping.\n",
    "            # if there is a later reference that matches ours, it will overwrite any previous reference data\n",
    "            # otherwise, the existing (different) reference data will be retained\n",
    "            if matchedReference:\n",
    "                break\n",
    "        \n",
    "    # everyone is assigned the employerQId as a value because either they showed up in the SPARQL search for employerQId\n",
    "    # or we are making a statement that they work for employerQId.\n",
    "    employees[employeeIndex]['employer'] = employerQId\n",
    "    if not matchedStatement:  # only generate the metadata if there isn't already a statement\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "        employees[employeeIndex]['employerReferenceSourceUrl'] = deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']\n",
    "        employees[employeeIndex]['employerReferenceRetrieved'] = wholeDateZ\n",
    "    # ******************\n",
    "    # NOTE: the end result here is that nothing is done if the \"employer [university name]\" statement (e.g. P108 Q29052) exists.\n",
    "    # If the statement has a different reference than the one we use, it's recorded.  \n",
    "    # ******************\n",
    "\n",
    "# *** This is a copy and paste of the employer section above, modified for affiliation\n",
    "\n",
    "# get data already in Wikidata about people affiliated with the department\n",
    "prop = 'P1416' # affiliation\n",
    "refProps = ['P854', 'P813'] # source URL, retrieved\n",
    "wikidataEmployerData = searchStatementAtWikidata(qIds, prop, deptSettings[deptShortName]['departmentQId'], refProps)\n",
    "#print(json.dumps(wikidataEmployerData, indent=2))\n",
    "\n",
    "# match people with affiliation data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matchedStatement = False\n",
    "    matchedReference = False\n",
    "    for wikidataEmployerDataIndex in range(0, len(wikidataEmployerData)):\n",
    "        if wikidataEmployerData[wikidataEmployerDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matchedStatement = True\n",
    "            employees[employeeIndex]['affiliationStatementUuid'] = wikidataEmployerData[wikidataEmployerDataIndex]['statementUuid']\n",
    "            employees[employeeIndex]['affiliationReferenceHash'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceHash']\n",
    "            # if there is a referenceHash then record the values for the two reference properties: P813, P854'; retrieved, source URL\n",
    "            if employees[employeeIndex]['affiliationReferenceHash']!= '':\n",
    "                # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                employees[employeeIndex]['affiliationReferenceSourceUrl'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0]\n",
    "                if wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0] == deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']:\n",
    "                    matchedReference = True\n",
    "                employees[employeeIndex]['affiliationReferenceRetrieved'] = '+' + wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][1]\n",
    "            # stop checking if there is an exact match to the reference URL. Otherwise keep looping.\n",
    "            # if there is a later reference that matches ours, it will overwrited any previous reference data\n",
    "            # otherwise, the existing (different) reference data will be retained\n",
    "            if matchedReference:\n",
    "                break\n",
    "        \n",
    "    # everyone is assigned the department as a value because either they showed up in the SPARQL search\n",
    "    # or we are making a statement that they are affiliated with the department.\n",
    "    employees[employeeIndex]['affiliation'] = deptSettings[deptShortName]['departmentQId']\n",
    "    if not matchedStatement:  # only generate the metadata if there isn't already a statement\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "        employees[employeeIndex]['affiliationReferenceSourceUrl'] = deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']\n",
    "        employees[employeeIndex]['affiliationReferenceRetrieved'] = wholeDateZ\n",
    "\n",
    "# get all of the data that is already in Wikidata about who are humans\n",
    "prop = 'P31' # instance of\n",
    "value = 'Q5' # human\n",
    "refProps = [] # no ref property needed\n",
    "wikidataHumanData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "\n",
    "# Find out which people have assertions that they are humans and record their statement IDs.\n",
    "# Assign the properties to all others.\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    for wikidataHumanIndex in range(0, len(wikidataHumanData)):\n",
    "        if wikidataHumanData[wikidataHumanIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            employees[employeeIndex]['instanceOfUuid'] = wikidataHumanData[wikidataHumanIndex]['statementUuid']\n",
    "    # everybody is assigned a value of 'human'\n",
    "    employees[employeeIndex]['instanceOf'] = 'Q5'\n",
    "\n",
    "# hack of human code immediately above\n",
    "\n",
    "# get all of the data that is already in Wikidata about the sex or gender of the researchers\n",
    "prop = 'P21' # sex or gender\n",
    "value = '' # don't provide a value so that it will return whatever value it finds\n",
    "refProps = [] # no ref property needed\n",
    "wikidataHumanData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "\n",
    "# Find out which people have assertions of sex/gender and record their statement IDs.\n",
    "# Assign the value for the property to all others.\n",
    "# NOTE: Wikidata doesn't seem to care a lot about references for this property and we don't really have one anyway\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataHumanIndex in range(0, len(wikidataHumanData)):\n",
    "        if wikidataHumanData[wikidataHumanIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['sexOrGenderUuid'] = wikidataHumanData[wikidataHumanIndex]['statementUuid']\n",
    "            # use the value in Wikidata and ignore the value in the 'gender' column of the table.\n",
    "            # extractFromIri() function strips the namespace from the qId\n",
    "            employees[employeeIndex]['sexOrGenderQId'] = extractFromIri(wikidataHumanData[wikidataHumanIndex]['statementValue'], 4)\n",
    "    if not matched:\n",
    "        # assign the value from the 'gender' column in the table if not already in Wikidata\n",
    "        if 'gender' in employees[employeeIndex]:\n",
    "            employees[employeeIndex]['sexOrGenderQId'] = decodeSexOrGender(employees[employeeIndex]['gender'])\n",
    "        else:\n",
    "            employees[employeeIndex]['sexOrGenderQId'] = ''\n",
    "\n",
    "# get all of the English language labels for the employees that are already in Wikidata\n",
    "labelType = 'label'\n",
    "language = 'en'\n",
    "wikidataLabels = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "\n",
    "# Match people with their labels\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataLabelIndex in range(0, len(wikidataLabels)):\n",
    "        if wikidataLabels[wikidataLabelIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['labelEn'] = wikidataLabels[wikidataLabelIndex]['string']\n",
    "    if not matched:\n",
    "        # assign the value from the 'name' column in the table if not already in Wikidata\n",
    "        if deptSettings[deptShortName]['labels']['source'] == 'column':\n",
    "            # then use the value from the default label column.\n",
    "            defaultLabelColumn = deptSettings[deptShortName]['labels']['value']\n",
    "            employees[employeeIndex]['labelEn'] = employees[employeeIndex][defaultLabelColumn]\n",
    "        else:\n",
    "            # or use the default label value.\n",
    "            employees[employeeIndex]['labelEn'] = deptSettings[deptShortName]['labels']['value']\n",
    "\n",
    "# get all of the English language descriptions for the employees that are already in Wikidata\n",
    "labelType = 'description'\n",
    "language = 'en'\n",
    "wikidataDescriptions = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "\n",
    "# Match people with their descriptions\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataDescriptionIndex in range(0, len(wikidataDescriptions)):\n",
    "        if wikidataDescriptions[wikidataDescriptionIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['description'] = wikidataDescriptions[wikidataDescriptionIndex]['string']\n",
    "    if not matched:\n",
    "        # assign a default value if not already in Wikidata\n",
    "        if deptSettings[deptShortName]['descriptions']['source'] == 'column':\n",
    "            # then use the value from the default description column.\n",
    "            defaultDescriptionColumn = deptSettings[deptShortName]['descriptions']['value']\n",
    "            employees[employeeIndex]['description'] = employees[employeeIndex][defaultDescriptionColumn]\n",
    "        else:\n",
    "            # or use the default description value.\n",
    "            employees[employeeIndex]['description'] = deptSettings[deptShortName]['descriptions']['value']\n",
    "\n",
    "# Get all of the aliases already at Wikidata for employees.  \n",
    "# Since there can be multiple aliases, they are stored as a list structure.\n",
    "# The writing script can handle multiple languages, but here we are only dealing with English ones.\n",
    "\n",
    "# retrieve the aliases in that language that already exist in Wikidata and match them with table rows\n",
    "labelType = 'alias'\n",
    "language = 'en'\n",
    "aliasesAtWikidata = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "for entityIndex in range(0, len(employees)):\n",
    "    personAliasList = []\n",
    "    if employees[entityIndex]['wikidataId'] != '':  # don't look for the label at Wikidata if the item doesn't yet exist\n",
    "        for wikiLabel in aliasesAtWikidata:\n",
    "            if employees[entityIndex]['wikidataId'] == wikiLabel['qId']:\n",
    "                personAliasList.append(wikiLabel['string'])\n",
    "    # if not found, the personAliasList list will remain empty\n",
    "    employees[entityIndex]['alias'] = json.dumps(personAliasList)\n",
    "\n",
    "# write the file\n",
    "filename = deptShortName + '-employees-to-write.csv'\n",
    "fieldnames = ['wikidataId', 'name', 'labelEn', 'alias', 'description', 'orcidStatementUuid', 'orcid', 'orcidReferenceHash', 'orcidReferenceValue', 'employerStatementUuid', 'employer', 'employerReferenceHash', 'employerReferenceSourceUrl', 'employerReferenceRetrieved', 'affiliationStatementUuid', 'affiliation', 'affiliationReferenceHash', 'affiliationReferenceSourceUrl', 'affiliationReferenceRetrieved', 'instanceOfUuid', 'instanceOf', 'sexOrGenderUuid', 'sexOrGenderQId', 'gender', 'degree', 'category', 'wikidataStatus', 'role']\n",
    "writeDictsToCsv(employees, filename, fieldnames)\n",
    "\n",
    "print()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
