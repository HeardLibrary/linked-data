{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VanderBot\n",
    "\n",
    "The scripts in this notebook are part of the development of VanderBot, a system to write information about Vanderbilt University researchers and their works to Wikidata.  \n",
    "\n",
    "This code is freely available under a CC0 license. Steve Baskauf 2019-12-16\n",
    "\n",
    "VanderBot 0.8 is under development and subject to continual change. At this point, it's too new to have any stable releases.  \n",
    "\n",
    "For more information, see [this page](https://github.com/HeardLibrary/linked-data/tree/master/publications).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Query() class\n",
    "\n",
    "Methods of the `Query()` class sends queries to Wikibase instances. It has the following methods:\n",
    "\n",
    "`.generic_query(query)` Sends a specified query to the endpoint and returns a list of item Q IDs, item labels, or literal values. The variable to be returned must be `?object`.\n",
    "\n",
    "`.single_property_values_for_item(qid)` Sends a subject Q ID to the endpoint and returns a list of item Q IDs, item labels, or literal values that are values of a specified property.\n",
    "\n",
    "`.labels_descriptions(qids)` Sends a list of subject Q IDs to the endpoint and returns a list of dictionaries of the form `{'qid': qnumber, 'string': string}` where `string` is either a label, description, or alias. Alternatively, an added graph pattern can be passed as `labelscreen` in lieu of the list of Q IDs. In that case, pass an empty list (`[]`) into the method. The screening graph pattern should have `?id` as its only unknown variable.\n",
    "\n",
    "`.search_statement(qids, reference_property_list)` Sends a list of Q IDs and a list of reference properties to the endpoint and returns information about statements using a specified property. If no value is specified, the information includes the values of the statements. For each statement, the reference UUID, reference property, and reference value is returned. If the statement has more than one reference, there will be multiple results per subject. Results are in the form `{'qId': qnumber, 'statementUuid': statement_uuid, 'statementValue': statement_value, 'referenceHash': reference_hash, 'referenceValue': reference_value}`\n",
    "\n",
    "It has the following attributes:\n",
    "\n",
    "| key | description | default value | applicable method |\n",
    "|:-----|:-----|:-----|:-----|\n",
    "| `endpoint` | endpoint URL of Wikabase | `https://query.wikidata.org/sparql` | all |\n",
    "| `mediatype` | Internet media type | `application/json` | all |\n",
    "| `useragent` | User-Agent string to send | `VanderBot/0.8` etc.| all |\n",
    "| `requestheader` | request headers to send |(generated dict) | all |\n",
    "| `sleep` | seconds to delay between queries | 0.25 | all |\n",
    "| `isitem` | `True` if value is item, `False` if value a literal | `True` | `generic_query`, `single_property_values_for_item` |\n",
    "| `uselabel` | `True` for label of item value , `False` for Q ID of item value | `True` | `generic_query`, `single_property_values_for_item` | \n",
    "| `lang` | language of label | `en` | `single_property_values_for_item`, `labels_descriptions`|\n",
    "| `labeltype` | returns `label`, `description`, or `alias` | `label` | `labels_descriptions` |\n",
    "| `labelscreen` | added triple pattern | empty string | `labels_descriptions` |\n",
    "| `pid` | property P ID | `P31` | `single_property_values_for_item`, `search_statement` |\n",
    "| `vid` | value Q ID | empty string | `search_statement` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Code\n",
    "\n",
    "This code block includes import statements, function definitions, and declarations of variables that are common to the rest of the script. It needs to be run once before the other code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "# For a particular processing round, set a short name for the department here.\n",
    "# This name is used to generate a set of unique processing files for that department.\n",
    "testEmployer = 'Vanderbilt University' # to test against Wikidata employer property\n",
    "employerQId = 'Q29052' # Vanderbilt University\n",
    "deathDateLimit = '2000' # any date deaths before this date will be assumed to not be a match\n",
    "\n",
    "# NOTE: eventually need to test against all affiliations in cases of faculty with multiple appointments\n",
    "\n",
    "# ***********************************\n",
    "# NOTE: the script fails if there is a current item in Wikidata that has the same values for both label and description. \n",
    "# A check needs to be run for this !!!\n",
    "# ***********************************\n",
    "\n",
    "# Here is some example JSON from a departmental configuration file (department-configuration.json):\n",
    "\n",
    "'''\n",
    "{\n",
    "  \"deptShortName\": \"anthropology\",\n",
    "  \"aads\": {\n",
    "    \"categories\": [\n",
    "      \"\"\n",
    "    ],\n",
    "    \"baseUrl\": \"https://as.vanderbilt.edu/aads/people/\",\n",
    "    \"nTables\": 1,\n",
    "    \"departmentSearchString\": \"African American and Diaspora Studies\",\n",
    "    \"departmentQId\": \"Q79117444\",\n",
    "    \"testAuthorAffiliation\": \"African American Diaspora Studies Vanderbilt\",\n",
    "    \"labels\": {\n",
    "      \"source\": \"column\",\n",
    "      \"value\": \"name\"\n",
    "    },\n",
    "    \"descriptions\": {\n",
    "      \"source\": \"constant\",\n",
    "      \"value\": \"African American and Diaspora Studies scholar\"\n",
    "    }\n",
    "  },\n",
    "  \"bsci\": {\n",
    "    \"categories\": [\n",
    "      \"primary-training-faculty\",\n",
    "      \"research-and-teaching-faculty\",\n",
    "      \"secondary-faculty\",\n",
    "      \"postdoc-fellows\",\n",
    "      \"emeriti\"\n",
    "    ],\n",
    "    \"baseUrl\": \"https://as.vanderbilt.edu/biosci/people/index.php?group=\",\n",
    "    \"nTables\": 1,\n",
    "    \"departmentSearchString\": \"Biological Sciences\",\n",
    "    \"departmentQId\": \"Q78041310\",\n",
    "    \"testAuthorAffiliation\": \"Biological Sciences Vanderbilt\",\n",
    "    \"labels\": {\n",
    "      \"source\": \"column\",\n",
    "      \"value\": \"name\"\n",
    "    },\n",
    "    \"descriptions\": {\n",
    "      \"source\": \"constant\",\n",
    "      \"value\": \"biology researcher\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''\n",
    "# Note that the first key: value pair sets the department to be processed.\n",
    "\n",
    "# The default labels and descriptions can either be a column in the table or set as a constant. \n",
    "# If it's a column, the value is the column header.  If it's a constant, the value is the string to assign as the value.\n",
    "\n",
    "# The nTables value is the number of HTML tables in the page to be searched.  Currently (2020-01-19) it isn't used\n",
    "# and the script just checks all of the tables, but it could be implemented if there are tables at the end that don't \n",
    "# include employee names.\n",
    "\n",
    "with open('department-configuration.json', 'rt', encoding='utf-8') as fileObject:\n",
    "    text = fileObject.read()\n",
    "deptSettings = json.loads(text)\n",
    "deptShortName = deptSettings['deptShortName']\n",
    "print('Department currently set for', deptShortName)\n",
    "\n",
    "wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "degreeList = [\n",
    "    {'string': 'Ph.D.', 'value': 'Ph.D.'},\n",
    "    {'string': 'PhD', 'value': 'Ph.D.'},\n",
    "    {'string': 'D.Phil.', 'value': 'D.Phil.'},\n",
    "    {'string': 'J.D.', 'value': 'J.D.'}\n",
    "     ]\n",
    "\n",
    "# NCBI identification requirements:\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'VanderBot' # give your application a name here\n",
    "\n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generateHeaderDictionary(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/0.8 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def writeCsv(fileName, array):\n",
    "    fileObject = open(fileName, 'w', newline='', encoding='utf-8')\n",
    "    writerObject = csv.writer(fileObject)\n",
    "    for row in array:\n",
    "        writerObject.writerow(row)\n",
    "    fileObject.close()\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def writeDictsToCsv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvFileObject:\n",
    "        writer = csv.DictWriter(csvFileObject, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def readDict(filename):\n",
    "    fileObject = open(filename, 'r', newline='', encoding='utf-8')\n",
    "    dictObject = csv.DictReader(fileObject)\n",
    "    array = []\n",
    "    for row in dictObject:\n",
    "        array.append(row)\n",
    "    fileObject.close()\n",
    "    return array\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extractQNumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts a local name from an IRI, specify the list item number for the last piece separated by slash\n",
    "def extractFromIri(iri, numberPieces):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[numberPieces]\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts a local name from an IRI, specify the list item number for the last piece separated by slash\n",
    "def extract_from_iri(iri, number_pieces):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[number_pieces]\n",
    "\n",
    "# see https://www.wikidata.org/wiki/Property:P21 for values\n",
    "def decodeSexOrGender(code):\n",
    "    code = code.lower()\n",
    "    if code == 'm':\n",
    "        qId = 'Q6581097'\n",
    "    elif code == 'f':\n",
    "        qId = 'Q6581072'\n",
    "    elif code == 'i':\n",
    "        qId = 'Q1097630'\n",
    "    elif code == 'tf':\n",
    "        qId = 'Q1052281'\n",
    "    elif code == 'tm':\n",
    "        qId = 'Q2449503'\n",
    "    else:\n",
    "        qId = ''\n",
    "    return qId\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, **kwargs):\n",
    "        # attributes for all methods\n",
    "        try:\n",
    "            self.lang = kwargs['lang']\n",
    "        except:\n",
    "            self.lang = 'en' # default to English\n",
    "        try:\n",
    "            self.mediatype = kwargs['mediatype']\n",
    "        except:\n",
    "            self.mediatype = 'application/json' # default to JSON formatted query results\n",
    "        try:\n",
    "            self.endpoint = kwargs['endpoint']\n",
    "        except:\n",
    "            self.endpoint = 'https://query.wikidata.org/sparql' # default to Wikidata endpoint\n",
    "        try:\n",
    "            self.useragent = kwargs['useragent']\n",
    "        except:\n",
    "            self.useragent = 'VanderBot/0.8 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)' \n",
    "        self.requestheader = {\n",
    "        'Accept' : self.mediatype,\n",
    "        'User-Agent': self.useragent\n",
    "        }\n",
    "        try:\n",
    "            self.pid = kwargs['pid'] # property's P ID\n",
    "        except:\n",
    "            self.pid = 'P31' # default to \"instance of\"  \n",
    "        try:\n",
    "            self.sleep = kwargs['sleep']\n",
    "        except:\n",
    "            self.sleep = 0.25 # default throtting of 0.25 seconds\n",
    "            \n",
    "        # attributes for single property values method\n",
    "        try:\n",
    "            self.isitem = kwargs['isitem']\n",
    "        except:\n",
    "            self.isitem = True # default to values are items rather than literals   \n",
    "        try:\n",
    "            self.uselabel = kwargs['uselabel']\n",
    "        except:\n",
    "            self.uselabel = True # default is to show labels of items\n",
    "            \n",
    "        # attributes for labels and descriptions method\n",
    "        try:\n",
    "            self.labeltype = kwargs['labeltype']\n",
    "        except:\n",
    "            self.labeltype = 'label' # default to \"label\". Other options: \"description\", \"alias\"\n",
    "        try:\n",
    "            self.labelscreen = kwargs['labelscreen']\n",
    "        except:\n",
    "            self.labelscreen = '' # instead of using a list of subject items, add this line to screen for items\n",
    "            \n",
    "        # attributes for search_statement method\n",
    "        try:\n",
    "            self.vid = kwargs['vid'] # Q ID of the value of a statement. \n",
    "        except:\n",
    "            self.vid = '' # default to no value (the method returns the value of the statement)\n",
    "            \n",
    "    # send a generic query and return a list of Q IDs\n",
    "    def generic_query(self, query):\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        results_list = []\n",
    "        try:\n",
    "        #if 1==1: # replace try: to let errors occur, also comment out the except: clause\n",
    "            data = r.json()\n",
    "            #print(data)\n",
    "            statements = data['results']['bindings']\n",
    "            if len(statements) > 0: # if no results, the list remains empty\n",
    "                for statement in statements:\n",
    "                    if self.isitem:\n",
    "                        if self.uselabel:\n",
    "                            result_value = statement['entity']['value']\n",
    "                        else:\n",
    "                            result_value = extract_qnumber(statement['entity']['value'])\n",
    "                    else:\n",
    "                        result_value = statement['entity']['value']\n",
    "                    results_list.append(result_value)\n",
    "        except:\n",
    "            results_list = [r.text]\n",
    "        \n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "            \n",
    "\n",
    "    # returns the value of a single property for an item by Q ID\n",
    "    def single_property_values_for_item(self, qid):\n",
    "        query = '''\n",
    "select distinct ?object where {\n",
    "    wd:'''+ qid + ''' wdt:''' + self.pid\n",
    "        if self.uselabel and self.isitem:\n",
    "            query += ''' ?objectItem.\n",
    "    ?objectItem rdfs:label ?object.\n",
    "    FILTER(lang(?object) = \"''' + self.lang +'\")'\n",
    "        else:\n",
    "            query += ''' ?object.'''            \n",
    "        query +=  '''\n",
    "    }'''\n",
    "        #print(query)\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        results_list = []\n",
    "        try:\n",
    "        #if 1==1: # replace try: to let errors occur, also comment out the except: clause\n",
    "            data = r.json()\n",
    "            #print(data)\n",
    "            statements = data['results']['bindings']\n",
    "            if len(statements) > 0: # if no results, the list remains empty\n",
    "                for statement in statements:\n",
    "                    if self.isitem:\n",
    "                        if self.uselabel:\n",
    "                            result_value = statement['object']['value']\n",
    "                        else:\n",
    "                            result_value = extract_qnumber(statement['object']['value'])\n",
    "                    else:\n",
    "                        result_value = statement['object']['value']\n",
    "                    results_list.append(result_value)\n",
    "        except:\n",
    "            results_list = [r.text]\n",
    "        \n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "    \n",
    "    # search for any of the \"label\" types: label, alias, description. qids is a list of Q IDs without namespaces\n",
    "    def labels_descriptions(self, qids):\n",
    "        # option to explicitly list subject Q IDs\n",
    "        if self.labelscreen == '':\n",
    "            # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "            alternatives = ''\n",
    "            for qid in qids:\n",
    "                alternatives += 'wd:' + qid + '\\n'\n",
    "\n",
    "        if self.labeltype == 'label':\n",
    "            predicate = 'rdfs:label'\n",
    "        elif self.labeltype == 'alias':\n",
    "            predicate = 'skos:altLabel'\n",
    "        elif self.labeltype == 'description':\n",
    "            predicate = 'schema:description'\n",
    "        else:\n",
    "            predicate = 'rdfs:label'        \n",
    "\n",
    "        # create a string for the query\n",
    "        query = '''\n",
    "select distinct ?id ?string where {'''\n",
    "        \n",
    "        # option to explicitly list subject Q IDs\n",
    "        if self.labelscreen == '':\n",
    "            query += '''\n",
    "      VALUES ?id\n",
    "    {\n",
    "''' + alternatives + '''\n",
    "    }'''\n",
    "        # option to screen for Q IDs by triple pattern\n",
    "        if self.labelscreen != '':\n",
    "            query += '''\n",
    "    ''' + self.labelscreen\n",
    "            \n",
    "        query += '''\n",
    "    ?id '''+ predicate + ''' ?string.\n",
    "    filter(lang(?string)=\"''' + self.lang + '''\")\n",
    "    }'''\n",
    "        #print(query)\n",
    "\n",
    "        results_list = []\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        data = r.json()\n",
    "        results = data['results']['bindings']\n",
    "        for result in results:\n",
    "            # remove wd: 'http://www.wikidata.org/entity/'\n",
    "            qnumber = extract_qnumber(result['id']['value'])\n",
    "            string = result['string']['value']\n",
    "            results_list.append({'qid': qnumber, 'string': string})\n",
    "\n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "\n",
    "    # Searches for statements using a particular property. If no value is set, the value will be returned.\n",
    "    def search_statement(self, qids, reference_property_list):\n",
    "        # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "        alternatives = ''\n",
    "        for qid in qids:\n",
    "            alternatives += 'wd:' + qid + '\\n'\n",
    "\n",
    "        # create a string for the query\n",
    "        query = '''\n",
    "select distinct ?id ?statement '''\n",
    "        # if no value was specified, find the value\n",
    "        if self.vid == '':\n",
    "            query += '?statementValue '\n",
    "        if len(reference_property_list) != 0:\n",
    "            query += '?reference '\n",
    "        for ref_prop_index in range(0, len(reference_property_list)):\n",
    "            query += '?refVal' + str(ref_prop_index) + ' '\n",
    "        query += '''\n",
    "    where {\n",
    "        VALUES ?id\n",
    "    {\n",
    "''' + alternatives + '''\n",
    "    }\n",
    "    ?id p:'''+ self.pid + ''' ?statement.\n",
    "    ?statement ps:'''+ self.pid\n",
    "\n",
    "        if self.vid == '': # return the value of the statement if no particular value is specified\n",
    "            query += ' ?statementValue.'\n",
    "        else:\n",
    "            query += ' wd:' + self.vid + '.' # specify the value to be searched for\n",
    "\n",
    "        if len(reference_property_list) != 0:\n",
    "            query += '''\n",
    "    optional {\n",
    "        ?statement prov:wasDerivedFrom ?reference.''' # search for references if there are any\n",
    "            for ref_prop_index in range(0, len(reference_property_list)):\n",
    "                query +='''\n",
    "        ?reference pr:''' + reference_property_list[ref_prop_index] + ' ?refVal' + str(ref_prop_index) + '.'\n",
    "            query +='''\n",
    "            }'''\n",
    "        query +='''\n",
    "      }'''\n",
    "        print(query)\n",
    "\n",
    "        results_list = []\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        data = r.json()\n",
    "        results = data['results']['bindings']\n",
    "        # ********** NOTE: need to deal with case where there are more than one reference per result\n",
    "        # This will result in several results with the same subject qNumber\n",
    "        for result in results:\n",
    "            # remove wd: 'http://www.wikidata.org/entity/'\n",
    "            qnumber = extract_qnumber(result['id']['value'])\n",
    "            # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "            no_domain = extract_from_iri(result['statement']['value'], 5)\n",
    "            # need to remove the qNumber that's appended in front of the UUID\n",
    "            pieces = no_domain.split('-')\n",
    "            last_pieces = pieces[1:len(pieces)]\n",
    "            s = \"-\"\n",
    "            statement_uuid = s.join(last_pieces)\n",
    "\n",
    "            # if no value was specified, get the value that was found in the search\n",
    "            if self.vid == '':\n",
    "                statement_value = result['statementValue']['value']\n",
    "            # extract the reference property data if any reference properties were specified\n",
    "            if len(reference_property_list) != 0:\n",
    "                if 'reference' in result:\n",
    "                    # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "                    reference_hash = extract_qnumber(result['reference']['value'])\n",
    "                else:\n",
    "                    reference_hash = ''\n",
    "                reference_values = []\n",
    "                for ref_prop_index in range(0, len(reference_property_list)):\n",
    "                    if 'refVal' + str(ref_prop_index) in result:\n",
    "                        reference_value = result['refVal' + str(ref_prop_index)]['value']\n",
    "                        # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "                        #if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                        #    referenceValue = referenceValue.split('T')[0]\n",
    "                    else:\n",
    "                        reference_value = ''\n",
    "                    reference_values.append(reference_value)\n",
    "            results_dict = {'qId': qnumber, 'statementUuid': statement_uuid}\n",
    "            # if no value was specified, get the value that was found in the search\n",
    "            if self.vid == '':\n",
    "                results_dict['statementValue'] = statement_value\n",
    "            if len(reference_property_list) != 0:\n",
    "                results_dict['referenceHash'] = reference_hash\n",
    "                results_dict['referenceValues'] = reference_values\n",
    "            results_list.append(results_dict)\n",
    "\n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "    \n",
    "# query for a single variable that's an item named 'item'\n",
    "# returns a list of results\n",
    "def searchWikidataForQIdByOrcid(orcid):\n",
    "    query = '''\n",
    "select distinct ?item where {\n",
    "  ?item wdt:P496 \"''' + employees[employeeIndex]['orcid'] + '''\".\n",
    "  }\n",
    "'''\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append(qNumber)\n",
    "    except:\n",
    "        results = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# returns a list of employer strings for the item with Wikidata ID qId; P108 is employer\n",
    "def searchWikidataEmployer(qId):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?employer where {\n",
    "        wd:'''+ qId + ''' wdt:P108 ?employerId.\n",
    "        ?employerId rdfs:label ?employer.\n",
    "        FILTER(lang(?employer) = 'en')\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                resultsList.append(statement['employer']['value'])\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "# returns a list of value Q IDs of the property propertyId for the item with Wikidata ID qId\n",
    "def searchWikidataSingleProperty(qId, propertyId, valueType):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?object where {\n",
    "        wd:'''+ qId + ''' wdt:''' + propertyId + ''' ?object.\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                if valueType == 'item':\n",
    "                    resultValue = extractQNumber(statement['object']['value'])\n",
    "                else:\n",
    "                    resultValue = statement['object']['value']\n",
    "                resultsList.append(resultValue)\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "def checkOrcid(orcid):\n",
    "    namespace = 'https://orcid.org/'\n",
    "    endpointUrl = namespace + orcid\n",
    "    acceptMediaType = 'application/ld+json'\n",
    "    r = requests.get(endpointUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    code = r.status_code\n",
    "    #print(r.text)\n",
    "    data = r.json()\n",
    "    response = {'code': code, 'data': data}\n",
    "    if response['code'] != 200:\n",
    "        print('Attempt to dereference ORCID resulted in HTTP response code ', response['code'])\n",
    "        data['orcidReferenceValue'] = ''\n",
    "    else:\n",
    "        print('Successfully retrieved')\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "    # delay a quarter second to avoid hitting the API too rapidly\n",
    "    sleep(0.25)\n",
    "    return(wholeDateZ)\n",
    "\n",
    "# if the value passed is '' then the value will be retrieved.  Otherwise, the value is used to screen.\n",
    "def searchStatementAtWikidata(qIds, prop, value, refPropList):\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?statement '\n",
    "    # if no value was specified, find the value\n",
    "    if value == '':\n",
    "        query += '?statementValue '\n",
    "    if len(refPropList) != 0:\n",
    "        query += '?reference '\n",
    "    for refPropIndex in range(0, len(refPropList)):\n",
    "        query += '?refVal' + str(refPropIndex) + ' '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id p:'''+ prop + ''' ?statement.\n",
    "  ?statement ps:'''+ prop\n",
    "    \n",
    "    if value == '':\n",
    "        query += ' ?statementValue.'\n",
    "    else:\n",
    "        query += ' wd:' + value + '.'\n",
    "\n",
    "    if len(refPropList) != 0:\n",
    "        query += '''\n",
    "  optional {\n",
    "    ?statement prov:wasDerivedFrom ?reference.'''\n",
    "        for refPropIndex in range(0, len(refPropList)):\n",
    "            query +='''\n",
    "    ?reference pr:''' + refPropList[refPropIndex] + ''' ?refVal''' + str(refPropIndex) + '''.'''\n",
    "        query +='''\n",
    "        }'''\n",
    "    query +='''\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    # ********** NOTE: need to deal with case where there are more than one reference per result\n",
    "    # This will result in several results with the same qNumeber, orcid, and referenceHash\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "        noDomain = extractFromIri(result['statement']['value'], 5)\n",
    "        # need to remove the qNumber that's appended in front of the UUID\n",
    "        \n",
    "        # NOTE: formerly used this:\n",
    "        #statementUuid = noDomain.partition(qNumber + '-')[2]\n",
    "        # However, there was at least one case where the appended qNumber had a lower case Q and failed to match.\n",
    "        # So needed a different approach.\n",
    "        pieces = noDomain.split('-')\n",
    "        lastPieces = pieces[1:len(pieces)]\n",
    "        s = \"-\"\n",
    "        statementUuid = s.join(lastPieces)\n",
    "\n",
    "        # if no value was specified, get the value that was found in the search\n",
    "        if value == '':\n",
    "            statementValue = result['statementValue']['value']\n",
    "        if len(refPropList) != 0:\n",
    "            if 'reference' in result:\n",
    "                # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "                referenceHash = extractFromIri(result['reference']['value'], 4)\n",
    "            else:\n",
    "                referenceHash = ''\n",
    "            referenceValues = []\n",
    "            for refPropIndex in range(0, len(refPropList)):\n",
    "                if 'refVal' + str(refPropIndex) in result:\n",
    "                    refVal = result['refVal' + str(refPropIndex)]['value']\n",
    "                    # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "                    #if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                    #    referenceValue = referenceValue.split('T')[0]\n",
    "                else:\n",
    "                    refVal = ''\n",
    "                referenceValues.append(refVal)\n",
    "        resultsDict = {'qId': qNumber, 'statementUuid': statementUuid}\n",
    "        # if no value was specified, get the value that was found in the search\n",
    "        if value == '':\n",
    "            resultsDict['statementValue'] = statementValue\n",
    "        if len(refPropList) != 0:\n",
    "            resultsDict['referenceHash'] = referenceHash\n",
    "            resultsDict['referenceValues'] = referenceValues\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    \n",
    "    return returnValue\n",
    "\n",
    "# search for any of the \"label\" types: label, alias, description\n",
    "def searchLabelsDescriptionsAtWikidata(qIds, labelType, language):\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    if labelType == 'label':\n",
    "        predicate = 'rdfs:label'\n",
    "    elif labelType == 'alias':\n",
    "        predicate = 'skos:altLabel'\n",
    "    elif labelType == 'description':\n",
    "        predicate = 'schema:description'\n",
    "    else:\n",
    "        predicate = 'rdfs:label'        \n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?string '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id '''+ predicate + ''' ?string.\n",
    "  filter(lang(?string)=\"''' + language + '''\")\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        string = result['string']['value']\n",
    "        resultsDict = {'qId': qNumber, 'string': string}\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    \n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query ORCID for Vanderbilt University people\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/orcid/orcid-get-json.ipynb\n",
    "\n",
    "Retrieves results 100 at a time, then processes them by extracting desired information.  NOTE: takes hours to run.\n",
    "\n",
    "Saves results in a file and the alternative names in a second file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [['orcid', 'givenNames', 'familyName', 'startDate', 'endDate', 'department', 'organization']]\n",
    "otherNameList = [['orcid', 'altName']]\n",
    "\n",
    "# use the API to search for people associated with Vanderbilt University\n",
    "# First search is for only one record, just to get the number of hits found\n",
    "searchUri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start=1&rows=1'\n",
    "acceptMediaType = 'application/json'\n",
    "response = requests.get(searchUri, headers = generateHeaderDictionary(acceptMediaType))\n",
    "data = response.json()\n",
    "numberResults = data[\"num-found\"]\n",
    "#print(data[\"num-found\"])\n",
    "numberPages = math.floor(numberResults/100)\n",
    "#print(numberPages)\n",
    "remainder = numberResults - 100*numberPages\n",
    "#print(remainder)\n",
    "\n",
    "for pageCount in range(0, numberPages+1):  # the remainder will be caught when pageCount = numberPages\n",
    "    print('page: ', pageCount)\n",
    "    searchUri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start='+str(pageCount*100+1)\n",
    "    response = requests.get(searchUri, headers={'Accept' : 'application/json'})\n",
    "    print(response.url)\n",
    "    data = response.json()\n",
    "    orcidsDictsList = data['result']\n",
    "\n",
    "    # extract the identifier strings from the data structure\n",
    "    orcids = []\n",
    "    for orcidDict in orcidsDictsList:\n",
    "        dictionary = {'id': orcidDict['orcid-identifier']['path'], 'iri': orcidDict['orcid-identifier']['uri']}\n",
    "        orcids.append(dictionary)\n",
    "\n",
    "    for orchidIndex in range(0, len(orcids)):\n",
    "        response = requests.get(orcids[orchidIndex]['iri'], headers={'Accept' : 'application/json'})\n",
    "        data = response.json()\n",
    "\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        orcidId = data['orcid-identifier']['path']\n",
    "        #print(orcidId)\n",
    "        # if there isn't a name, then go on to the next ORCID\n",
    "        if not data['person']['name']:\n",
    "            continue\n",
    "        if data['person']['name']['given-names']:  \n",
    "            givenNames = data['person']['name']['given-names']['value']\n",
    "        else:\n",
    "            givenNames = ''\n",
    "        if data['person']['name']['family-name']:\n",
    "            familyName = data['person']['name']['family-name']['value']\n",
    "        else:\n",
    "            familyName = ''\n",
    "        #print(givenNames, ' ', familyName)\n",
    "        otherNames = data['person']['other-names']['other-name']\n",
    "        for otherName in otherNames:\n",
    "            #print(otherName['content'])\n",
    "            otherNameList.append([orcidId, otherName['content']])\n",
    "\n",
    "        affiliations = data['activities-summary']['employments']['affiliation-group']\n",
    "        #print(json.dumps(affiliations, indent = 2))\n",
    "        for affiliation in affiliations:\n",
    "            summaries = affiliation['summaries']\n",
    "            #print(summaries)\n",
    "            #print()\n",
    "            for summary in summaries:\n",
    "                employment = summary['employment-summary']\n",
    "                #print(json.dumps(employment, indent = 2))\n",
    "                startDate = ''\n",
    "                if employment['start-date']:\n",
    "                    if employment['start-date']['year']:\n",
    "                        startDate += employment['start-date']['year']['value']\n",
    "                        startMonth = employment['start-date']['month']\n",
    "                        if startMonth:\n",
    "                            startDate += '-' + startMonth['value']\n",
    "                            startDay = employment['start-date']['day']\n",
    "                            if startDay:\n",
    "                                startDate += '-' + startDay['value']\n",
    "                #print('start date: ', startDate)\n",
    "                endDate = ''\n",
    "                if employment['end-date']:\n",
    "                    if employment['end-date']['year']:\n",
    "                        endDate += employment['end-date']['year']['value']\n",
    "                        endMonth = employment['end-date']['month']\n",
    "                        if endMonth:\n",
    "                            endDate += '-' + endMonth['value']\n",
    "                            endDay = employment['end-date']['day']\n",
    "                            if endDay:\n",
    "                                endDate += '-' + endDay['value']\n",
    "                #print('end date: ', endDate)\n",
    "                department = employment['department-name']\n",
    "                # if there is no value for department, set it to empty string\n",
    "                if not department:\n",
    "                    department = ''\n",
    "                #print(department)\n",
    "                if employment['organization']:\n",
    "                    organization = employment['organization']['name']\n",
    "                #print(organization)\n",
    "                if 'Vanderbilt University' in organization:\n",
    "                    print(orcidId, givenNames, familyName, startDate, endDate, department, organization)\n",
    "                    table.append([orcidId, givenNames, familyName, startDate, endDate, department, organization])\n",
    "                #print(table)\n",
    "        sleep(.25)\n",
    "\n",
    "print()\n",
    "print('Done')\n",
    "fileName = 'orcid_data.csv'\n",
    "writeCsv(fileName, table)\n",
    "fileName = 'orcid_other_names.csv'\n",
    "writeCsv(fileName, otherNameList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape departmental website\n",
    "\n",
    "script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/scrape-bsci.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bsci_type_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    # get the tables from the page\n",
    "    tableObjects = soupObject.find_all('table')\n",
    "    for tableObject in tableObjects:  # this assumes that all tables on the page contain names\n",
    "    \n",
    "        # get the rows from the table\n",
    "        rowObjectsList = tableObject.find_all('tr')\n",
    "        for rowObject in rowObjectsList:\n",
    "            try:\n",
    "                # get the cells from each row\n",
    "                cellObjectsList = rowObject.find_all('td')\n",
    "                # picture is in cell 0, name and title is in cell 1\n",
    "                nameCell = cellObjectsList[1]\n",
    "                # the name part is bolded\n",
    "                name = nameCell('strong')[0].text\n",
    "                # remove leading and trailing whitespace, including newlines\n",
    "                name = name.strip()\n",
    "            except:\n",
    "                # if it can't find the strong tag or the second cell, give up on that row\n",
    "                pass\n",
    "\n",
    "            # check to see if the name has already been added to the list (some depts put people on two category lists)\n",
    "            found = False\n",
    "            for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                if person['name'] == name:\n",
    "                    found = True\n",
    "                    break  # quit looking for the person\n",
    "            if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                # separate degrees from names\n",
    "                degree = ''\n",
    "                for testDegree in degreeList:\n",
    "                    if testDegree['string'] in name:\n",
    "                        name = name.partition(', ' + testDegree['string'])[0]\n",
    "                        # correct any malformed strings\n",
    "                        degree = testDegree['value']\n",
    "\n",
    "                try:\n",
    "                    # process the roles text\n",
    "                    dirtyText  = str(nameCell)\n",
    "                    # get rid of trailing td tag\n",
    "                    nameCellText = dirtyText.split('</td>')[0]\n",
    "                    cellLines = nameCellText.split('<br/>')\n",
    "                    roles = []\n",
    "                    for lineIndex in range(1, len(cellLines)):\n",
    "                        roleDict = {}\n",
    "                        # remove leading and trailling whitespace\n",
    "                        rawText = cellLines[lineIndex].strip()\n",
    "                        if ' of ' in rawText:\n",
    "                            pieces = rawText.split(' of ')\n",
    "                            roleDict['title'] = pieces[0]\n",
    "                            roleDict['department'] = pieces[1]\n",
    "                            roles.append(roleDict)\n",
    "                        elif ' in ' in rawText:\n",
    "                            pieces = rawText.split(' in ')\n",
    "                            roleDict['title'] = pieces[0]\n",
    "                            roleDict['department'] = pieces[1]\n",
    "                            roles.append(roleDict)\n",
    "                        else:\n",
    "                            roleDict['title'] = rawText\n",
    "                            roleDict['department'] = ''\n",
    "                            roles.append(roleDict)\n",
    "                        if ', Emeritus' in roleDict['department']:\n",
    "                            roleDict['department'] = roleDict['department'].split(', Emeritus')[0]\n",
    "                            roleDict['title'] = 'Emeritus ' + roleDict['title']\n",
    "                    rolesJson = json.dumps(roles)\n",
    "\n",
    "                except:\n",
    "                    rolesJson = ''\n",
    "                accumulationTable.append([name, degree, rolesJson, category])\n",
    "    return accumulationTable\n",
    "\n",
    "def cineart_type_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    # get the tables from the page\n",
    "    tableObjects = soupObject.find_all('table')\n",
    "    for tableObject in tableObjects:\n",
    "        rowObjectsList = tableObject.find_all('tr')\n",
    "        for rowObject in rowObjectsList:\n",
    "            try:\n",
    "                # get the cells from each row\n",
    "                cellObjectsList = rowObject.find_all('td')\n",
    "                # picture is in cell 0, name is in cell 1\n",
    "                nameCell = cellObjectsList[1]\n",
    "                # the name part is heading 4\n",
    "                name = nameCell('h4')[0].text\n",
    "                # remove leading and trailing whitespace, including newlines\n",
    "                name = name.strip()\n",
    "            except:\n",
    "                # if it can't find the strong tag or the second cell, give up on that row\n",
    "                pass\n",
    "            accumulationTable.append([name, '', '[]', category])\n",
    "    return accumulationTable\n",
    "\n",
    "def amstudies_scrape(soupObject):\n",
    "    categories = ['administrative', 'core', 'secondary', 'affiliated']\n",
    "    accumulationTable = []\n",
    "    content = soupObject.find_all('section')[0]\n",
    "    article = content('article')[0]\n",
    "    ps = soupObject.find_all('p')\n",
    "    for p in ps:\n",
    "        # ideosyncratic screen for administrators\n",
    "        found = False\n",
    "        if len(p.find_all('a')) != 0:\n",
    "            possibleName = p.find_all('a')[0] # admin faculty names are in the a tags\n",
    "            if not '@' in possibleName.text: # eliminate the email address a tags\n",
    "                if not '?' in possibleName.get('href'): # eliminate link with (c) in href value\n",
    "                    found = True\n",
    "                    name = possibleName.text\n",
    "        if found:\n",
    "            stringText = str(p)\n",
    "            role = stringText.split('<br/>')[1].strip()\n",
    "\n",
    "            accumRoles = []\n",
    "            roleDict = {}\n",
    "            roleDict['title'] = role\n",
    "            roleDict['department'] = role\n",
    "            accumRoles.append(roleDict)\n",
    "            accumulationTable.append([name, '', json.dumps(accumRoles), 'administrative'])            \n",
    "            #accumulationTable.append([name, '', '[\"title\": \"' + role + '\"]', 'administrative'])\n",
    "        # screen for core faculty\n",
    "        secondFound = False\n",
    "        if not found:\n",
    "            names = p.find_all('strong')\n",
    "            if len(names) == 1:\n",
    "                secondFound = True\n",
    "                name = names[0].text.strip()\n",
    "                category = 'core'\n",
    "        if secondFound:\n",
    "            stringText = str(p)\n",
    "            role = stringText.split('<br/>')[1].strip()\n",
    "            if role != '</strong>Program Administrator':  \n",
    "                accumRoles = []\n",
    "                roleDict = {}\n",
    "                roleDict['title'] = role\n",
    "                roleDict['department'] = role\n",
    "                accumRoles.append(roleDict)\n",
    "                accumulationTable.append([name, '', json.dumps(accumRoles), 'core'])\n",
    "                #accumulationTable.append([name, '', '[\"title\": \"' + role + '\"]', 'core'])\n",
    "    outerDivs = soupObject.find_all('div')\n",
    "    outerDiv = outerDivs[5]\n",
    "    innerDivs = outerDiv.find_all('div')\n",
    "    secondaryDiv = innerDivs[2]\n",
    "    accumulationTable = pull_amstudies_divs(secondaryDiv, 'secondary', accumulationTable)\n",
    "    affiliatedDiv = innerDivs[5]\n",
    "    accumulationTable = pull_amstudies_divs(affiliatedDiv, 'affiliated', accumulationTable)\n",
    "\n",
    "    return accumulationTable\n",
    "\n",
    "# Note: this is so ideosyncratic that the roles need to be manually edited after running\n",
    "def pull_amstudies_divs(div, category, accumulationTable):\n",
    "    if category == 'secondary':\n",
    "        p = div.find_all('div')[1]\n",
    "    else:\n",
    "        p = div\n",
    "    names = p.find_all('strong')\n",
    "    text = str(p)\n",
    "    rolesBlobs = text.split(',')\n",
    "    roles = []\n",
    "    for roleString in rolesBlobs[1:len(rolesBlobs)]:\n",
    "        role = roleString.split('<')[0].strip()\n",
    "        if role != 'Health':\n",
    "            if role != 'and Society':\n",
    "                if role != 'and Society and Anthropology':\n",
    "                    if 'of Medicine' in role:\n",
    "                        roles.append(role + ', Health, and Society')\n",
    "                    else:\n",
    "                        roles.append(role)\n",
    "    for personNumber in range(0, len(names)):\n",
    "        accumRoles = []\n",
    "        roleDict = {}\n",
    "        roleDict['title'] = roles[personNumber]\n",
    "        roleDict['department'] = roles[personNumber]\n",
    "        accumRoles.append(roleDict)\n",
    "        accumulationTable.append([names[personNumber].text, '', json.dumps(accumRoles), category])\n",
    "    return accumulationTable\n",
    "\n",
    "def art_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    divObjects = soupObject.find_all('div')\n",
    "    for div in divObjects:\n",
    "        try:\n",
    "            if div.get('class')[0] == 'row':\n",
    "                pObjects = div.find_all('p')\n",
    "                for p in pObjects:\n",
    "                    aObjects = p.find_all('a')\n",
    "                    for a in aObjects:\n",
    "                        name = a.text\n",
    "                        if not '@' in name:\n",
    "                            if name != 'CV':\n",
    "                                if name != 'F':\n",
    "                                    if name == 'arrar Hood Cusomato':\n",
    "                                        name = 'Farrar Hood Cusomato'\n",
    "\n",
    "                                    # avoid duplicate entries\n",
    "                                    found = False\n",
    "                                    for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                                        if person[0] == name:\n",
    "                                            found = True\n",
    "                                            break  # quit looking for the person\n",
    "                                    if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                                        accumulationTable.append([name, '', '[]', category])\n",
    "        except:\n",
    "            pass\n",
    "    return accumulationTable\n",
    "\n",
    "def asian_studies_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    divObjects = soupObject.find_all('div')\n",
    "    for div in divObjects:\n",
    "        try:\n",
    "            if div.get('class')[0] == 'row':\n",
    "                pObjects = div.find_all('p')\n",
    "                for p in pObjects:\n",
    "                    aObjects = p.find_all('a')\n",
    "                    if len(aObjects) >= 1:\n",
    "                        for a in aObjects:\n",
    "                            if not '@' in str(a):\n",
    "                                name = a.text.strip()\n",
    "                                if name != '':\n",
    "                                    if name != 'Alejandro':\n",
    "                                        if name == 'Acierto':\n",
    "                                            name = 'Alejandro Acierto'\n",
    "                                            \n",
    "                                        # avoid duplicate entries\n",
    "                                        found = False\n",
    "                                        for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                                            if person[0] == name:\n",
    "                                                found = True\n",
    "                                                break  # quit looking for the person\n",
    "                                        if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                                            accumulationTable.append([name, '', '[]', category])\n",
    "        except:\n",
    "            pass\n",
    "    return accumulationTable\n",
    "\n",
    "def chemistry_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    # get the tables from the page\n",
    "    tableObjects = soupObject.find_all('table')\n",
    "    for tableObject in tableObjects:\n",
    "        pObjects = tableObject.find_all('p') # first two tables (primary and secondary appointments) have p elements\n",
    "        for p in pObjects:\n",
    "            if p.text.strip() != '':\n",
    "                name = p.text.strip()\n",
    "                # avoid duplicate entries\n",
    "                found = False\n",
    "                for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                    if person[0] == name:\n",
    "                        found = True\n",
    "                        break  # quit looking for the person\n",
    "                if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                    accumulationTable.append([name, '', '[]', category])\n",
    "        if len(pObjects) == 0: # last tables (non-tenure track) don't have p elements\n",
    "            try:\n",
    "                rowObjects = tableObject.find_all('tr') # last tables (non-tenure track) have tr elements\n",
    "                for rowObject in rowObjects:\n",
    "                    columnObjects = rowObject.find_all('td')\n",
    "                    if columnObjects[0].text.strip() != 'Name':\n",
    "                        name = columnObjects[0].text.strip()\n",
    "                        # avoid duplicate entries\n",
    "                        found = False\n",
    "                        for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                            if person[0] == name:\n",
    "                                found = True\n",
    "                                break  # quit looking for the person\n",
    "                        if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                            accumulationTable.append([name, '', '[]', category])\n",
    "            except:\n",
    "                pass\n",
    "    return accumulationTable\n",
    "        \n",
    "def comsci_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    divObjects = soupObject.find_all('div')\n",
    "    for div in divObjects:\n",
    "        try:\n",
    "            if div.get('class')[0] == 'panel-body':\n",
    "                pObjects = div.find_all('a')\n",
    "                for p in pObjects:\n",
    "                    name = p.text.strip()\n",
    "                    # avoid duplicate entries\n",
    "                    found = False\n",
    "                    for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                        if person[0] == name:\n",
    "                            found = True\n",
    "                            break  # quit looking for the person\n",
    "                    if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                        accumulationTable.append([name, '', '[]', category])\n",
    "        except:\n",
    "            pass\n",
    "    return accumulationTable\n",
    "\n",
    "def communication_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    articleObjects = soupObject.find_all('article')\n",
    "    for article in articleObjects:\n",
    "        try:\n",
    "            if article.get('class')[0] == 'primary-content':\n",
    "                divObjects = article.find_all('div')\n",
    "                for divObject in divObjects:\n",
    "                    try:\n",
    "                        if divObject.get('class')[1] == 'four_fifth':\n",
    "                            aObjects = divObject.find_all('a')\n",
    "                            if aObjects[0].text.strip() != 'Stephanie Covington':\n",
    "                                name = aObjects[0].text.strip()\n",
    "                                # avoid duplicate entries\n",
    "                                found = False\n",
    "                                for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                                    if person[0] == name:\n",
    "                                        found = True\n",
    "                                        break  # quit looking for the person\n",
    "                                if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                                    accumulationTable.append([name, '', '[]', category])\n",
    "                    except:\n",
    "                        pass\n",
    "        except:\n",
    "            pass\n",
    "    return accumulationTable\n",
    "\n",
    "def europeanstudies_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    articleObjects = soupObject.find_all('article')\n",
    "    for article in articleObjects:\n",
    "        if article.get('class')[0] == 'primary-content':\n",
    "            strongObjects = article.find_all('strong')\n",
    "            for strongObject in strongObjects:\n",
    "                name = strongObject.text.strip()\n",
    "                if name[-1] == ',':\n",
    "                    name = name[0:len(name)-1]\n",
    "                accumulationTable.append([name, '', '[]', category])\n",
    "    return accumulationTable\n",
    "\n",
    "def  frit_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    articleObjects = soupObject.find_all('article')\n",
    "    for article in articleObjects:\n",
    "        if article.get('class')[0] == 'primary-content':\n",
    "            pObjects = article.find_all('p')\n",
    "            for pObject in pObjects:\n",
    "                if '<a ' in str(pObject):\n",
    "                    if '<strong>' in str(pObject):\n",
    "                        aObjects = pObject.find_all('a')\n",
    "                        for aObject in aObjects:\n",
    "                            textBlob = aObject.text.strip()\n",
    "                            if textBlob != 'email':\n",
    "                                name = textBlob\n",
    "\n",
    "                                # avoid duplicate entries\n",
    "                                found = False\n",
    "                                for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                                    if person[0] == name:\n",
    "                                        found = True\n",
    "                                        break  # quit looking for the person\n",
    "                                if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                                    accumulationTable.append([name, '', '[]', category])\n",
    "    return accumulationTable\n",
    "\n",
    "def  historyart_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    articleObjects = soupObject.find_all('article')\n",
    "    for article in articleObjects:\n",
    "        if article.get('class')[0] == 'primary-content':\n",
    "            pObjects = article.find_all('p')\n",
    "            for pObject in pObjects:\n",
    "                aObjects = pObject.find_all('strong')\n",
    "                for aObject in aObjects:\n",
    "                    textBlob = aObject.text.strip()\n",
    "                    if textBlob !='EMERITI':\n",
    "                        name = textBlob\n",
    "                        \n",
    "                        # avoid duplicate entries\n",
    "                        found = False\n",
    "                        for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                            if person[0] == name:\n",
    "                                found = True\n",
    "                                break  # quit looking for the person\n",
    "                        if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                            accumulationTable.append([name, '', '[]', category])\n",
    "    return accumulationTable\n",
    "\n",
    "outputTable = [['name', 'degree', 'role', 'category']]\n",
    "categories = deptSettings[deptShortName]['categories']\n",
    "\n",
    "acceptMediaType = 'text/html'\n",
    "for category in categories:\n",
    "    url = deptSettings[deptShortName]['baseUrl'] + category\n",
    "    response = requests.get(url, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    soupObject = BeautifulSoup(response.text,features=\"html5lib\")\n",
    "    scrapeType = deptSettings[deptShortName]['scrapeType']\n",
    "    if scrapeType == 1:\n",
    "        temp = bsci_type_scrape(soupObject, category)\n",
    "    elif scrapeType == 2:\n",
    "        temp = cineart_type_scrape(soupObject, category)\n",
    "    elif scrapeType == 3:\n",
    "        temp = amstudies_scrape(soupObject)\n",
    "    elif scrapeType == 4:\n",
    "        temp = art_scrape(soupObject, category)\n",
    "    elif scrapeType == 5:\n",
    "        temp = asian_studies_scrape(soupObject, category)\n",
    "    elif scrapeType == 6:\n",
    "        temp = chemistry_scrape(soupObject, category)\n",
    "    elif scrapeType == 7:\n",
    "        temp = comsci_scrape(soupObject, category)\n",
    "    elif scrapeType == 8:\n",
    "        temp = communication_scrape(soupObject, category)\n",
    "    elif scrapeType == 9:\n",
    "        temp = europeanstudies_scrape(soupObject, category)\n",
    "    elif scrapeType == 10:\n",
    "        temp = frit_scrape(soupObject, category)\n",
    "    elif scrapeType == 11:\n",
    "        temp = historyart_scrape(soupObject, category)\n",
    "    outputTable += temp\n",
    "\n",
    "fileName = deptShortName + '-employees.csv'\n",
    "writeCsv(fileName, outputTable)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match departmental people with ORCID results\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/orcid/match_bsci_orcid.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = deptShortName + '-employees.csv'\n",
    "employees = readDict(filename)\n",
    "\n",
    "filename = 'orcid_data.csv'\n",
    "orcidData = readDict(filename)\n",
    "\n",
    "testRatio = 90\n",
    "departmentTestRatio = 90\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for row in orcidData:\n",
    "        name = row['givenNames'] + ' ' + row['familyName']\n",
    "        #ratio = fuzz.ratio(name, employees[employeeIndex][0])\n",
    "        #partialRatio = fuzz.partial_ratio(name, employees[employeeIndex][0])\n",
    "        #sortRatio = fuzz.token_sort_ratio(name, employees[employeeIndex][0])\n",
    "        \n",
    "        output = ''\n",
    "        # the set ratio seems to do the best job of matching\n",
    "        setRatio = fuzz.token_set_ratio(name, employees[employeeIndex]['name'])\n",
    "        if setRatio >= testRatio:\n",
    "            output = str(setRatio) + ' ' + name + ' / ' + employees[employeeIndex]['name']\n",
    "            \n",
    "            if row['department'] == '':\n",
    "                output += \" WARNING: no department given in ORCID\"\n",
    "            else:\n",
    "                # carry out a secondary test to see if any of the departments listed in the department's web page\n",
    "                # are a good match to the department given in the ORCID record\n",
    "                \n",
    "                # expand the role JSON into a list of dictionaries\n",
    "                print(name)\n",
    "                print(employees[employeeIndex]['role'])\n",
    "                roleDict = json.loads(employees[employeeIndex]['role'])\n",
    "                departmentMatch = False\n",
    "                for department in roleDict:\n",
    "                    setRatio = fuzz.token_set_ratio(deptSettings[deptShortName]['departmentSearchString'], row['department'])\n",
    "                    if setRatio > departmentTestRatio:\n",
    "                        departmentMatch = True\n",
    "                        output += ' ' + str(setRatio) + ' ' + row['department']\n",
    "                if not departmentMatch:\n",
    "                    output += ' WARNING: ' + row['department'] + ' less than ' + str(departmentTestRatio) + '% match to any dept.'\n",
    "            print(output)\n",
    "            matched = True\n",
    "            foundOrcid = row['orcid']\n",
    "            # We only care about the first good match to an ORCID record, kill the loop after that\n",
    "            break\n",
    "    if matched:\n",
    "        employees[employeeIndex]['orcid'] = foundOrcid\n",
    "    else:\n",
    "        employees[employeeIndex]['orcid'] = ''\n",
    "        \n",
    "filename = deptShortName + '-employees-with-orcid.csv'\n",
    "fieldnames = ['name', 'degree', 'category', 'orcid', 'role']\n",
    "writeDictsToCsv(employees, filename, fieldnames)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Vanderbilt people data from Wikidata\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/download-vanderbilt-people.py\n",
    "\n",
    "Performs a SPARQL query to find people in Wikidata that are employed at Vanderbilt.  Returns name, description, start date, end date, and ORCID if it has them.  Output to vanderbilt_wikidata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''select distinct  ?person ?name ?orcid ?startDate ?endDate ?description where {\n",
    "  ?person p:P108 ?statement.\n",
    "  ?statement ps:P108  wd:Q29052.\n",
    "  optional{\n",
    "    ?person rdfs:label ?name.\n",
    "    FILTER(lang(?name)=\"en\")\n",
    "    }\n",
    "  optional{?statement pq:P580 ?startDate.}\n",
    "  optional{?statement pq:P582 ?endDate.}\n",
    "  optional{?person wdt:P496 ?orcid.}\n",
    "  optional{\n",
    "    ?person schema:description ?description.\n",
    "    FILTER(lang(?description)=\"en\")\n",
    "          }\n",
    "  }'''\n",
    "\n",
    "# The endpoint defaults to returning XML, so the Accept: header is required\n",
    "r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers={'Accept' : 'application/json'})\n",
    "\n",
    "data = r.json()\n",
    "print(json.dumps(data,indent = 2))\n",
    "\n",
    "table = [['wikidataIri', 'name', 'description', 'startDate', 'endDate', 'orcid']]\n",
    "items = data['results']['bindings']\n",
    "for item in items:\n",
    "    wikidataIri = item['person']['value']\n",
    "    name = ''\n",
    "    if 'name' in item:\n",
    "        name = item['name']['value']\n",
    "    description = ''\n",
    "    if 'description' in item:\n",
    "        description = item['description']['value']\n",
    "    startDate = ''\n",
    "    if 'startDate' in item:\n",
    "        startDate = item['startDate']['value']\n",
    "    endDate = ''\n",
    "    if 'endDate' in item:\n",
    "        endDate = item['endDate']['value']\n",
    "    orcid = ''\n",
    "    if 'orcid' in item:\n",
    "        orcid = item['orcid']['value']\n",
    "    table.append([wikidataIri, name, description, startDate, endDate, orcid])\n",
    "    \n",
    "fileName = 'vanderbilt_wikidata.csv'\n",
    "writeCsv(fileName, table)\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Vanderbilt people's altLabels from Wikidata\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/download-vanderbilt-people-altlabels.py\n",
    "\n",
    "Similar to previous query, except when there is a match, it downloads the altLabels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''select distinct  ?person ?altLabel where {\n",
    "  ?person p:P108 ?statement.\n",
    "  ?statement ps:P108  wd:Q29052.\n",
    "  ?person skos:altLabel ?altLabel.\n",
    "  FILTER(lang(?altLabel)=\"en\")\n",
    "}'''\n",
    "\n",
    "# The endpoint defaults to returning XML, so the Accept: header is required\n",
    "r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers={'Accept' : 'application/json'})\n",
    "\n",
    "data = r.json()\n",
    "print(json.dumps(data,indent = 2))\n",
    "\n",
    "table = [['wikidataIri', 'altLabel']]\n",
    "items = data['results']['bindings']\n",
    "for item in items:\n",
    "    wikidataIri = item['person']['value']\n",
    "    altLabel = ''\n",
    "    if 'altLabel' in item:\n",
    "        altLabel = item['altLabel']['value']\n",
    "    table.append([wikidataIri, altLabel])\n",
    "    \n",
    "fileName = 'vanderbilt_wikidata_altlabels.csv'\n",
    "writeCsv(fileName, table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match employees to Wikidata\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/match_bsci_wikidata.ipynb\n",
    "\n",
    "Attempts to match records of people Wikidata knows to work at Vanderbilt with departmental people by matching their ORCIDs, then name strings. If there isn't a match with the downloaded Wikidata records, for employees with ORCIDs, the script attempts to find them in Wikidata by directly doing a SPARQL search for their ORCID.\n",
    "\n",
    "As people are matched (or determined to not have a match), a code is recorded with information about how the match was made.  Here are the values:\n",
    "\n",
    "```\n",
    "0=unmatched\n",
    "1=matched with ORCID in both sources\n",
    "2=ORCID from match to ORCID records but name match to Wikidata (no ORCID)\n",
    "3=no ORCID from match to ORCID records but name match to Wikidata (with ORCID); could happen if affiliation isn't matched in ORCID\n",
    "4=no ORCID from match to ORCID records but name match to Wikidata (no ORCID)\n",
    "5=ORCID from match to ORCID records and found via SPARQL ORCID search (likely non-VU affiliated in Wikidata)\n",
    "6=ORCID from match to ORCID records and found via SPARQL name search (non-VU affiliated without ORCID)\n",
    "7=no name match\n",
    "8=ORCID from match to ORCID records, error in SPARQL ORCID search\n",
    "9=no ORCID from match to ORCID records, error in SPARQL name search\n",
    "10=affiliation match in article\n",
    "11=match by human choice after looking at entity data\n",
    "12=no matching entities were possible matches\n",
    "13=match pre-existing Wikidata entry from another department\n",
    "```\n",
    "\n",
    "## Download the labels and descriptions of all existing institutional people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_label_query = Query(labelscreen='?id wdt:P1416 ?deptOrCollege.?deptOrCollege wdt:P749+ wd:' + employerQId + '.')\n",
    "org_labels = org_label_query.labels_descriptions('')\n",
    "print(len(org_labels))\n",
    "\n",
    "org_description_query = Query(labeltype='description', labelscreen='?id wdt:P1416 ?deptOrCollege.?deptOrCollege wdt:P749+ wd:' + employerQId + '.')\n",
    "org_descriptions = org_description_query.labels_descriptions('')\n",
    "print(len(org_descriptions))\n",
    "\n",
    "retrieve_orcid_query = Query(isitem=False, pid='P496')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now actually do the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = deptShortName + '-employees-with-orcid.csv'\n",
    "employees = readDict(filename)\n",
    "\n",
    "filename = 'vanderbilt_wikidata.csv'\n",
    "wikidataData = readDict(filename)\n",
    "\n",
    "testRatio = 90\n",
    "departmentTestRatio = 90\n",
    "\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "\n",
    "    matchStatus = 0\n",
    "\n",
    "    # first check to see if the name matches a previously uploaded person from another department\n",
    "    for labeldict in org_labels:\n",
    "        setRatio = fuzz.token_set_ratio(labeldict['string'], employees[employeeIndex]['name'])\n",
    "        if setRatio >= 82:  # this ratio was determined empirically to catch nearly all name variants without catching bad names\n",
    "            for descdict in org_descriptions:\n",
    "                if labeldict['qid'] == descdict['qid']:\n",
    "                    description = descdict['string']\n",
    "                    break\n",
    "            print('Likely match:', employees[employeeIndex]['name'], 'with', setRatio, labeldict['string'], '/', description, '/ https://www.wikidata.org/wiki/' + labeldict['qid'])\n",
    "            if setRatio < 90: # most false positives have match values < 90, so verify them\n",
    "                responseChoice = input('Press Enter to accept or enter anything else to reject')\n",
    "                if responseChoice != '':\n",
    "                    continue # give up on this potential match and move on to the next one\n",
    "            matchStatus = 13\n",
    "            employees[employeeIndex]['wikidataStatus'] = str(matchStatus)\n",
    "            employees[employeeIndex]['wikidataId'] = labeldict['qid']\n",
    "            possible_orcid = retrieve_orcid_query.single_property_values_for_item(labeldict['qid'])\n",
    "            if len(possible_orcid) == 1: # assign the ORCID value if there is exactly one\n",
    "                employees[employeeIndex]['orcid'] = possible_orcid[0]\n",
    "            if fuzz.ratio(labeldict['string'], employees[employeeIndex]['name']) < 75 :\n",
    "                print('WARNING: Check for a name reversal with', employees[employeeIndex]['name'])\n",
    "            break # quit the rest of the looping\n",
    "    if matchStatus == 13:\n",
    "        continue # do not need to perform the rest of the checks\n",
    "   \n",
    "    for row in wikidataData:\n",
    "        # We know the employee has an ORCID, so try to match it\n",
    "        if employees[employeeIndex]['orcid'] != '':\n",
    "            # There's a match, hooray!\n",
    "            if employees[employeeIndex]['orcid'] == row['orcid']:\n",
    "                print('orcid match: ', row['name'] + ' ' + row['orcid'])\n",
    "                matchStatus = 1\n",
    "                employees[employeeIndex]['wikidataId'] = extractQNumber(row['wikidataIri'])\n",
    "            # No ORCID match - see if the name matches\n",
    "            else:\n",
    "                # NOTE: There was a case where \"Morgan Daniels\" had a high match to \"Daniel Morgan\"\n",
    "                # based on the fuzz token set ratio I'm using. \n",
    "                # I've added a test for the regular fuzz ratio to try to detect name reversals.\n",
    "                \n",
    "                # avoid cases where there is a name match with a person known to be wrong because the ORCID doesn't match\n",
    "                if row['orcid'] == '': # if the employee has an ORCID it should have already matched if the wikidata person has one\n",
    "                    setRatio = fuzz.token_set_ratio(row['name'], employees[employeeIndex]['name'])\n",
    "                    if setRatio >= testRatio:\n",
    "                        if fuzz.ratio(row['name'], employees[employeeIndex]['name']) < 75 :\n",
    "                            print('WARNING: Check for a name reversal')\n",
    "\n",
    "                        print('name match: ', str(setRatio) + ' ' + row['name'] + ' / ' + employees[employeeIndex]['name'] + ' WD description: ' + row['description'])\n",
    "                        matchStatus = 2\n",
    "                        employees[employeeIndex]['wikidataId'] = extractQNumber(row['wikidataIri'])\n",
    "        # As far as we know, the employee doesn't have an ORCID, so try to match the name\n",
    "        else:\n",
    "            setRatio = fuzz.token_set_ratio(row['name'], employees[employeeIndex]['name'])\n",
    "            # We get a name match \n",
    "            if setRatio >= testRatio:\n",
    "                if fuzz.ratio(row['name'], employees[employeeIndex]['name']) < 75 :\n",
    "                    print('WARNING: Check for a name reversal')\n",
    "\n",
    "                # For some reason, Wikidata has the ORCID, so grab it\n",
    "                if row['orcid'] != '':\n",
    "                    print('name match: ', str(setRatio) + ' ' + row['name'] + ' / ' + employees[employeeIndex]['name'] + ' ORCID:' + row['orcid'])\n",
    "                    employees[employeeIndex]['orcid'] = row['orcid']\n",
    "                    matchStatus = 3\n",
    "                # Wikidata doesn't have an ORCID\n",
    "                else:\n",
    "                    print('name match: ', str(setRatio) + ' ' + row['name'] + ' / ' + employees[employeeIndex]['name'] + ' WD description: ' + row['description'])\n",
    "                    matchStatus = 4\n",
    "                employees[employeeIndex]['wikidataId'] = extractQNumber(row['wikidataIri'])\n",
    "        # We've gone all the way through the without finding a match\n",
    "\n",
    "    # Do a last ditch attempt to try to find the person in Wikidata by doing a SPARQL search for their ORCID\n",
    "    if matchStatus == 0:\n",
    "        if employees[employeeIndex]['orcid'] != '':\n",
    "            results = searchWikidataForQIdByOrcid(employees[employeeIndex]['orcid'])\n",
    "            if len(results) > 0:\n",
    "                print('SPARQL ORCID search: ', employees[employeeIndex]['name'], results)\n",
    "                if len(results) == 1:\n",
    "                    # if search fails and return an error message\n",
    "                    if len(results[0]) > 15:\n",
    "                        matchStatus = 8\n",
    "                        print('Error message in ORCID search')\n",
    "                    else:\n",
    "                        matchStatus = 5\n",
    "                        employees[employeeIndex]['wikidataId'] = results[0]\n",
    "                else:\n",
    "                    print('ERROR: multiple results for same ORCID')\n",
    "    # after every possible matching method, record the matchStatus\n",
    "    employees[employeeIndex]['wikidataStatus'] = str(matchStatus)\n",
    "#print(employees)\n",
    "\n",
    "filename = deptShortName + '-employees-with-wikidata.csv'\n",
    "fieldnames = ['wikidataId', 'name', 'degree', 'category', 'orcid', 'wikidataStatus', 'role']\n",
    "writeDictsToCsv(employees, filename, fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosscheck people against publications\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/crosscheck-publications.ipynb\n",
    "\n",
    "Checks possible Wikidata records against publications in CrossRef and PubMed to see if the author metadata will disambiguate the Wikidata record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptMediaType = 'application/json'\n",
    "requestHeaderDictionary = generateHeaderDictionary(acceptMediaType)\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        initials.append(piece[0:1])\n",
    "    \n",
    "    # NOTE: currently doesn't handle \", Jr.\", \"III\", etc.\n",
    "    \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsDict\n",
    "\n",
    "# returns a list of results of articles by person with Wikidata ID qId\n",
    "def searchWikidataArticle(qId):\n",
    "    resultsList = []\n",
    "    # P50 is \"author\"; P698 is the PubMed ID of the article; P356 is the DOI of the article\n",
    "    query = '''select distinct ?title ?doi ?pmid where {\n",
    "      ?article wdt:P50 wd:''' + qId + '''.\n",
    "      optional {\n",
    "          ?article rdfs:label ?title.\n",
    "          FILTER(lang(?title) = 'en')\n",
    "          }\n",
    "      optional {?article wdt:P698 ?pmid.}\n",
    "      optional {?article wdt:P356 ?doi.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            if 'title' in statement:\n",
    "                title = statement['title']['value']\n",
    "                #print('title=',title)\n",
    "            else:\n",
    "                title = ''\n",
    "            if 'pmid' in statement:\n",
    "                pmid = statement['pmid']['value']\n",
    "            else:\n",
    "                pmid = ''\n",
    "            if 'doi' in statement:\n",
    "                doi = statement['doi']['value']\n",
    "            else:\n",
    "                doi = ''\n",
    "            resultsList.append({'title': title, 'pmid': pmid, 'doi': doi})\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "def retrievePubMedData(pmid):\n",
    "    fetchUrl = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    paramDict = {\n",
    "        'tool': toolName, \n",
    "        'email': emailAddress,\n",
    "        'db': 'pubmed', \n",
    "         #'retmode': 'xml', \n",
    "        'rettype': 'abstract', \n",
    "        'id': pmid\n",
    "    }\n",
    "    response = requests.get(fetchUrl, params=paramDict)    \n",
    "    #print(response.url)\n",
    "    if response.status_code == 404:\n",
    "        affiliations = [] # return an empty list if the constructed URL won't dereference\n",
    "    else:\n",
    "        pubData = response.text  # the response text is XML\n",
    "        #print(pubData)  # uncomment this line to see the XML\n",
    "\n",
    "        # process the returned XML, see https://docs.python.org/2/library/xml.etree.elementtree.html\n",
    "        root = et.fromstring(pubData)\n",
    "        try:\n",
    "            title = root.findall('.//ArticleTitle')[0].text\n",
    "        except:\n",
    "            title = ''\n",
    "        names = root.findall('.//Author')\n",
    "        affiliations = []\n",
    "        for name in names:\n",
    "            try:\n",
    "                affiliation = name.find('./AffiliationInfo/Affiliation').text\n",
    "            except:\n",
    "                affiliation = ''\n",
    "            try:\n",
    "                lastName = name.find('./LastName').text\n",
    "            except:\n",
    "                lastName = ''\n",
    "            try:\n",
    "                foreName = name.find('./ForeName').text\n",
    "            except:\n",
    "                foreName = ''\n",
    "            try:\n",
    "                idField = name.find('./Identifier')\n",
    "                if idField.get('Source') == 'ORCID':\n",
    "                    orcid = idField.text\n",
    "                else:\n",
    "                    orcid = ''\n",
    "            except:\n",
    "                orcid = ''\n",
    "\n",
    "            #print(lastName)\n",
    "            #print(affiliation)\n",
    "            affiliations.append({'affiliation': affiliation, 'surname': lastName, 'forename': foreName, 'orcid': orcid})\n",
    "        #print()\n",
    "\n",
    "    # See https://www.ncbi.nlm.nih.gov/books/NBK25497/ for usage guidelines. \n",
    "    # An API key is required for more than 3 requests per second.\n",
    "    sleep(0.5) # wait half a second before hitting the API again to avoid getting blocked\n",
    "    return affiliations\n",
    "\n",
    "def retrieveCrossRefDoi(doi):\n",
    "    authorList = []\n",
    "    crossRefEndpointUrl = 'https://api.crossref.org/works/'\n",
    "    encodedDoi = urllib.parse.quote(doi)\n",
    "    searchUrl = crossRefEndpointUrl + encodedDoi\n",
    "    acceptMediaType = 'application/json'\n",
    "    response = requests.get(searchUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    if response.status_code == 404:\n",
    "        authorList = [] # return an empty list if the DOI won't dereference at CrossRef\n",
    "    else:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            #print(json.dumps(data, indent = 2))\n",
    "            if 'author' in data['message']:\n",
    "                authors = data['message']['author']\n",
    "                for author in authors:\n",
    "                    authorDict = {}\n",
    "                    if 'ORCID' in author:\n",
    "                        authorDict['orcid'] = author['ORCID']\n",
    "                    else:\n",
    "                        authorDict['orcid'] = ''\n",
    "                    if 'given' in author:\n",
    "                        authorDict['givenName'] = author['given']\n",
    "                    else:\n",
    "                        authorDict['givenName'] = ''\n",
    "                    if 'family' in author:\n",
    "                        authorDict['familyName'] = author['family']\n",
    "                    else:\n",
    "                        authorDict['familyName'] = ''\n",
    "                    affiliationList = []\n",
    "                    if 'affiliation' in author:\n",
    "                        for affiliation in author['affiliation']:\n",
    "                            affiliationList.append(affiliation['name'])\n",
    "                    # if there aren't any affiliations, the list will remain empty\n",
    "                    authorDict['affiliation'] = affiliationList\n",
    "                    authorList.append(authorDict)\n",
    "        except:\n",
    "            authorList = [data]\n",
    "    return authorList\n",
    "\n",
    "# ***** BODY OF SEARCH\n",
    "filename = deptShortName + '-employees-with-wikidata.csv'\n",
    "employees = readDict(filename)\n",
    "\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "#for employeeIndex in range(0, 1): # just do one person for testing\n",
    "    # perform search only for people who weren't already matched\n",
    "    if employees[employeeIndex]['wikidataStatus'] == '0':\n",
    "        matchStatus = 0\n",
    "        print('--------------------------')\n",
    "        results = searchNameAtWikidata(employees[employeeIndex]['name'])\n",
    "        if len(results) == 0:\n",
    "            print('No Wikidata name match: ', employees[employeeIndex]['name'])\n",
    "            matchStatus = 7\n",
    "            print()\n",
    "        else:\n",
    "            print('SPARQL name search: ', employees[employeeIndex]['name'])\n",
    "            if len(results) == 1:\n",
    "                if 'error' in results[0]:\n",
    "                    matchStatus = 9\n",
    "                    print('Error message in name search:', results[0]['error'])\n",
    "                    break # discontinue processing this person\n",
    "\n",
    "            qIds = []\n",
    "            nameVariants = []\n",
    "            potentialOrcid = []\n",
    "            for result in results:\n",
    "                qIds.append(result['qId'])\n",
    "                nameVariants.append(result['name'])\n",
    "            \n",
    "            testAuthor = employees[employeeIndex]['name']\n",
    "            testOrcid = employees[employeeIndex]['orcid']\n",
    "\n",
    "            if testOrcid == '':\n",
    "                print('(no ORCID)')\n",
    "            else:\n",
    "                print('ORCID: ', testOrcid)\n",
    "            print()\n",
    "            \n",
    "            foundMatch = False # start the flag with the person not being matched\n",
    "            possibleMatch = False # start the flag with there not being a possibility that the person could match\n",
    "            for qIdIndex in range(0, len(qIds)):\n",
    "                potentialOrcid.append('') # default to no ORCID found for that person\n",
    "                print()\n",
    "                print(qIdIndex, 'Wikidata ID: ', qIds[qIdIndex], ' Name variant: ', nameVariants[qIdIndex], ' ', 'https://www.wikidata.org/wiki/' + qIds[qIdIndex])\n",
    "                wdClassList = searchWikidataSingleProperty(qIds[qIdIndex], 'P31', 'item')\n",
    "                # if there is a class property, check if it's a human\n",
    "                if len(wdClassList) != 0:\n",
    "                    # if it's not a human\n",
    "                    if wdClassList[0] != 'Q5':\n",
    "                        print('This item is not a human!')\n",
    "                        break\n",
    "                        \n",
    "                # check for a death date\n",
    "                deathDateList = searchWikidataSingleProperty(qIds[qIdIndex], 'P570', 'string')\n",
    "                if len(deathDateList) == 0:\n",
    "                    print('No death date given.')\n",
    "                else:\n",
    "                    deathDate = deathDateList[0][0:10] # all dates are converted to xsd:dateTime and will have a y-m-d date\n",
    "                    if deathDate < deathDateLimit:\n",
    "                        # if the person died a long time ago, don't retrieve other stuff\n",
    "                        print('This person died in ', deathDate)\n",
    "                        break\n",
    "                    else:\n",
    "                        # if the person died recently, we still might be interested in them so keep going\n",
    "                        print('This person died in ', deathDate)\n",
    "\n",
    "                descriptors = searchWikidataDescription(qIds[qIdIndex])\n",
    "                employers = searchWikidataEmployer(qIds[qIdIndex])\n",
    "                #print(descriptors)\n",
    "                if descriptors != {}:\n",
    "                    if descriptors['description'] != '':\n",
    "                        print('description: ', descriptors['description'])\n",
    "                    for occupation in descriptors['occupation']:\n",
    "                        print('occupation: ', occupation)\n",
    "                    for employer in employers:\n",
    "                        print('employer: ', employer)\n",
    "                    if descriptors['orcid'] != '':\n",
    "                        if testOrcid == '':\n",
    "                            # **** NOTE: if the person has an ORCID, it may be possible to find articles via ORCID\n",
    "                            # that aren't linked in Wikidata. Not sure if this happens often enough to handle it\n",
    "                            print('ORCID: ', descriptors['orcid'])\n",
    "                            potentialOrcid[qIdIndex] = descriptors['orcid']\n",
    "                        else:\n",
    "                            # This should always be true if the SPARQL query for ORCID was already done\n",
    "                            if testOrcid != descriptors['orcid']:\n",
    "                                print('*** NOT the same person; ORCID ' + descriptors['orcid'] + ' does not match.')\n",
    "                                break # don't continue the loop (look up references) since it's definitely not a match\n",
    "                            else:\n",
    "                                print('*** An ORCID match! How did it get missed in the earlier SPARQL query?')\n",
    "                                break\n",
    "                else:\n",
    "                    print('No description or occupation given.')\n",
    "\n",
    "                result = searchWikidataArticle(qIds[qIdIndex])\n",
    "                if len(result) == 0:\n",
    "                    print('No articles authored by that person')\n",
    "                else:\n",
    "                    articleCount = 0\n",
    "                    for article in result:\n",
    "                        print()\n",
    "                        print('Checking article: ', article['title'])\n",
    "                        if article['pmid'] == '':\n",
    "                            print('No PubMed ID')\n",
    "                        else:\n",
    "                            print('Checking authors in PubMed article: ', article['pmid'])\n",
    "                            pubMedAuthors = retrievePubMedData(article['pmid'])\n",
    "                            if pubMedAuthors == []:\n",
    "                                print('PubMed ID does not seem to be valid.')\n",
    "                            #print(pubMedAuthors)\n",
    "                            for author in pubMedAuthors:\n",
    "                                nameTestRatio = fuzz.token_set_ratio(author['surname'], testAuthor)\n",
    "                                #print(nameTestRatio, author['surname'])\n",
    "                                if nameTestRatio >= 90:\n",
    "                                    # if the PubMed metadata gives an ORCID for the matched person, record it unless \n",
    "                                    # the ORCID has already been gotten from the Wikidata record\n",
    "                                    if author['orcid'] != '':\n",
    "                                        if testOrcid == '':\n",
    "                                            print('ORCID from article: ', author['orcid'])\n",
    "                                            if potentialOrcid[qIdIndex] == '':\n",
    "                                                potentialOrcid[qIdIndex] = author['orcid']\n",
    "                                        else:\n",
    "                                            if testOrcid != author['orcid']:\n",
    "                                                print('*** NOT the same person; ORCID ' + author['orcid'] + ' does not match.')\n",
    "                                                break # don't continue the loop (look up authors) since it's definitely not a match\n",
    "                                            else:\n",
    "                                                print('*** An ORCID match!')\n",
    "                                                foundMatch = True\n",
    "                                                matchStatus = 6\n",
    "                                                break # don't continue the loop (look up authors) since it's an ORCID match\n",
    "\n",
    "                                    if author['affiliation'] != '': \n",
    "                                        setRatio = fuzz.token_set_ratio(deptSettings[deptShortName]['testAuthorAffiliation'], author['affiliation'])\n",
    "                                        print('Affiliation test: ', setRatio, author['affiliation'])\n",
    "                                        if setRatio >= 90:\n",
    "                                            foundMatch = True\n",
    "                                            matchStatus = 10\n",
    "                                            break # don't continue the loop (look up authors) since it's an affiliation match\n",
    "                                    else:\n",
    "                                        break # give up on this article because no affiliation string\n",
    "                        # Don't look up the DOI if it's already found a match with PubMed\n",
    "                        if foundMatch:\n",
    "                            break # stop checking articles after a PubMed one has matched\n",
    "                        else:\n",
    "                            if article['doi'] == '':\n",
    "                                print('No DOI')\n",
    "                            else:\n",
    "                                print('Checking authors in DOI article: ', article['doi'])\n",
    "                                doiAuthors = retrieveCrossRefDoi(article['doi'])\n",
    "                                if doiAuthors == []:\n",
    "                                    print('DOI does not dereference at CrossRef')\n",
    "                                for author in doiAuthors:\n",
    "                                    nameTestRatio = fuzz.token_set_ratio(author['familyName'], testAuthor)\n",
    "                                    #print(nameTestRatio, author['familyName'])\n",
    "                                    if nameTestRatio >= 90:\n",
    "                                        if author['orcid'] != '':\n",
    "                                            if testOrcid == '':\n",
    "                                                # DOI records the entire ORCID URI, not just the ID number\n",
    "                                                # so pull the last 19 characters from the string\n",
    "                                                print('ORCID from article: ', author['orcid'][-19:])\n",
    "                                                # only add the ORCID from article if there isn't already one,\n",
    "                                                # for example, one gotten from the Wikidata record itself\n",
    "                                                if potentialOrcid[qIdIndex] == '':\n",
    "                                                    potentialOrcid[qIdIndex] = author['orcid'][-19:]\n",
    "                                            else:\n",
    "                                                if testOrcid != author['orcid']:\n",
    "                                                    print('*** NOT the same person; ORCID ' + author['orcid'] + ' does not match.')\n",
    "                                                    break # don't continue the loop (look up authors) since it's definitely not a match\n",
    "                                                else:\n",
    "                                                    print('*** An ORCID match!')\n",
    "                                                    foundMatch = True\n",
    "                                                    matchStatus = 6\n",
    "                                                    break # don't continue the loop (look up authors) since it's an ORCID match\n",
    "\n",
    "\n",
    "                                        if len(author['affiliation']) > 0:\n",
    "                                            for affiliation in author['affiliation']:\n",
    "                                                setRatio = fuzz.token_set_ratio(deptSettings[deptShortName]['testAuthorAffiliation'], affiliation)\n",
    "                                                print('Affiliation test: ', setRatio, affiliation)\n",
    "                                                if setRatio >= 90:\n",
    "                                                    foundMatch = True\n",
    "                                                    matchStatus = 10\n",
    "                                                    break # don't continue the loop (look up authors) since it's an affiliation match\n",
    "                                        else:\n",
    "                                            break # give up on this article because no affiliation string\n",
    "                            if foundMatch:\n",
    "                                break # stop checking articles after a DOI one has matched\n",
    "                        articleCount += 1\n",
    "                        if articleCount > 10:\n",
    "                            checkMore = input('There are more than 10 articles. Press Enter to skip the rest or enter anything to get the rest.')\n",
    "                            if checkMore == '':\n",
    "                                break\n",
    "                    if foundMatch:\n",
    "                        print('***', qIds[qIdIndex], ' is a match.')\n",
    "                        print()\n",
    "                        employees[employeeIndex]['wikidataId'] = qIds[qIdIndex]\n",
    "                        employees[employeeIndex]['orcid'] = potentialOrcid[qIdIndex]\n",
    "                        break # quit checking Q IDs since the person was matched\n",
    "                    else:\n",
    "                        print('No match found.')\n",
    "                print('Employee: ', employees[employeeIndex]['name'], ' vs. name variant: ', nameVariants[qIdIndex])\n",
    "                possibleMatch = True # made it all the way through the loop without hitting a break, so a match is possible\n",
    "                print()\n",
    "            if not foundMatch:\n",
    "                if not possibleMatch:\n",
    "                    matchStatus = 12\n",
    "                else:\n",
    "                    choiceString = input('Enter the number of the matched entity, or press Enter/return if none match: ')\n",
    "                    if choiceString == '':\n",
    "                        matchStatus = 7\n",
    "                    else:\n",
    "                        # NOTE: there is no error trapping here for mis-entry !!!\n",
    "                        choice = int(choiceString)\n",
    "                        matchStatus = 11\n",
    "                        employees[employeeIndex]['wikidataId'] = qIds[choice]\n",
    "                        # write a discovered ORCID only if the person didn't already have one\n",
    "                        if (potentialOrcid[choice] != '') and (employees[employeeIndex]['orcid'] == ''):\n",
    "                            employees[employeeIndex]['orcid'] = potentialOrcid[choice]\n",
    "                    print()\n",
    "                \n",
    "        # record the final match status\n",
    "        employees[employeeIndex]['wikidataStatus'] = str(matchStatus)\n",
    "    \n",
    "    # write the file after each person is checked in case the user crashes the script\n",
    "    filename = deptShortName + '-employees-curated.csv'\n",
    "    fieldnames = ['wikidataId', 'name', 'degree', 'category', 'orcid', 'wikidataStatus', 'role']\n",
    "    writeDictsToCsv(employees, filename, fieldnames)\n",
    "\n",
    "print()\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download various statements and references, then generate write file\n",
    "\n",
    "NOTE: between the previous step and this one, one can add a gender/sex column to the table that will be processed if it exists.  Column header: 'gender'.  Allowed values (from Wikidata): m=male, f=female, i=intersex, tf=transgender female, tm=transgender male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: if a statement has already been made and supported by a reference that is different from the reference\n",
    "# created by this script, the reference hash will be for the other reference and not the one created here. \n",
    "# So the stored value of the hash will not necessarily correspond to the adjacent reference in the table.\n",
    "# The script will have to be changed if a different behavior is desired (i.e. creating a second reference for the one statement.)\n",
    "\n",
    "filename = deptShortName + '-employees-curated.csv'\n",
    "employees = readDict(filename)\n",
    "\n",
    "# create a list of the employees who have Wikidata qIDs\n",
    "qIds = []\n",
    "for employee in employees:\n",
    "    if employee['wikidataId'] != '':\n",
    "        qIds.append(employee['wikidataId'])\n",
    "\n",
    "# get all of the ORCID data that is already in Wikidata\n",
    "prop = 'P496' # ORCID iD\n",
    "value = '' # since no value is passed, the search will retrieve the value\n",
    "refProps = ['P813'] # retrieved\n",
    "wikidataOrcidData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "#print(json.dumps(wikidataOrcidData, indent=2))\n",
    "\n",
    "# match people who have ORCIDs with ORCID data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataOrcidDataIndex in range(0, len(wikidataOrcidData)):\n",
    "        if wikidataOrcidData[wikidataOrcidDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            if employees[employeeIndex]['orcid'] != wikidataOrcidData[wikidataOrcidDataIndex]['statementValue']:\n",
    "                print('Non-matching ORCID for ', employees[employeeIndex]['name'])\n",
    "            # if there is a match, record whatever data was retrieved\n",
    "            else:\n",
    "                employees[employeeIndex]['orcidStatementUuid'] = wikidataOrcidData[wikidataOrcidDataIndex]['statementUuid']\n",
    "                employees[employeeIndex]['orcidReferenceHash'] = wikidataOrcidData[wikidataOrcidDataIndex]['referenceHash']\n",
    "                # if there is no referenceHash then try to dereference the ORCID\n",
    "                if employees[employeeIndex]['orcidReferenceHash']== '':\n",
    "                    # if there is a match, check whether the ORCID record can be retrieved\n",
    "                    print('Checking ORCID for Wikidata matched: ', employees[employeeIndex]['name'])\n",
    "                    # returned value is the current date if successful; empty string if not\n",
    "                    employees[employeeIndex]['orcidReferenceValue'] = checkOrcid(employees[employeeIndex]['orcid'])\n",
    "                # if there is an existing reference, record the value for the first reference property (only one ref property)\n",
    "                else:\n",
    "                    print('Already an ORCID reference for: ', employees[employeeIndex]['name'])\n",
    "                    # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                    employees[employeeIndex]['orcidReferenceValue'] = '+' + wikidataOrcidData[wikidataOrcidDataIndex]['referenceValues'][0]\n",
    "            # stop checking at the first match.\n",
    "            break\n",
    "    # if the person doesn't match with those whose ORCIDs came back from the query...\n",
    "    if not matched:\n",
    "        # check for access if they have an ORCID (not present in Wikidata)\n",
    "        if employees[employeeIndex]['orcid'] != '':\n",
    "            print('Checking ORCID for unmatched: ', employees[employeeIndex]['name'])\n",
    "            # the function returns the current date (to use as the retrieved date) if the ORCID is found, otherwise empty string\n",
    "            employees[employeeIndex]['orcidReferenceValue'] = checkOrcid(employees[employeeIndex]['orcid'])\n",
    "\n",
    "# get data already in Wikidata about people employed at Vanderbilt\n",
    "prop = 'P108' # employer\n",
    "refProps = ['P854', 'P813'] # source URL, retrieved\n",
    "wikidataEmployerData = searchStatementAtWikidata(qIds, prop, employerQId, refProps)\n",
    "#print(json.dumps(wikidataEmployerData, indent=2))\n",
    "\n",
    "# match people with employment data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matchedStatement = False\n",
    "    matchedReference = False\n",
    "    for wikidataEmployerDataIndex in range(0, len(wikidataEmployerData)):\n",
    "        if wikidataEmployerData[wikidataEmployerDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matchedStatement = True\n",
    "            employees[employeeIndex]['employerStatementUuid'] = wikidataEmployerData[wikidataEmployerDataIndex]['statementUuid']\n",
    "            employees[employeeIndex]['employerReferenceHash'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceHash']\n",
    "            # if there is a referenceHash then record the values for the two reference properties: P813, P854'; retrieved, source URL\n",
    "            if employees[employeeIndex]['employerReferenceHash']!= '':\n",
    "                # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                employees[employeeIndex]['employerReferenceSourceUrl'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0]\n",
    "                if wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0] == deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']:\n",
    "                    matchedReference = True\n",
    "                employees[employeeIndex]['employerReferenceRetrieved'] = '+' + wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][1]\n",
    "            # stop checking if there is an exact match to the reference URL. Otherwise keep looping.\n",
    "            # if there is a later reference that matches ours, it will overwrite any previous reference data\n",
    "            # otherwise, the existing (different) reference data will be retained\n",
    "            if matchedReference:\n",
    "                break\n",
    "    # everyone is assigned the employerQId as a value because either they showed up in the SPARQL search for employerQId\n",
    "    # or we are making a statement that they work for employerQId.\n",
    "    employees[employeeIndex]['employer'] = employerQId\n",
    "    if not matchedReference:  # generate the reference metadata if the reference URL wasn't found\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "        employees[employeeIndex]['employerReferenceSourceUrl'] = deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']\n",
    "        employees[employeeIndex]['employerReferenceRetrieved'] = wholeDateZ\n",
    "\n",
    "# *** This is a copy and paste of the employer section above, modified for affiliation\n",
    "\n",
    "# get data already in Wikidata about people affiliated with the department\n",
    "prop = 'P1416' # affiliation\n",
    "refProps = ['P854', 'P813'] # source URL, retrieved\n",
    "wikidataEmployerData = searchStatementAtWikidata(qIds, prop, deptSettings[deptShortName]['departmentQId'], refProps)\n",
    "#print(json.dumps(wikidataEmployerData, indent=2))\n",
    "\n",
    "# match people with affiliation data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matchedStatement = False\n",
    "    matchedReference = False\n",
    "    for wikidataEmployerDataIndex in range(0, len(wikidataEmployerData)):\n",
    "        if wikidataEmployerData[wikidataEmployerDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matchedStatement = True\n",
    "            employees[employeeIndex]['affiliationStatementUuid'] = wikidataEmployerData[wikidataEmployerDataIndex]['statementUuid']\n",
    "            employees[employeeIndex]['affiliationReferenceHash'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceHash']\n",
    "            # if there is a referenceHash then record the values for the two reference properties: P813, P854'; retrieved, source URL\n",
    "            if employees[employeeIndex]['affiliationReferenceHash']!= '':\n",
    "                # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                employees[employeeIndex]['affiliationReferenceSourceUrl'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0]\n",
    "                if wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0] == deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']:\n",
    "                    matchedReference = True\n",
    "                employees[employeeIndex]['affiliationReferenceRetrieved'] = '+' + wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][1]\n",
    "            # stop checking if there is an exact match to the reference URL. Otherwise keep looping.\n",
    "            # if there is a later reference that matches ours, it will overwrited any previous reference data\n",
    "            # otherwise, the existing (different) reference data will be retained\n",
    "            if matchedReference:\n",
    "                break\n",
    "        \n",
    "    # everyone is assigned the department as a value because either they showed up in the SPARQL search\n",
    "    # or we are making a statement that they are affiliated with the department.\n",
    "    employees[employeeIndex]['affiliation'] = deptSettings[deptShortName]['departmentQId']\n",
    "    if not matchedReference:  # generate the reference metadata if the reference URL wasn't found\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "        employees[employeeIndex]['affiliationReferenceSourceUrl'] = deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']\n",
    "        employees[employeeIndex]['affiliationReferenceRetrieved'] = wholeDateZ\n",
    "\n",
    "# get all of the data that is already in Wikidata about who are humans\n",
    "prop = 'P31' # instance of\n",
    "value = 'Q5' # human\n",
    "refProps = [] # no ref property needed\n",
    "wikidataHumanData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "\n",
    "# Find out which people have assertions that they are humans and record their statement IDs.\n",
    "# Assign the properties to all others.\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    for wikidataHumanIndex in range(0, len(wikidataHumanData)):\n",
    "        if wikidataHumanData[wikidataHumanIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            employees[employeeIndex]['instanceOfUuid'] = wikidataHumanData[wikidataHumanIndex]['statementUuid']\n",
    "    # everybody is assigned a value of 'human'\n",
    "    employees[employeeIndex]['instanceOf'] = 'Q5'\n",
    "\n",
    "# hack of human code immediately above\n",
    "\n",
    "# get all of the data that is already in Wikidata about the sex or gender of the researchers\n",
    "prop = 'P21' # sex or gender\n",
    "value = '' # don't provide a value so that it will return whatever value it finds\n",
    "refProps = [] # no ref property needed\n",
    "wikidataHumanData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "\n",
    "# Find out which people have assertions of sex/gender and record their statement IDs.\n",
    "# Assign the value for the property to all others.\n",
    "# NOTE: Wikidata doesn't seem to care a lot about references for this property and we don't really have one anyway\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataHumanIndex in range(0, len(wikidataHumanData)):\n",
    "        if wikidataHumanData[wikidataHumanIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['sexOrGenderUuid'] = wikidataHumanData[wikidataHumanIndex]['statementUuid']\n",
    "            # use the value in Wikidata and ignore the value in the 'gender' column of the table.\n",
    "            # extractFromIri() function strips the namespace from the qId\n",
    "            employees[employeeIndex]['sexOrGenderQId'] = extractFromIri(wikidataHumanData[wikidataHumanIndex]['statementValue'], 4)\n",
    "    if not matched:\n",
    "        # assign the value from the 'gender' column in the table if not already in Wikidata\n",
    "        if 'gender' in employees[employeeIndex]:\n",
    "            employees[employeeIndex]['sexOrGenderQId'] = decodeSexOrGender(employees[employeeIndex]['gender'])\n",
    "        else:\n",
    "            employees[employeeIndex]['sexOrGenderQId'] = ''\n",
    "\n",
    "# get all of the English language labels for the employees that are already in Wikidata\n",
    "labelType = 'label'\n",
    "language = 'en'\n",
    "wikidataLabels = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "\n",
    "# Match people with their labels\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataLabelIndex in range(0, len(wikidataLabels)):\n",
    "        if wikidataLabels[wikidataLabelIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['labelEn'] = wikidataLabels[wikidataLabelIndex]['string']\n",
    "    if not matched:\n",
    "        # assign the value from the 'name' column in the table if not already in Wikidata\n",
    "        if deptSettings[deptShortName]['labels']['source'] == 'column':\n",
    "            # then use the value from the default label column.\n",
    "            defaultLabelColumn = deptSettings[deptShortName]['labels']['value']\n",
    "            employees[employeeIndex]['labelEn'] = employees[employeeIndex][defaultLabelColumn]\n",
    "        else:\n",
    "            # or use the default label value.\n",
    "            employees[employeeIndex]['labelEn'] = deptSettings[deptShortName]['labels']['value']\n",
    "\n",
    "# get all of the English language descriptions for the employees that are already in Wikidata\n",
    "labelType = 'description'\n",
    "language = 'en'\n",
    "wikidataDescriptions = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "\n",
    "# Match people with their descriptions\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataDescriptionIndex in range(0, len(wikidataDescriptions)):\n",
    "        if wikidataDescriptions[wikidataDescriptionIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['description'] = wikidataDescriptions[wikidataDescriptionIndex]['string']\n",
    "    if not matched:\n",
    "        # assign a default value if not already in Wikidata\n",
    "        if deptSettings[deptShortName]['descriptions']['source'] == 'column':\n",
    "            # then use the value from the default description column.\n",
    "            defaultDescriptionColumn = deptSettings[deptShortName]['descriptions']['value']\n",
    "            employees[employeeIndex]['description'] = employees[employeeIndex][defaultDescriptionColumn]\n",
    "        else:\n",
    "            # or use the default description value.\n",
    "            employees[employeeIndex]['description'] = deptSettings[deptShortName]['descriptions']['value']\n",
    "\n",
    "# Get all of the aliases already at Wikidata for employees.  \n",
    "# Since there can be multiple aliases, they are stored as a list structure.\n",
    "# The writing script can handle multiple languages, but here we are only dealing with English ones.\n",
    "\n",
    "# retrieve the aliases in that language that already exist in Wikidata and match them with table rows\n",
    "labelType = 'alias'\n",
    "language = 'en'\n",
    "aliasesAtWikidata = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "for entityIndex in range(0, len(employees)):\n",
    "    personAliasList = []\n",
    "    if employees[entityIndex]['wikidataId'] != '':  # don't look for the label at Wikidata if the item doesn't yet exist\n",
    "        for wikiLabel in aliasesAtWikidata:\n",
    "            if employees[entityIndex]['wikidataId'] == wikiLabel['qId']:\n",
    "                personAliasList.append(wikiLabel['string'])\n",
    "    # if not found, the personAliasList list will remain empty\n",
    "    employees[entityIndex]['alias'] = json.dumps(personAliasList)\n",
    "\n",
    "# set the departmental short name for all entities\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    employees[employeeIndex]['department'] = deptShortName\n",
    "\n",
    "# write the file\n",
    "filename = deptShortName + '-employees-to-write.csv'\n",
    "fieldnames = ['department', 'wikidataId', 'name', 'labelEn', 'alias', 'description', 'orcidStatementUuid', 'orcid', 'orcidReferenceHash', 'orcidReferenceValue', 'employerStatementUuid', 'employer', 'employerReferenceHash', 'employerReferenceSourceUrl', 'employerReferenceRetrieved', 'affiliationStatementUuid', 'affiliation', 'affiliationReferenceHash', 'affiliationReferenceSourceUrl', 'affiliationReferenceRetrieved', 'instanceOfUuid', 'instanceOf', 'sexOrGenderUuid', 'sexOrGenderQId', 'gender', 'degree', 'category', 'wikidataStatus', 'role']\n",
    "writeDictsToCsv(employees, filename, fieldnames)\n",
    "\n",
    "print()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set file name in CSV metadata file\n",
    "\n",
    "Prior to writing the data to Wikidata using the `process_csv_metadata_full.py` script, the input file name needs to be changed in the `csv-metadata.json` file to have the correct `deptShortName` for the department. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('csv-metadata.json', 'rt', encoding='utf-8') as inFileObject:\n",
    "    text = inFileObject.read()\n",
    "schema = json.loads(text)\n",
    "schema['tables'][0]['url'] = deptShortName + '-employees-to-write.csv'\n",
    "outText = json.dumps(schema, indent = 2)\n",
    "with open('csv-metadata.json', 'wt', encoding='utf-8') as outFileObject:\n",
    "    outFileObject.write(outText)\n",
    "print('Department to be written:', deptShortName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
