{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query ORCID for Vanderbilt University people\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/orcid/orcid-get-json.ipynb\n",
    "\n",
    "Retrieves results 100 at a time, then processes them by extracting desired information.  NOTE: takes hours to run.\n",
    "\n",
    "Saves results in a file and the alternative names in a second file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# function to write results to a file\n",
    "def writeCsv(fileName, array):\n",
    "    fileObject = open(fileName, 'w', newline='', encoding='utf-8')\n",
    "    writerObject = csv.writer(fileObject)\n",
    "    for row in array:\n",
    "        writerObject.writerow(row)\n",
    "    fileObject.close()\n",
    "    \n",
    "table = [['orcid', 'givenNames', 'familyName', 'startDate', 'endDate', 'department', 'organization']]\n",
    "otherNameList = [['orcid', 'altName']]\n",
    "\n",
    "# use the API to search for people associated with Vanderbilt University\n",
    "# First search is for only one record, just to get the number of hits found\n",
    "searchUri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start=1&rows=1'\n",
    "response = requests.get(searchUri, headers={'Accept' : 'application/json'})\n",
    "data = response.json()\n",
    "numberResults = data[\"num-found\"]\n",
    "#print(data[\"num-found\"])\n",
    "numberPages = math.floor(numberResults/100)\n",
    "#print(numberPages)\n",
    "remainder = numberResults - 100*numberPages\n",
    "#print(remainder)\n",
    "\n",
    "for pageCount in range(0, numberPages+1):  # the remainder will be caught when pageCount = numberPages\n",
    "    print('page: ', pageCount)\n",
    "    searchUri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start='+str(pageCount*100+1)\n",
    "    response = requests.get(searchUri, headers={'Accept' : 'application/json'})\n",
    "    print(response.url)\n",
    "    data = response.json()\n",
    "    orcidsDictsList = data['result']\n",
    "\n",
    "    # extract the identifier strings from the data structure\n",
    "    orcids = []\n",
    "    for orcidDict in orcidsDictsList:\n",
    "        dictionary = {'id': orcidDict['orcid-identifier']['path'], 'iri': orcidDict['orcid-identifier']['uri']}\n",
    "        orcids.append(dictionary)\n",
    "\n",
    "    for orchidIndex in range(0, len(orcids)):\n",
    "        response = requests.get(orcids[orchidIndex]['iri'], headers={'Accept' : 'application/json'})\n",
    "        data = response.json()\n",
    "\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        orcidId = data['orcid-identifier']['path']\n",
    "        #print(orcidId)\n",
    "        # if there isn't a name, then go on to the next ORCID\n",
    "        if not data['person']['name']:\n",
    "            continue\n",
    "        if data['person']['name']['given-names']:  \n",
    "            givenNames = data['person']['name']['given-names']['value']\n",
    "        else:\n",
    "            givenNames = ''\n",
    "        if data['person']['name']['family-name']:\n",
    "            familyName = data['person']['name']['family-name']['value']\n",
    "        else:\n",
    "            familyName = ''\n",
    "        #print(givenNames, ' ', familyName)\n",
    "        otherNames = data['person']['other-names']['other-name']\n",
    "        for otherName in otherNames:\n",
    "            #print(otherName['content'])\n",
    "            otherNameList.append([orcidId, otherName['content']])\n",
    "\n",
    "        affiliations = data['activities-summary']['employments']['affiliation-group']\n",
    "        #print(json.dumps(affiliations, indent = 2))\n",
    "        for affiliation in affiliations:\n",
    "            summaries = affiliation['summaries']\n",
    "            #print(summaries)\n",
    "            #print()\n",
    "            for summary in summaries:\n",
    "                employment = summary['employment-summary']\n",
    "                #print(json.dumps(employment, indent = 2))\n",
    "                startDate = ''\n",
    "                if employment['start-date']:\n",
    "                    if employment['start-date']['year']:\n",
    "                        startDate += employment['start-date']['year']['value']\n",
    "                        startMonth = employment['start-date']['month']\n",
    "                        if startMonth:\n",
    "                            startDate += '-' + startMonth['value']\n",
    "                            startDay = employment['start-date']['day']\n",
    "                            if startDay:\n",
    "                                startDate += '-' + startDay['value']\n",
    "                #print('start date: ', startDate)\n",
    "                endDate = ''\n",
    "                if employment['end-date']:\n",
    "                    if employment['end-date']['year']:\n",
    "                        endDate += employment['end-date']['year']['value']\n",
    "                        endMonth = employment['end-date']['month']\n",
    "                        if endMonth:\n",
    "                            endDate += '-' + endMonth['value']\n",
    "                            endDay = employment['end-date']['day']\n",
    "                            if endDay:\n",
    "                                endDate += '-' + endDay['value']\n",
    "                #print('end date: ', endDate)\n",
    "                department = employment['department-name']\n",
    "                # if there is no value for department, set it to empty string\n",
    "                if not department:\n",
    "                    department = ''\n",
    "                #print(department)\n",
    "                if employment['organization']:\n",
    "                    organization = employment['organization']['name']\n",
    "                #print(organization)\n",
    "                if 'Vanderbilt University' in organization:\n",
    "                    print(orcidId, givenNames, familyName, startDate, endDate, department, organization)\n",
    "                    table.append([orcidId, givenNames, familyName, startDate, endDate, department, organization])\n",
    "                #print(table)\n",
    "        sleep(.25)\n",
    "\n",
    "print()\n",
    "print('Done')\n",
    "fileName = 'orcid_data.csv'\n",
    "writeCsv(fileName, table)\n",
    "fileName = 'orcid_other_names.csv'\n",
    "writeCsv(fileName, otherNameList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape departmental website\n",
    "\n",
    "script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/scrape-bsci.ipynb\n",
    "\n",
    "Note: currently script is ideosyncratic to https://as.vanderbilt.edu/biosci/people/index.php?group= patterned pages in the Vanderbilt BSCI department website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "from bs4 import BeautifulSoup # web-scraping library\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def writeCsv(fileName, array):\n",
    "    fileObject = open(fileName, 'w', newline='', encoding='utf-8')\n",
    "    writerObject = csv.writer(fileObject)\n",
    "    for row in array:\n",
    "        writerObject.writerow(row)\n",
    "    fileObject.close()\n",
    "\n",
    "acceptMediaType = 'text/html'\n",
    "userAgentHeader = 'BaskaufScraper/0.1 (steve.baskauf@vanderbilt.edu)'\n",
    "requestHeaderDictionary = {\n",
    "    'Accept' : acceptMediaType,\n",
    "    'User-Agent': userAgentHeader\n",
    "}\n",
    "\n",
    "outputTable = [['name', 'degree', 'role', 'category']]\n",
    "categories = ['primary-training-faculty', 'research-and-teaching-faculty', 'secondary-faculty', 'postdoc-fellows', 'emeriti']\n",
    "degreeList = [\n",
    "    {'string': 'Ph.D.', 'value': 'Ph.D.'},\n",
    "    {'string': 'PhD', 'value': 'Ph.D.'},\n",
    "    {'string': 'D.Phil.', 'value': 'D.Phil.'},\n",
    "    {'string': 'J.D.', 'value': 'J.D.'}\n",
    "     ]\n",
    "\n",
    "for category in categories:\n",
    "    url = 'https://as.vanderbilt.edu/biosci/people/index.php?group=' + category\n",
    "    response = requests.get(url, headers = requestHeaderDictionary)\n",
    "    soupObject = BeautifulSoup(response.text,features=\"html5lib\")\n",
    "\n",
    "    # get the first table from the page\n",
    "    tableObject = soupObject.find_all('table')[0]\n",
    "    \n",
    "    # get the rows from the table\n",
    "    rowObjectsList = tableObject.find_all('tr')\n",
    "    for rowObject in rowObjectsList:\n",
    "        # get the cells from each row\n",
    "        cellObjectsList = rowObject.find_all('td')\n",
    "        # picture is in cell 0, name and title is in cell 1\n",
    "        nameCell = cellObjectsList[1]\n",
    "        # the name part is bolded\n",
    "        name = nameCell('strong')[0].text\n",
    "        \n",
    "        # separate degrees from names\n",
    "        degree = ''\n",
    "        for testDegree in degreeList:\n",
    "            if testDegree['string'] in name:\n",
    "                name = name.partition(', ' + testDegree['string'])[0]\n",
    "                # correct any malformed strings\n",
    "                degree = testDegree['value']\n",
    "\n",
    "        try:\n",
    "            # process the roles text\n",
    "            dirtyText  = str(nameCell)\n",
    "            # get rid of trailing td tag\n",
    "            nameCellText = dirtyText.split('</td>')[0]\n",
    "            cellLines = nameCellText.split('<br/>')\n",
    "            roles = []\n",
    "            for lineIndex in range(1, len(cellLines)):\n",
    "                roleDict = {}\n",
    "                if ' of ' in cellLines[lineIndex]:\n",
    "                    pieces = cellLines[lineIndex].split(' of ')\n",
    "                    roleDict['title'] = pieces[0]\n",
    "                    roleDict['department'] = pieces[1]\n",
    "                    roles.append(roleDict)\n",
    "                elif ' in ' in cellLines[lineIndex]:\n",
    "                    pieces = cellLines[lineIndex].split(' in ')\n",
    "                    roleDict['title'] = pieces[0]\n",
    "                    roleDict['department'] = pieces[1]\n",
    "                    roles.append(roleDict)\n",
    "                else:\n",
    "                    roleDict['title'] = cellLines[lineIndex]\n",
    "                    roleDict['department'] = ''\n",
    "                    roles.append(roleDict)\n",
    "                if ', Emeritus' in roleDict['department']:\n",
    "                    roleDict['department'] = roleDict['department'].split(', Emeritus')[0]\n",
    "                    roleDict['title'] = 'Emeritus ' + roleDict['title']\n",
    "            rolesJson = json.dumps(roles)\n",
    "\n",
    "        except:\n",
    "            rolesJson = ''\n",
    "        outputTable.append([name, degree, rolesJson, category])            \n",
    "\n",
    "fileName = 'bsci-employees.csv'\n",
    "writeCsv(fileName, outputTable)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match BSCI people with ORCID results\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/orcid/match_bsci_orcid.ipynb\n",
    "\n",
    "Adds data to the bsci-employees.csv file and outputs as bsci-employees-with-orcid.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def readDict(filename):\n",
    "    fileObject = open(filename, 'r', newline='', encoding='utf-8')\n",
    "    dictObject = csv.DictReader(fileObject)\n",
    "    array = []\n",
    "    for row in dictObject:\n",
    "        array.append(row)\n",
    "    fileObject.close()\n",
    "    return array\n",
    "\n",
    "filename = 'bsci-employees.csv'\n",
    "bsciEmployees = readDict(filename)\n",
    "\n",
    "filename = 'orcid_data.csv'\n",
    "orcidData = readDict(filename)\n",
    "\n",
    "testRatio = 90\n",
    "departmentTestRatio = 90\n",
    "for employeeIndex in range(0, len(bsciEmployees)):\n",
    "    matched = False\n",
    "    for row in orcidData:\n",
    "        name = row['givenNames'] + ' ' + row['familyName']\n",
    "        #ratio = fuzz.ratio(name, bsciEmployees[employeeIndex][0])\n",
    "        #partialRatio = fuzz.partial_ratio(name, bsciEmployees[employeeIndex][0])\n",
    "        #sortRatio = fuzz.token_sort_ratio(name, bsciEmployees[employeeIndex][0])\n",
    "        \n",
    "        output = ''\n",
    "        # the set ratio seems to do the best job of matching\n",
    "        setRatio = fuzz.token_set_ratio(name, bsciEmployees[employeeIndex]['name'])\n",
    "        if setRatio >= testRatio:\n",
    "            output = str(setRatio) + ' ' + name + ' / ' + bsciEmployees[employeeIndex]['name']\n",
    "            \n",
    "            if row['department'] == '':\n",
    "                output += \" WARNING: no department given in ORCID\"\n",
    "            else:\n",
    "                # carry out a secondary test to see if any of the departments listed in the BSCI page\n",
    "                # are a good match to the department given in the ORCID record\n",
    "                \n",
    "                # expand the role JSON into a list of dictionaries\n",
    "                roleDict = json.loads(bsciEmployees[employeeIndex]['role'])\n",
    "                departmentMatch = False\n",
    "                for department in roleDict:\n",
    "                    setRatio = fuzz.token_set_ratio(department['department'], row['department'])\n",
    "                    if setRatio > departmentTestRatio:\n",
    "                        departmentMatch = True\n",
    "                        output += ' ' + str(setRatio) + ' ' + row['department']\n",
    "                if not departmentMatch:\n",
    "                    output += ' WARNING: ' + row['department'] + ' less than ' + str(departmentTestRatio) + '% match to any dept.'\n",
    "            print(output)\n",
    "            matched = True\n",
    "            foundOrcid = row['orcid']\n",
    "            # We only care about the first good match to an ORCID record, kill the loop after that\n",
    "            break\n",
    "    if matched:\n",
    "        bsciEmployees[employeeIndex]['orcid'] = foundOrcid\n",
    "    else:\n",
    "        bsciEmployees[employeeIndex]['orcid'] = ''\n",
    "        \n",
    "filename = 'bsci-employees-with-orcid.csv'\n",
    "with open(filename, 'w', newline='') as csvFileObject:\n",
    "    fieldnames = ['name', 'degree', 'category', 'orcid', 'role']\n",
    "    writer = csv.DictWriter(csvFileObject, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in bsciEmployees:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Vanderbilt people data from Wikidata\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/download-vanderbilt-people.py\n",
    "\n",
    "Performs a SPARQL query to find people in Wikidata that are employed at Vanderbilt.  Returns name, description, start date, end date, and ORCID if it has them.  Output to vanderbilt_wikidata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# function to write results to a file\n",
    "def writeCsv(fileName, array):\n",
    "    fileObject = open(fileName, 'w', newline='', encoding='utf-8')\n",
    "    writerObject = csv.writer(fileObject)\n",
    "    for row in array:\n",
    "        writerObject.writerow(row)\n",
    "    fileObject.close()\n",
    "\n",
    "endpointUrl = 'https://query.wikidata.org/sparql'\n",
    "query = '''select distinct  ?person ?name ?orcid ?startDate ?endDate ?description where {\n",
    "  ?person p:P108 ?statement.\n",
    "  ?statement ps:P108  wd:Q29052.\n",
    "  optional{\n",
    "    ?person rdfs:label ?name.\n",
    "    FILTER(lang(?name)=\"en\")\n",
    "    }\n",
    "  optional{?statement pq:P580 ?startDate.}\n",
    "  optional{?statement pq:P582 ?endDate.}\n",
    "  optional{?person wdt:P496 ?orcid.}\n",
    "  optional{\n",
    "    ?person schema:description ?description.\n",
    "    FILTER(lang(?description)=\"en\")\n",
    "          }\n",
    "  }'''\n",
    "\n",
    "# The endpoint defaults to returning XML, so the Accept: header is required\n",
    "r = requests.get(endpointUrl, params={'query' : query}, headers={'Accept' : 'application/json'})\n",
    "\n",
    "data = r.json()\n",
    "print(json.dumps(data,indent = 2))\n",
    "\n",
    "table = [['wikidataIri', 'name', 'description', 'startDate', 'endDate', 'orcid']]\n",
    "items = data['results']['bindings']\n",
    "for item in items:\n",
    "    wikidataIri = item['person']['value']\n",
    "    name = ''\n",
    "    if 'name' in item:\n",
    "        name = item['name']['value']\n",
    "    description = ''\n",
    "    if 'description' in item:\n",
    "        description = item['description']['value']\n",
    "    startDate = ''\n",
    "    if 'startDate' in item:\n",
    "        startDate = item['startDate']['value']\n",
    "    endDate = ''\n",
    "    if 'endDate' in item:\n",
    "        endDate = item['endDate']['value']\n",
    "    orcid = ''\n",
    "    if 'orcid' in item:\n",
    "        orcid = item['orcid']['value']\n",
    "    table.append([wikidataIri, name, description, startDate, endDate, orcid])\n",
    "    \n",
    "fileName = 'vanderbilt_wikidata.csv'\n",
    "writeCsv(fileName, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Vanderbilt people's altLabels from Wikidata\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/download-vanderbilt-people-altlabels.py\n",
    "\n",
    "Similar to previous query, except when there is a match, it downloads the altLabels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# function to write results to a file\n",
    "def writeCsv(fileName, array):\n",
    "    fileObject = open(fileName, 'w', newline='', encoding='utf-8')\n",
    "    writerObject = csv.writer(fileObject)\n",
    "    for row in array:\n",
    "        writerObject.writerow(row)\n",
    "    fileObject.close()\n",
    "\n",
    "endpointUrl = 'https://query.wikidata.org/sparql'\n",
    "query = '''select distinct  ?person ?altLabel where {\n",
    "  ?person p:P108 ?statement.\n",
    "  ?statement ps:P108  wd:Q29052.\n",
    "  ?person skos:altLabel ?altLabel.\n",
    "  FILTER(lang(?altLabel)=\"en\")\n",
    "}'''\n",
    "\n",
    "# The endpoint defaults to returning XML, so the Accept: header is required\n",
    "r = requests.get(endpointUrl, params={'query' : query}, headers={'Accept' : 'application/json'})\n",
    "\n",
    "data = r.json()\n",
    "print(json.dumps(data,indent = 2))\n",
    "\n",
    "table = [['wikidataIri', 'altLabel']]\n",
    "items = data['results']['bindings']\n",
    "for item in items:\n",
    "    wikidataIri = item['person']['value']\n",
    "    altLabel = ''\n",
    "    if 'altLabel' in item:\n",
    "        altLabel = item['altLabel']['value']\n",
    "    table.append([wikidataIri, altLabel])\n",
    "    \n",
    "fileName = 'vanderbilt_wikidata_altlabels.csv'\n",
    "writeCsv(fileName, table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match BSCI people to Wikidata\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/match_bsci_wikidata.ipynb\n",
    "\n",
    "Attempts to match records of people Wikidata knows to work at Vanderbilt with departmental people by matching their ORCIDs, then name strings.\n",
    "\n",
    "Finally, it attempts to do a SPARQL query on name string variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "def readDict(filename):\n",
    "    fileObject = open(filename, 'r', newline='', encoding='utf-8')\n",
    "    dictObject = csv.DictReader(fileObject)\n",
    "    array = []\n",
    "    for row in dictObject:\n",
    "        array.append(row)\n",
    "    fileObject.close()\n",
    "    return array\n",
    "\n",
    "def extractQNumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# query for a single variable that's an item named 'item'\n",
    "# returns a list of results\n",
    "def searchWikidataForQId(query):\n",
    "    results = []\n",
    "    endpointUrl = 'https://query.wikidata.org/sparql'\n",
    "    acceptMediaType = 'application/json'\n",
    "    userAgentHeader = 'BaskaufScraper/0.1 (steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "    'Accept' : acceptMediaType,\n",
    "    'User-Agent': userAgentHeader\n",
    "}\n",
    "    r = requests.get(endpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append(qNumber)\n",
    "    except:\n",
    "        results = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = searchWikidataForQId(query)\n",
    "    return results\n",
    "\n",
    "filename = 'bsci-employees-with-orcid.csv'\n",
    "bsciEmployees = readDict(filename)\n",
    "\n",
    "filename = 'vanderbilt_wikidata.csv'\n",
    "wikidataData = readDict(filename)\n",
    "\n",
    "testRatio = 90\n",
    "departmentTestRatio = 90\n",
    "\n",
    "for employeeIndex in range(0, len(bsciEmployees)):\n",
    "    # 0=unmatched\n",
    "    # 1=matched with ORCID in both sources\n",
    "    # 2=ORCID in BSCI but name match to Wikidata (no ORCID)\n",
    "    # 3=no ORCID in BSCI but name match to Wikidata (with ORCID); could happen if affiliation isn't matched in ORCID\n",
    "    # 4=no ORCID in BSCI but name match to Wikidata (no ORCID)\n",
    "    # 5=ORCID in BSCI and found via SPARQL ORCID search (likely non-VU affiliated in Wikidata)\n",
    "    # 6=ORCID in BSCI and found via SPARQL name search (non-VU affiliated without ORCID)\n",
    "    # 7=no ORCID in BSCI, no name match\n",
    "    # 8=ORCID in BSCI, error in SPARQL ORCID search\n",
    "    # 9=no ORCID in BSCI, error in SPARQL name search\n",
    "    matchStatus = 0\n",
    "    for row in wikidataData:\n",
    "        # We know the employee has an ORCID, so try to match it\n",
    "        if bsciEmployees[employeeIndex]['orcid'] != '':\n",
    "            # There's a match, hooray!\n",
    "            if bsciEmployees[employeeIndex]['orcid'] == row['orcid']:\n",
    "                print('orcid match: ', row['name'] + ' ' + row['orcid'])\n",
    "                matchStatus = 1\n",
    "                bsciEmployees[employeeIndex]['wikidataId'] = extractQNumber(row['wikidataIri'])\n",
    "            # No ORCID match - see if the name matches\n",
    "            else:\n",
    "                setRatio = fuzz.token_set_ratio(row['name'], bsciEmployees[employeeIndex]['name'])\n",
    "                if setRatio >= testRatio:\n",
    "                    print('name match: ', str(setRatio) + ' ' + row['name'] + ' / ' + bsciEmployees[employeeIndex]['name'] + ' BSCI:' + bsciEmployees[employeeIndex]['orcid'])\n",
    "                    matchStatus = 2\n",
    "                    bsciEmployees[employeeIndex]['wikidataId'] = extractQNumber(row['wikidataIri'])\n",
    "        # As far as we know, the employee doesn't have an ORCID, so try to match the name\n",
    "        else:\n",
    "            setRatio = fuzz.token_set_ratio(row['name'], bsciEmployees[employeeIndex]['name'])\n",
    "            # We get a name match \n",
    "            if setRatio >= testRatio:\n",
    "                # For some reason, Wikidata has the ORCID, so grab it\n",
    "                if row['orcid'] != '':\n",
    "                    print('name match: ', str(setRatio) + ' ' + row['name'] + ' / ' + bsciEmployees[employeeIndex]['name'] + ' ORCID:' + row['orcid'])\n",
    "                    bsciEmployees[employeeIndex]['orcid'] = row['orcid']\n",
    "                    matchStatus = 3\n",
    "                # Wikidata doesn't have an ORCID\n",
    "                else:\n",
    "                    print('name match: ', str(setRatio) + ' ' + row['name'] + ' / ' + bsciEmployees[employeeIndex]['name'] + ' WD description: ' + row['description'])\n",
    "                    matchStatus = 4\n",
    "                bsciEmployees[employeeIndex]['wikidataId'] = extractQNumber(row['wikidataIri'])\n",
    "        # We've gone all the way through the without finding a match\n",
    "\n",
    "    # Do a last ditch attempt to try to find the person in Wikidata by doing a SPARQL search for their ORCID\n",
    "    if matchStatus == 0:\n",
    "        if bsciEmployees[employeeIndex]['orcid'] != '':\n",
    "            query = '''\n",
    "select distinct ?item where {\n",
    "  ?item wdt:P496 \"''' + bsciEmployees[employeeIndex]['orcid'] + '''\".\n",
    "  }\n",
    "'''\n",
    "            results = searchWikidataForQId(query)\n",
    "            if len(results) > 0:\n",
    "                print('SPARQL ORCID search: ', bsciEmployees[employeeIndex]['name'], results)\n",
    "                if len(results) == 1:\n",
    "                    # if search fails and return an error message\n",
    "                    if len(results[0]) > 15:\n",
    "                        matchStatus = 8\n",
    "                        print('Error message in ORCID search')\n",
    "                    else:\n",
    "                        matchStatus = 5\n",
    "                        bsciEmployees[employeeIndex]['wikidataId'] = results[0]\n",
    "                else:\n",
    "                    print('ERROR: multiple results for same ORCID')\n",
    "print('done')\n",
    "#print(bsciEmployees)\n",
    "\n",
    "filename = 'bsci-employees-with-wikidata.csv'\n",
    "with open(filename, 'w', newline='') as csvFileObject:\n",
    "    fieldnames = ['wikidataId', 'name', 'degree', 'category', 'orcid', 'wikidataStatus', 'role']\n",
    "    writer = csv.DictWriter(csvFileObject, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in bsciEmployees:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosscheck people against publications\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/crosscheck-publications.ipynb\n",
    "\n",
    "Checks possible Wikidata records against publications in CrossRef and PubMed to see if the author metadata will disambituate the Wikidata record.\n",
    "\n",
    "Note: this needs to be integrated with the previous script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import xml.etree.ElementTree as et\n",
    "from time import sleep\n",
    "import urllib\n",
    "\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'testApiScript' # give your application a name here\n",
    "\n",
    "wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "acceptMediaType = 'application/json'\n",
    "userAgentHeader = 'BaskaufScraper/0.1 (mailto:steve.baskauf@vanderbilt.edu)'\n",
    "requestHeaderDictionary = {\n",
    "    'Accept' : acceptMediaType,\n",
    "    'User-Agent': userAgentHeader\n",
    "    }\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsDict\n",
    "\n",
    "# returns a list of employer strings for the item with Wikidata ID qId;,P108 is employer\n",
    "def searchWikidataEmployer(qId):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?employer where {\n",
    "        wd:'''+ qId + ''' wdt:P108 ?employerId.\n",
    "        ?employerId rdfs:label ?employer.\n",
    "        FILTER(lang(?employer) = 'en')\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                resultsList.append(statement['employer']['value'])\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "# returns a list of results of articles by person with Wikidata ID qId\n",
    "def searchWikidataArticle(qId):\n",
    "    resultsList = []\n",
    "    # P50 is \"author\"; P698 is the PubMed ID of the article; P356 is the DOI of the article\n",
    "    query = '''select distinct ?title ?doi ?pmid where {\n",
    "      ?article wdt:P50 wd:''' + qId + '''.\n",
    "      optional {\n",
    "          ?article rdfs:label ?title.\n",
    "          FILTER(lang(?title) = 'en')\n",
    "          }\n",
    "      optional {?article wdt:P698 ?pmid.}\n",
    "      optional {?article wdt:P356 ?doi.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            if 'title' in statement:\n",
    "                title = statement['title']['value']\n",
    "                print('title=',title)\n",
    "            else:\n",
    "                title = ''\n",
    "            if 'pmid' in statement:\n",
    "                pmid = statement['pmid']['value']\n",
    "            else:\n",
    "                pmid = ''\n",
    "            if 'doi' in statement:\n",
    "                doi = statement['doi']['value']\n",
    "            else:\n",
    "                doi = ''\n",
    "            resultsList.append({'title': title, 'pmid': pmid, 'doi': doi})\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "def retrievePubMedData(pmid):\n",
    "    fetchUrl = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    paramDict = {\n",
    "        'tool': toolName, \n",
    "        'email': emailAddress,\n",
    "        'db': 'pubmed', \n",
    "         #'retmode': 'xml', \n",
    "        'rettype': 'abstract', \n",
    "        'id': pmid\n",
    "    }\n",
    "    response = requests.get(fetchUrl, params=paramDict)\n",
    "    #print(response.url)\n",
    "    pubData = response.text  # the response text is XML\n",
    "    #print(pubData)  # uncomment this line to see the XML\n",
    "\n",
    "    # process the returned XML, see https://docs.python.org/2/library/xml.etree.elementtree.html\n",
    "    root = et.fromstring(pubData)\n",
    "    try:\n",
    "        title = root.findall('.//ArticleTitle')[0].text\n",
    "    except:\n",
    "        title = ''\n",
    "    try:\n",
    "        print(title)\n",
    "    except:\n",
    "        print('')\n",
    "    names = root.findall('.//Author')\n",
    "    affiliations = []\n",
    "    for name in names:\n",
    "        try:\n",
    "            affiliation = name.find('./AffiliationInfo/Affiliation').text\n",
    "        except:\n",
    "            affiliation = ''\n",
    "        try:\n",
    "            lastName = name.find('./LastName').text\n",
    "        except:\n",
    "            lastName = ''\n",
    "        try:\n",
    "            foreName = name.find('./ForeName').text\n",
    "        except:\n",
    "            foreName = ''\n",
    "              \n",
    "        #print(lastName)\n",
    "        #print(affiliation)\n",
    "        affiliations.append({'affiliation': affiliation, 'surname': lastName, 'forename': foreName})\n",
    "    #print()\n",
    "\n",
    "    # See https://www.ncbi.nlm.nih.gov/books/NBK25497/ for usage guidelines. \n",
    "    # An API key is required for more than 3 requests per second.\n",
    "    sleep(0.5) # wait half a second before hitting the API again to avoid getting blocked\n",
    "    return affiliations\n",
    "\n",
    "def retrieveCrossRefDoi(doi):\n",
    "    authorList = []\n",
    "    crossRefEndpointUrl = 'https://api.crossref.org/works/'\n",
    "    encodedDoi = urllib.parse.quote(doi)\n",
    "    searchUrl = crossRefEndpointUrl + encodedDoi\n",
    "    response = requests.get(searchUrl, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = response.json()\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        if 'author' in data['message']:\n",
    "            authors = data['message']['author']\n",
    "            for author in authors:\n",
    "                authorDict = {}\n",
    "                if 'ORCID' in author:\n",
    "                    authorDict['orcid'] = author['ORCID']\n",
    "                else:\n",
    "                    authorDict['orcid'] = ''\n",
    "                if 'given' in author:\n",
    "                    authorDict['givenName'] = author['given']\n",
    "                else:\n",
    "                    authorDict['givenName'] = ''\n",
    "                if 'family' in author:\n",
    "                    authorDict['familyName'] = author['family']\n",
    "                else:\n",
    "                    authorDict['familyName'] = ''\n",
    "                affiliationList = []\n",
    "                if 'affiliation' in author:\n",
    "                    for affiliation in author['affiliation']:\n",
    "                        affiliationList.append(affiliation['name'])\n",
    "                # if there aren't any affiliations, the list will remain empty\n",
    "                authorDict['affiliation'] = affiliationList\n",
    "                authorList.append(authorDict)\n",
    "    except:\n",
    "        authorList = [data]\n",
    "    return authorList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Body of search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'bsci-employees-with-wikidata.csv'\n",
    "bsciEmployees = readDict(filename)\n",
    "\n",
    "for employeeIndex in range(0, len(bsciEmployees)):\n",
    "    \n",
    "    # ************ left off here\n",
    "    # try a name search as a last resort\n",
    "    # NOTE: There is a significant number of cases where the person has an ORCID and is in Wikidata\n",
    "    # but we don't know their ORCID because their affiliation didn't get into Wikidata or ORCID\n",
    "    # These cases will need to be checked manually against publications to make sure they are \n",
    "    # the right people.\n",
    "        results = searchNameAtWikidata(bsciEmployees[employeeIndex]['name'])\n",
    "        if len(results) > 0:\n",
    "            print('SPARQL name search: ', bsciEmployees[employeeIndex]['name'], results)\n",
    "            if len(results) == 1:\n",
    "                # check for error message in results\n",
    "                if len(results[0]) > 15:\n",
    "                    matchStatus = 9\n",
    "                    print('Error message in name search')\n",
    "                else:\n",
    "                    matchStatus = 6\n",
    "        else:\n",
    "            print('No Wikidata match: ', bsciEmployees[employeeIndex]['name'])\n",
    "            matchStatus = 7\n",
    "    bsciEmployees[employeeIndex]['wikidataStatus'] = str(matchStatus)\n",
    "    # ************************\n",
    "\n",
    "    #qIds = [\"Q21503132\", \"Q45530486\", \"Q45579795\", \"Q45579952\", \"Q45580596\", \"Q45631936\", \"Q56480357\", \"Q57416670\", \"Q57550074\", \"Q59553435\", \"Q70150244\"]\n",
    "    #qIds = [\"Q16910840\", \"Q64091655\", \"Q66741850\", \"Q67221376\"]\n",
    "    qIds = [\"Q64091698\"]\n",
    "    testString = 'Biological Sciences Vanderbilt'\n",
    "    testEmployer = 'Vanderbilt University'\n",
    "    #testAuthor = 'Peng Xu'\n",
    "    #testAuthor = 'Thomas Clements'\n",
    "    testAuthor = 'Larisa DeSantis'\n",
    "    #testOrcid = '0000-0001-7103-3692'\n",
    "    testOrcid = ''\n",
    "    testOrcid = ''\n",
    "\n",
    "    print('Checking identities for ', testAuthor)\n",
    "    if testOrcid == '':\n",
    "        print('(no ORCID)')\n",
    "    else:\n",
    "        print('ORCID: ', testOrcid)\n",
    "    print()\n",
    "    for qIdIndex in range(0, len(qIds)):\n",
    "        print(qIdIndex, 'Wikidata ID: ', qIds[qIdIndex])\n",
    "        descriptors = searchWikidataDescription(qIds[qIdIndex])\n",
    "        employers = searchWikidataEmployer(qIds[qIdIndex])\n",
    "        #print(descriptors)\n",
    "        if descriptors != {}:\n",
    "            if descriptors['description'] != '':\n",
    "                print('description: ', descriptors['description'])\n",
    "            for occupation in descriptors['occupation']:\n",
    "                print('occupation: ', occupation)\n",
    "            for employer in employers:\n",
    "                print('employer: ', employer)\n",
    "            if descriptors['orcid'] != '':\n",
    "                if testOrcid == '':\n",
    "                    # **** NOTE: if the person has an ORCID, it may be possible to find articles via ORCID\n",
    "                    # that aren't linked in Wikidata. Not sure if this happens often enough to handle it\n",
    "                    print('ORCID: ', descriptors['orcid'])\n",
    "                else:\n",
    "                    # This should always be true if the SPARQL query for ORCID was already done\n",
    "                    if testOrcid != descriptors['orcid']:\n",
    "                        print('*** NOT the same person; ORCID ' + descriptors['orcid'] + ' does not match.')\n",
    "                        break # don't continue the loop (look up references) since it's definitely not a match\n",
    "                    else:\n",
    "                        print('*** An ORCID match! How did it get missed in the earlier SPARQL query?')\n",
    "                        break\n",
    "        else:\n",
    "            print('No description or occupation given.')\n",
    "\n",
    "        result = searchWikidataArticle(qIds[qIdIndex])\n",
    "        if len(result) == 0:\n",
    "            print('No articles authored by that person')\n",
    "        else:\n",
    "            foundMatch = False\n",
    "            for article in result:\n",
    "                print('Checking article: ', article['title'])\n",
    "                if article['pmid'] == '':\n",
    "                    print('No PubMed ID')\n",
    "                else:\n",
    "                    print('Checking authors in PubMed article: ', article['pmid'])\n",
    "                    pubMedAuthors = retrievePubMedData(article['pmid'])\n",
    "                    for author in pubMedAuthors:\n",
    "                        nameTestRatio = fuzz.token_set_ratio(author['surname'], testAuthor)\n",
    "                        print(nameTestRatio, author['surname'])\n",
    "                        if nameTestRatio >= 90:\n",
    "                            if author['affiliation'] != '': \n",
    "                                setRatio = fuzz.token_set_ratio(testString, author['affiliation'])\n",
    "                                print('Affiliation test: ', setRatio, author['affiliation'])\n",
    "                                if setRatio >= 90:\n",
    "                                    foundMatch = True\n",
    "                            else:\n",
    "                                break # give up on this article because no affiliation string\n",
    "                # Don't look up the DOI if it's already found a match with PubMed\n",
    "                if foundMatch:\n",
    "                    break # stop checking articles after one has matched\n",
    "                else:\n",
    "                    if article['doi'] == '':\n",
    "                        print('No DOI')\n",
    "                    else:\n",
    "                        print('Checking authors in DOI article: ', article['doi'])\n",
    "                        doiAuthors = retrieveCrossRefDoi(article['doi'])\n",
    "                        for author in doiAuthors:\n",
    "                            nameTestRatio = fuzz.token_set_ratio(author['familyName'], testAuthor)\n",
    "                            print(nameTestRatio, author['familyName'])\n",
    "                            if nameTestRatio >= 90:\n",
    "                                if author['orcid'] != '':\n",
    "                                    if testOrcid == '':\n",
    "                                        print('ORCID from article: ', author['orcid'])\n",
    "                                    else:\n",
    "                                        if testOrcid != author['orcid']:\n",
    "                                            print('*** NOT the same person; ORCID ' + author['orcid'] + ' does not match.')\n",
    "                                            break # don't continue the loop (look up references) since it's definitely not a match\n",
    "                                        else:\n",
    "                                            print('*** An ORCID match!')\n",
    "                                            foundMatch = True\n",
    "                                            break\n",
    "\n",
    "\n",
    "                                if len(author['affiliation']) > 0:\n",
    "                                    for affiliation in author['affiliation']:\n",
    "                                        setRatio = fuzz.token_set_ratio(testString, affiliation)\n",
    "                                        print('Affiliation test: ', setRatio, affiliation)\n",
    "                                        if setRatio >= 90:\n",
    "                                            foundMatch = True\n",
    "                                #else:\n",
    "                                #    break # give up on this article because no affiliation string\n",
    "            if foundMatch:\n",
    "                print('***', qId, ' has a match.')\n",
    "            else:\n",
    "                print('No match found.')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
