{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once if you need to install the python-Levenshtein package\n",
    "\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data using DOI\n",
    "\n",
    "NOTE: this script is under development and is not intended for production use anywhere. It's been made public in case you want to borrow or hack any of the code for your own purposes. So no promises about anything!\n",
    "\n",
    "It's got a ton of functions that are defined, but not used. I've stashed them here because I might use some of them later.\n",
    "\n",
    "Currently the script only uses the CrossRef API, but potentially could use others like DataCite\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This section imports libraries, sets default values, and defines functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve_doi_data, a script for downloading data from CrossRef and preparing it to upload to Wikidata\n",
    "version = '0.1'\n",
    "created = '2021-11-06'\n",
    "\n",
    "# (c) 2021 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# Note: this script requires the VanderBot script to upload the generated data to Wikidata\n",
    "# For details, see https://github.com/HeardLibrary/linked-data/tree/master/vanderbot\n",
    "\n",
    "import requests   # best library to manage HTTP transactions\n",
    "import json\n",
    "import re\n",
    "from time import sleep\n",
    "import csv\n",
    "import sys\n",
    "#import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "accept_media_type = 'application/json'\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "user_agent_header = 'retrieve_doi_data/0.1 (https://github.com/HeardLibrary/linked-data/tree/master/publications/crossref; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "sparql_sleep = 0.1\n",
    "default_language = 'en'\n",
    "\n",
    "# NCBI identification requirements:\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "email_address = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "tool_name = 'retrieve_doi_data0.1' # give your application a name here, no spaces\n",
    "\n",
    "# Option to log to a file instead of the console\n",
    "log_path = '' # path to log file, default to none\n",
    "log_object = sys.stdout # log output defaults to the console screen\n",
    "file_path = '' # path to output directory, default to current working directory\n",
    "\n",
    "opts = [opt for opt in sys.argv[1:] if opt.startswith('-')]\n",
    "args = [arg for arg in sys.argv[1:] if not arg.startswith('-')]\n",
    "\n",
    "if '--log' in opts: # set output to specified log file or path including file name\n",
    "    log_path = args[opts.index('--log')]\n",
    "    log_object = open(log_path, 'wt', encoding='utf-8') # direct output sent to log_object to log file instead of sys.stdout\n",
    "if '-L' in opts: # set output to specified log file or path including file name\n",
    "    log_path = args[opts.index('-L')]\n",
    "    log_object = open(log_path, 'wt', encoding='utf-8') # direct output sent to log_object to log file instead of sys.stdout\n",
    "if '--path' in opts: # set path to input directory. If omitted, the path is the current working directory\n",
    "    file_path = args[opts.index('--path')]\n",
    "if '-P' in opts: # set path to input directory. If omitted, the path is the current working directory\n",
    "    file_path = args[opts.index('-P')]\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# *** For now, hard-code logging to crossref_errors.txt in the section that invokes the functions\n",
    "#log_path = 'crossref_errors.txt'\n",
    "#log_object = open(log_path, 'wt', encoding='utf-8')\n",
    "\n",
    "# Also hard-coding the path until moved out of Jupyter notebook environment, see below\n",
    "# --------------------------\n",
    "\n",
    "# List of known work types used by CrossRef\n",
    "work_types = [\n",
    "    {\n",
    "    'crossref_type_string': 'journal-article',\n",
    "    'qid': 'Q18918145', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'journal article'\n",
    "    },\n",
    "    {\n",
    "    'crossref_type_string': 'book-chapter',\n",
    "    'qid': 'Q21481766', # \"academic chapter\"\n",
    "    'description': 'academic book chapter'\n",
    "    },\n",
    "    {\n",
    "    'crossref_type_string': 'monograph',\n",
    "    'qid': 'Q193495', # monograph\n",
    "    'description': 'monograph'\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "# Output CSV label/description fields to be populated without references\n",
    "out_fields_labels = ['label_' + default_language, 'description_' + default_language]\n",
    "\n",
    "# Output CSV property fields to be populated without references\n",
    "out_fields_noref = ['instance_of']\n",
    "\n",
    "# Output CSV fields that include reference fields\n",
    "out_fields_ref = ['doi', 'pmid', 'published', 'title_' + default_language, 'journal', 'volume', 'page', 'issue']\n",
    "    \n",
    "# ------------------------\n",
    "# Utility functions\n",
    "# ------------------------\n",
    "\n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# Determine the current CrossRef rate limit by an initial ping\n",
    "response = requests.get('https://api.crossref.org/works/10.3233/SW-150203', headers=generate_header_dictionary(accept_media_type))\n",
    "crossref_headers = response.headers\n",
    "limit_count = int(crossref_headers['x-rate-limit-limit'])\n",
    "interval_string = crossref_headers['x-rate-limit-interval']\n",
    "interval_sec = int(interval_string[:len(interval_string)-1]) # remove the \"s\" from the end\n",
    "api_sleep = interval_sec / limit_count + 0.005\n",
    "\n",
    "# Due to problems with direct POST of UTF-8, changed to POST with URL-encoded parameters\n",
    "# See https://www.w3.org/TR/sparql11-protocol/#update-via-post-urlencoded\n",
    "# and https://stackoverflow.com/questions/34618149/post-unicode-string-to-web-service-using-python-requests-library\n",
    "\n",
    "# NOTE: there are still some issues that have not been worked out with quotation marks in query strings.\n",
    "# Still working on this; see also the send_sparql_query() below.\n",
    "def generate_sparql_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "#        'Content-Type': 'application/sparql-query',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "sparql_request_header = generate_sparql_header_dictionary(accept_media_type, user_agent_header)\n",
    "\n",
    "# Generate the current UTC xsd:date\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Open a CSV and read in the header fields as a list\n",
    "def get_csv_fieldnames(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        reader_object = csv.reader(file_object)\n",
    "        for row in reader_object: # iterate through only the first row (the header)\n",
    "            return row\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Load JSON file data from local drive into a Python data structure\n",
    "def load_json_into_data_struct(path):\n",
    "    with open(path, 'rt', encoding='utf-8') as file_object:\n",
    "        file_text = file_object.read()\n",
    "    structure = json.loads(file_text)\n",
    "    # uncomment the following line to view the data\n",
    "    # print(json.loads(structure, indent = 2))\n",
    "    return(structure)\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "# NOTE: request_header and endpoint are global variables defined earlier in the script\n",
    "def send_sparql_query(query_string):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    #print('querying SPARQL endpoint to acquire item metadata')\n",
    "    #response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    response = requests.post(endpoint, data=dict(query=query_string), headers=sparql_request_header)\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    #print('done retrieving data')\n",
    "    # print(json.dumps(results, indent=2))\n",
    "    \n",
    "    sleep(sparql_sleep) # delay to avoid hitting the Query Service too fast\n",
    "    return results\n",
    "\n",
    "# ------------------------\n",
    "# CrossRef search-related  functions\n",
    "# ------------------------\n",
    "\n",
    "# Use the CrossRef registration agency test to find out who issued the non-CrosRef DOI\n",
    "def discover_issuing_agency(doi):\n",
    "    encoded_doi = urllib.parse.quote(doi)\n",
    "    search_url = 'https://api.crossref.org/works/' + encoded_doi + '/agency'\n",
    "    response = requests.get(search_url, headers=generate_header_dictionary(accept_media_type))\n",
    "    sleep(api_sleep) # delay to avoid hitting the API faster than acceptable rate\n",
    "    if response.status_code == 404: # return \"not found\" if URL doesn't dereference\n",
    "        return 'not found'\n",
    "    try: # Try to parse as JSON\n",
    "        data = response.json()\n",
    "        return data['message']['agency']['id']\n",
    "    except: # if the response isn't JSON, then just return the response text\n",
    "        return response.text\n",
    "\n",
    "# See https://github.com/CrossRef/rest-api-doc for API details\n",
    "# Note: no authentication required, but must be \"nice\": observe rate limit, provide mailto:\n",
    "def retrieve_crossref_data(doi):\n",
    "    crossref_endpoint_url = 'https://api.crossref.org/works/'\n",
    "    # urllib.parse.quote performs URL encoding of a string\n",
    "    encoded_doi = urllib.parse.quote(doi)\n",
    "    search_url = crossref_endpoint_url + encoded_doi\n",
    "    response = requests.get(search_url, headers=generate_header_dictionary(accept_media_type))\n",
    "    article_dict = {}\n",
    "    if response.status_code == 404: # return empty dict if not found\n",
    "        # *** NOTE: at some future time, look up data at alternative issuing agencies\n",
    "        # For example, see https://support.datacite.org/docs/api-get-doi\n",
    "        \n",
    "        # For now, just log it\n",
    "        print(doi, 'not CrossRef. DOI issuing agency:', discover_issuing_agency(doi), '\\n', file=log_object)\n",
    "        sleep(api_sleep)\n",
    "        return article_dict\n",
    "    else:\n",
    "        author_list = []\n",
    "        try:\n",
    "            response_structure = response.json()\n",
    "            data = response_structure['message']\n",
    "        except:\n",
    "            # if not JSON, just return the response text\n",
    "            article_dict['message'] = response.text\n",
    "            sleep(api_sleep)\n",
    "            return article_dict\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        article_dict['doi'] = doi\n",
    "        if 'author' in data:\n",
    "            authors = data['author']\n",
    "            for author in authors:\n",
    "                authorDict = {}\n",
    "                if 'ORCID' in author:\n",
    "                    # Note: in CrossRef, the ORCIDs are given as full IRIs, so ORCID string needs to be extracted\n",
    "                    authorDict['orcid'] = extract_local_name(author['ORCID'])\n",
    "                else:\n",
    "                    authorDict['orcid'] = ''\n",
    "                if 'sequence' in author:\n",
    "                    authorDict['sequence'] = author['sequence']\n",
    "                else:\n",
    "                    authorDict['sequence'] = ''\n",
    "                if 'given' in author:\n",
    "                    authorDict['givenName'] = author['given']\n",
    "                else:\n",
    "                    authorDict['givenName'] = ''\n",
    "                if 'family' in author:\n",
    "                    authorDict['familyName'] = author['family']\n",
    "                else:\n",
    "                    authorDict['familyName'] = ''\n",
    "                affiliationList = []\n",
    "                if 'affiliation' in author:\n",
    "                    for affiliation in author['affiliation']:\n",
    "                        affiliationList.append(affiliation['name'])\n",
    "                # if there aren't any affiliations, the list will remain empty\n",
    "                authorDict['affiliation'] = affiliationList\n",
    "                author_list.append(authorDict)\n",
    "            article_dict['authors'] = author_list\n",
    "        '''\n",
    "        if 'issued' in data:\n",
    "            issued = data['issued']['date-parts'][0]\n",
    "            issued_date = str(issued[0])\n",
    "            if len(issued) > 1:\n",
    "                if len(str(issued[1])) == 1:\n",
    "                    issued_date += '-0'+ str(issued[1])\n",
    "                else:\n",
    "                    issued_date += '-'+ str(issued[1])\n",
    "                if len(issued) > 2:                \n",
    "                    if len(str(issued[2])) == 1:\n",
    "                        issued_date += '-0'+ str(issued[2])\n",
    "                    else:\n",
    "                        issued_date += '-'+ str(issued[2])\n",
    "            article_dict['published'] = issued_date\n",
    "        else:\n",
    "            article_dict['published'] = ''\n",
    "        '''\n",
    "        if 'created' in data:\n",
    "            article_dict['published'] = data['created']['date-time'][0:10] # get date part from dateTime\n",
    "        else:\n",
    "            article_dict['published'] = ''            \n",
    "        if 'volume' in data:\n",
    "            article_dict['volume'] = data['volume']\n",
    "        else:\n",
    "            article_dict['volume'] = ''\n",
    "        if 'issue' in data:\n",
    "            article_dict['issue'] = data['issue']\n",
    "        else:\n",
    "            article_dict['issue'] = ''\n",
    "        if 'page' in data:\n",
    "            article_dict['page'] = data['page']\n",
    "        else:\n",
    "            article_dict['page'] = ''\n",
    "        if 'ISSN' in data:\n",
    "            article_dict['journal_issn'] = data['ISSN']\n",
    "        else:\n",
    "            article_dict['journal_issn'] = []\n",
    "        if 'title' in data:\n",
    "            if len(data['title']) > 0:\n",
    "                article_dict['title_' + default_language] = data['title'][0]\n",
    "                article_dict['label_' + default_language] = data['title'][0]\n",
    "        else:\n",
    "            article_dict['title_' + default_language] = ''\n",
    "            article_dict['label_' + default_language] = ''\n",
    "        if 'container-title' in data:\n",
    "            if len(data['container-title']) > 0:\n",
    "                article_dict['journal_title'] = data['container-title'][0]\n",
    "        else:\n",
    "            article_dict['journal_title'] = ''\n",
    "         \n",
    "        if 'type' in data:\n",
    "            found = False\n",
    "            for work_type in work_types:\n",
    "                if data['type'] == work_type['crossref_type_string']:\n",
    "                    found = True\n",
    "                    article_dict['instance_of'] = work_type['qid']\n",
    "                    article_dict['description_' + default_language] = work_type['description']\n",
    "            if not found:\n",
    "                article_dict['instance_of'] = ''\n",
    "                article_dict['description_' + default_language] = ''\n",
    "        else:\n",
    "            article_dict['instance_of'] = ''\n",
    "            article_dict['description_' + default_language] = ''\n",
    "\n",
    "    sleep(api_sleep)\n",
    "    return article_dict\n",
    "\n",
    "def extract_doi_metadata(crossref_results, doi, pmid, today, alt_reference):\n",
    "    crossref_results = extract_journal_qid(crossref_results)\n",
    "    #print(crossref_results)\n",
    "    \n",
    "    out_dict = {'qid': ''}\n",
    "    for field in out_fields_labels:   \n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field] = crossref_results[field]\n",
    "    #print()\n",
    "    for field in out_fields_noref:   \n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field + '_uuid'] = ''\n",
    "        out_dict[field] = crossref_results[field]\n",
    "    #print()\n",
    "    for field in out_fields_ref:\n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field + '_uuid'] = ''\n",
    "        if field == 'published':\n",
    "            out_dict[field + '_nodeId'] = ''\n",
    "            out_dict[field + '_val'] = crossref_results[field]\n",
    "            out_dict[field + '_prec'] = ''\n",
    "        else:\n",
    "            out_dict[field] = crossref_results[field]\n",
    "        # Only add a reference if there is a value for that field\n",
    "        if crossref_results[field] == '':\n",
    "            out_dict[field + '_ref1_hash'] = ''\n",
    "            out_dict[field + '_ref1_referenceUrl'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_val'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_prec'] = ''\n",
    "        else:\n",
    "            out_dict[field + '_ref1_hash'] = ''\n",
    "            if field == 'pmid':\n",
    "                out_dict[field + '_ref1_referenceUrl'] = 'https://pubmed.ncbi.nlm.nih.gov/' + crossref_results['pmid'] + '/'\n",
    "            else:\n",
    "                if alt_reference == '':\n",
    "                    out_dict[field + '_ref1_referenceUrl'] = 'http://doi.org/' + doi\n",
    "                else:\n",
    "                    out_dict[field + '_ref1_referenceUrl'] = alt_reference\n",
    "            out_dict[field + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_val'] = today\n",
    "            out_dict[field + '_ref1_retrieved_prec'] = ''\n",
    "    return(out_dict)\n",
    "\n",
    "# ------------------------\n",
    "# Wikidata search-related  functions\n",
    "# ------------------------\n",
    "\n",
    "# This function just returns either True or False depending on whether there is an item in Wikidata with a DOI\n",
    "def doi_in_wikidata(doi):\n",
    "    query_string = '''ask {\n",
    "    {?qid wdt:P356 \"''' + doi + '''\".}\n",
    "UNION\n",
    "    {?qid wdt:P356 \"''' + doi.upper() + '''\".}\n",
    "    }'''\n",
    "    response = requests.post(endpoint, data=dict(query=query_string), headers=sparql_request_header)\n",
    "    data = response.json()['boolean']\n",
    "    return(data)\n",
    "\n",
    "# Look up the ISSN from CrossRef in Wikidata\n",
    "def extract_journal_qid(crossref_results):\n",
    "    if len(crossref_results['journal_issn']) == 0:\n",
    "        crossref_results['journal'] = ''\n",
    "        print('article:', crossref_results['label_' + default_language], 'has no ISSN.\\n', file=log_object)\n",
    "        return crossref_results\n",
    "\n",
    "    # Create VALUES list for items\n",
    "    issns_string = ''\n",
    "    for issn in crossref_results['journal_issn']:\n",
    "        issns_string += '\"' + issn + '\"\\n'\n",
    "    # Remove trailing newline\n",
    "    issns_string = issns_string[:len(issns_string)-1]\n",
    "\n",
    "    # Build query string\n",
    "    query_string = '''select distinct ?journal ?journalLabel where {\n",
    "      VALUES ?issn\n",
    "        {\n",
    "    ''' + issns_string + '''\n",
    "        }\n",
    "      ?journal wdt:P236 ?issn.\n",
    "      ?journal rdfs:label ?journalLabel.\n",
    "      filter(lang(?journalLabel)=\"en\")\n",
    "    }'''\n",
    "    #print(query_string)\n",
    "\n",
    "    # Send query to endpoint\n",
    "    query_results = send_sparql_query(query_string)\n",
    "    #pp.pprint(query_results)\n",
    "\n",
    "    if len(query_results) > 1:\n",
    "        print('Warning! More than one publication in Wikidata matched the ISSN for article', crossref_results['label_' + default_language], file=log_object)\n",
    "        print(query_results, '\\n', file=log_object)\n",
    "\n",
    "    # Extract Q ID from SPARQL query results. If there is more than one result, the last one will be used for the Q ID\n",
    "    for result in query_results:\n",
    "        journal_qid = extract_local_name(result['journal']['value'])\n",
    "        journal_name = result['journalLabel']['value']\n",
    "        if journal_name != crossref_results['journal_title']:\n",
    "            # NOTE: did empirical testing to see which kind of fuzzy matching worked best\n",
    "            #ratio = fuzz.ratio(journal_name, crossref_results['journal_title'])\n",
    "            #partial_ratio = fuzz.partial_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #sort_ratio = fuzz.token_sort_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #set_ratio = fuzz.token_set_ratio(journal_name, crossref_results['journal_title'])\n",
    "            w_ratio = fuzz.WRatio(journal_name, crossref_results['journal_title'])\n",
    "            #print('name similarity ratio', ratio)\n",
    "            #print('partial ratio', partial_ratio)\n",
    "            #print('sort_ratio', sort_ratio)\n",
    "            #print('set_ratio', set_ratio)\n",
    "            if w_ratio < 99:\n",
    "                print('article:', crossref_results['label_' + default_language], 'w_ratio:', w_ratio, 'Warning: Wikidata journal: \"' + journal_name + '\"', journal_qid, 'does not match CrossRef journal title: \"' + crossref_results['journal_title'] + '\"\\n', file=log_object)\n",
    "        #print('article:', crossref_results['label_' + default_language], 'journal:', journal_qid, journal_name)\n",
    "    crossref_results['journal'] = journal_qid\n",
    "    return crossref_results\n",
    "\n",
    "def generate_name_alternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        name_version = ''\n",
    "        for piece_number in range(0, len(pieces)-1):\n",
    "            name_version += pieces[piece_number] + ' '\n",
    "        name_version += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first and last name only\n",
    "    name_version = pieces[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial and last name only\n",
    "    name_version = initials[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    name_version = initials[0] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with last name\n",
    "    name_version = initials[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number]\n",
    "    name_version += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchWikidataForQIdByOrcid(orcid):\n",
    "    query_string = '''\n",
    "select distinct ?qid ?label ?description where {\n",
    "    ?qid wdt:P496 \"''' + orcid + '''\".\n",
    "    ?qid rdfs:label ?label.\n",
    "    FILTER(lang(?label)=\"'''+ default_language + '''\")\n",
    "    OPTIONAL {\n",
    "    ?qid schema:description ?description.\n",
    "    FILTER(lang(?description)=\"'''+ default_language + '''\")\n",
    "    }\n",
    "      }\n",
    "    '''\n",
    "    #print(query_string)\n",
    "\n",
    "    results = send_sparql_query(query_string)\n",
    "    #print(json.dumps(results, indent=2))\n",
    "\n",
    "    if len(results) > 1:\n",
    "        print('Warning!!! Multiple items with same ORCID!')\n",
    "        print(results)\n",
    "    if len(results) == 0:\n",
    "        out_dict = {}        \n",
    "    else:\n",
    "        out_dict = {\n",
    "            'qid': extract_local_name(results[0]['qid']['value']),\n",
    "            'label': results[0]['label']['value']\n",
    "            }\n",
    "        if 'description' in results[0]:\n",
    "            out_dict['description'] = results[0]['description']['value']\n",
    "        else:\n",
    "            out_dict['description'] = ''           \n",
    "    return out_dict\n",
    "\n",
    "def search_name_at_wikidata(name):\n",
    "    # carry out search for most languages that use Latin characters, plus some other commonly used languages\n",
    "    # See https://doi.org/10.1145/3233391.3233965\n",
    "    language_codes = [\n",
    "        'en',\n",
    "        'es',\n",
    "        'pt',\n",
    "        'fr',\n",
    "        'it',\n",
    "        'nl',\n",
    "        'de',\n",
    "        'da',\n",
    "        'et',\n",
    "        'hu',\n",
    "        'ga',\n",
    "        'ro',\n",
    "        'sk',\n",
    "        'sl',\n",
    "        'zu',\n",
    "        'tr',\n",
    "        'sv',\n",
    "        'zh',\n",
    "        'ru',\n",
    "        'ja',\n",
    "        'ar',\n",
    "        'pl',\n",
    "        'uk',\n",
    "        'ca',\n",
    "        'cs',\n",
    "        'la',\n",
    "        'nb',\n",
    "        'hu',\n",
    "        'he',\n",
    "        'eo',\n",
    "        'fi',\n",
    "        'ko'\n",
    "      ]\n",
    "    name_list = generate_name_alternatives(name)\n",
    "    alternatives = ''\n",
    "    for language_code in language_codes:\n",
    "        for alternative in name_list:\n",
    "            # get rid of quotes, which will break the query\n",
    "            alternative = alternative.replace('\"', '')\n",
    "            alternative = alternative.replace(\"'\", '')\n",
    "            alternatives += '\"' + alternative + '\"@' + language_code + '\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "#    r = requests.post(endpoint, data=query.encode('utf-8'), headers=sparql_request_header)\n",
    "    r = requests.post(endpoint, data=dict(query=query), headers=sparql_request_header)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        #print(data)\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidata_iri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qnumber = extract_local_name(wikidata_iri)\n",
    "            results.append({'qid': qnumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', ' ')\n",
    "    name = name.replace(',', ' ')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Get rid of empty pieces formed from extra spaces\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def screen_qids(qids, screens):\n",
    "    qid_values =''\n",
    "    for qid in qids:\n",
    "        qid_values += 'wd:' + qid + '\\n'\n",
    "\n",
    "    graph_pattern = ''\n",
    "    first_screen = True\n",
    "    for screen in screens:\n",
    "        # Each requirement in a screen has an AND relationship (all must be satisfied)\n",
    "        subgraph_pattern = ''\n",
    "        for requirement in screen:\n",
    "\n",
    "            # Set the value if required or use a dummy variable if any value is allowed\n",
    "            if requirement['entity'] == '':\n",
    "                value = '?var' + requirement['property'] # add the property string to the variable to guarantee uniqueness\n",
    "            elif re.match(r'Q\\d+', requirement['entity']): # regex to match Q IDs\n",
    "                value = 'wd:' + requirement['entity']\n",
    "            else: # if not nothing or a Q ID, assume it's a string literal\n",
    "                if requirement['lang'] == '':\n",
    "                    value = '\"' + requirement['entity'] + '\"'\n",
    "                else:\n",
    "                    value = '\"' + requirement['entity'] + '\"@' + requirement['lang']\n",
    "\n",
    "            # Set the property (label, description, or P value)\n",
    "            if requirement['property'] == 'label':\n",
    "                property = 'rdfs:label'\n",
    "            elif requirement['property'] == 'description':\n",
    "                property = 'schema:description'\n",
    "            else:\n",
    "                property = 'wdt:' + requirement['property']\n",
    "\n",
    "            # Place the value in either the subject or object position in the triple\n",
    "            if requirement['position'] == 'object':\n",
    "                triple_pattern = '?qid ' + property + ' ' + value + '.'\n",
    "            else:\n",
    "                triple_pattern = value + ' ' + property + ' ?qid.'\n",
    "\n",
    "            # Add filters if needed\n",
    "            if requirement['filter_type'] == '<' or requirement['filter_type'] == '>': \n",
    "                # note: string comparison only e.g. for datetimes, needs modification for actual numbers\n",
    "                triple_pattern += '\\nFILTER (STR(?var' + requirement['property'] + ') ' + requirement['filter_type'] + ' \"' + requirement['filter_string'] + '\")'\n",
    "\n",
    "            if requirement['filter_type'] == 'in': \n",
    "                # note: string comparison only\n",
    "                triple_pattern += '\\nFILTER (CONTAINS(?var' + requirement['property'] + ', \"' + requirement['filter_string'] + '\"))'\n",
    "\n",
    "            # Use MINUS if you want to exclude items that fit the pattern.\n",
    "            if requirement['require'] == 'exclude':\n",
    "                triple_pattern = 'minus {' + triple_pattern + '}'\n",
    "\n",
    "            triple_pattern += '\\n'\n",
    "            #print(triple_pattern)\n",
    "            subgraph_pattern += triple_pattern\n",
    "\n",
    "        # Now attach the subgraph pattern to any previous subgraph patterns using UNION to great an OR relationship\n",
    "        subgraph_pattern = '{\\n' + subgraph_pattern + '}\\n' # create a subgraph pattern so that several can be UNIONed\n",
    "        if first_screen: # The first subgraph pattern doesn't need the UNION inserted\n",
    "            first_screen = False\n",
    "        else:\n",
    "            graph_pattern = graph_pattern + 'UNION\\n'\n",
    "        graph_pattern += subgraph_pattern \n",
    "\n",
    "    query_string = '''\n",
    "    select distinct ?qid ?label ?description where {\n",
    "      VALUES ?qid\n",
    "      {\n",
    "      ''' + qid_values + '''}\n",
    "    ''' + graph_pattern + '''\n",
    "    \n",
    "    ?qid rdfs:label ?label.\n",
    "    FILTER(lang(?label)=\"'''+ default_language + '''\")\n",
    "    \n",
    "    OPTIONAL {\n",
    "    ?qid schema:description ?description.\n",
    "    FILTER(lang(?description)=\"'''+ default_language + '''\")\n",
    "    }\n",
    "      }\n",
    "    '''\n",
    "    #print(query_string)\n",
    "\n",
    "    results = send_sparql_query(query_string)\n",
    "    #print(json.dumps(results, indent=2))\n",
    "\n",
    "    return_list = []\n",
    "    for result in results:\n",
    "        out_dict = {\n",
    "            'qid': extract_local_name(result['qid']['value']),\n",
    "            'label': result['label']['value']\n",
    "            }\n",
    "        if 'description' in result:\n",
    "            out_dict['description'] = result['description']['value']\n",
    "        else:\n",
    "            out_dict['description'] = ''           \n",
    "        return_list.append(out_dict)\n",
    "    return return_list\n",
    "\n",
    "# returns a list of results of articles by person with Wikidata ID qid\n",
    "def search_wikidata_article(qid):\n",
    "    results_list = []\n",
    "    # P50 is \"author\"; P698 is the PubMed ID of the article\n",
    "    query = '''select distinct ?title ?pmid where {\n",
    "      ?article wdt:P50 wd:''' + qid + '''.\n",
    "      optional {\n",
    "          ?article rdfs:label ?title.\n",
    "          FILTER(lang(?title) = \"'''+ default_language + '''\")\n",
    "          }\n",
    "      optional {?article wdt:P698 ?pmid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.post(endpoint, data=dict(query=query), headers=sparql_request_header)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            if 'title' in statement:\n",
    "                title = statement['title']['value']\n",
    "                #print('title=',title)\n",
    "            else:\n",
    "                title = ''\n",
    "            if 'pmid' in statement:\n",
    "                pmid = statement['pmid']['value']\n",
    "            else:\n",
    "                pmid = ''\n",
    "            results_list.append({'title': title, 'pmid': pmid})\n",
    "    except:\n",
    "        results_list = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results_list\n",
    "\n",
    "# returns lists of occupations, employers, and affiliations for a person with Wikidata ID qid\n",
    "def search_wikidata_occ_emp_aff(qid):\n",
    "    results_list = []\n",
    "\n",
    "    query_string = '''select distinct ?occupation ?employer ?affiliation where {\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P108 ?employerId.\n",
    "            ?employerId rdfs:label ?employer.\n",
    "            FILTER(lang(?employer) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P1416 ?affiliationId.\n",
    "            ?affiliationId rdfs:label ?affiliation.\n",
    "            FILTER(lang(?affiliation) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "        }'''\n",
    "    \n",
    "    #print(query_string)\n",
    "    statements = send_sparql_query(query_string)\n",
    "    #print(statements)\n",
    "    \n",
    "    # pull all possible occupations\n",
    "    occupationList = []\n",
    "    employerList = []\n",
    "    affiliationList = []\n",
    "    for statement in statements:\n",
    "        if 'occupation' in statement:\n",
    "            occupationList.append(statement['occupation']['value'])\n",
    "        if 'employer' in statement:\n",
    "            employerList.append(statement['employer']['value'])\n",
    "        if 'affiliation' in statement:\n",
    "            affiliationList.append(statement['affiliation']['value'])\n",
    "    occupationList = list(set(occupationList))\n",
    "    employerList = list(set(employerList))\n",
    "    affiliationList = list(set(affiliationList))\n",
    "    #print(occupationList)\n",
    "    #print(employerList)\n",
    "    #print(affiliationList)\n",
    "    \n",
    "    # delay to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return occupationList, employerList, affiliationList \n",
    "\n",
    "# ------------------------\n",
    "# PubMed-related functions\n",
    "# ------------------------\n",
    "\n",
    "def retrieve_pubmed_id(doi):\n",
    "    fetch_url = 'https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/'\n",
    "    param_dict = {\n",
    "        'tool': tool_name, \n",
    "        'email': email_address,\n",
    "        'format': 'json',\n",
    "        'ids': doi\n",
    "    }\n",
    "    response = requests.get(fetch_url, params=param_dict)    \n",
    "    #print(response.url)\n",
    "    if response.status_code == 404:\n",
    "        pmid = '' # return an empty string if the constructed URL won't dereference\n",
    "    else:\n",
    "        try:\n",
    "            response_json = response.json()\n",
    "            pmid = response_json['records'][0]['pmid']\n",
    "        except:\n",
    "            pmid = ''\n",
    "\n",
    "    # See https://www.ncbi.nlm.nih.gov/books/NBK25497/ for usage guidelines. \n",
    "    # An API key is required for more than 3 requests per second.\n",
    "    sleep(0.35) # wait half a second before hitting the API again to avoid getting blocked\n",
    "    return pmid\n",
    "\n",
    "def retrieve_pubmed_data(pmid):\n",
    "    fetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    param_dict = {\n",
    "        'tool': tool_name, \n",
    "        'email': email_address,\n",
    "        'db': 'pubmed', \n",
    "         #'retmode': 'xml', \n",
    "        'rettype': 'abstract', \n",
    "        'id': pmid\n",
    "    }\n",
    "    response = requests.get(fetch_url, params=param_dict)    \n",
    "    #print(response.url)\n",
    "    if response.status_code == 404:\n",
    "        affiliations = [] # return an empty list if the constructed URL won't dereference\n",
    "    else:\n",
    "        pubData = response.text  # the response text is XML\n",
    "        #print(pubData)  # uncomment this line to see the XML\n",
    "\n",
    "        # process the returned XML, see https://docs.python.org/2/library/xml.etree.elementtree.html\n",
    "        root = et.fromstring(pubData)\n",
    "        try:\n",
    "            title = root.findall('.//ArticleTitle')[0].text\n",
    "        except:\n",
    "            title = ''\n",
    "        names = root.findall('.//Author')\n",
    "        affiliations = []\n",
    "        for name in names:\n",
    "            try:\n",
    "                affiliation = name.find('./AffiliationInfo/Affiliation').text\n",
    "            except:\n",
    "                affiliation = ''\n",
    "            try:\n",
    "                surname = name.find('./LastName').text\n",
    "            except:\n",
    "                surname = ''\n",
    "            try:\n",
    "                forename = name.find('./ForeName').text\n",
    "            except:\n",
    "                forename = ''\n",
    "            try:\n",
    "                id_field = name.find('./Identifier')\n",
    "                if id_field.get('Source') == 'ORCID':\n",
    "                    orcid = id_field.text\n",
    "                else:\n",
    "                    orcid = ''\n",
    "            except:\n",
    "                orcid = ''\n",
    "\n",
    "            #print(lastName)\n",
    "            #print(affiliation)\n",
    "            affiliations.append({'affiliation': affiliation, 'surname': surname, 'forename': forename, 'orcid': orcid})\n",
    "        #print()\n",
    "\n",
    "    # See https://www.ncbi.nlm.nih.gov/books/NBK25497/ for usage guidelines. \n",
    "    # An API key is required for more than 3 requests per second.\n",
    "    sleep(0.35) # wait before hitting the API again to avoid getting blocked\n",
    "    #print(affiliations)\n",
    "    return affiliations\n",
    "\n",
    "# NOTE: The affiliation is a list of strings. All other arguments are strings\n",
    "def identified_in_pubmed(pmid, name, affiliations, orcid):\n",
    "    department_test_ratio = 70 # ratio required when a generic name similarity is crosschecked with dept name\n",
    "    test_ratio = 90 # similarity required for a potential match of a generic wikidata match\n",
    "    screen = False\n",
    "    potentialOrcid = ''\n",
    "\n",
    "    #print('Checking authors in PubMed article: ', pmid)\n",
    "    pubmed_authors = retrieve_pubmed_data(pmid)\n",
    "    if pubmed_authors == []:\n",
    "        print('PubMed ID does not seem to be valid.')\n",
    "    #print(pubmed_authors)\n",
    "    for pubmed_author in pubmed_authors:\n",
    "        # Perform a check based on pubmed_author surnames and departments. \n",
    "        # Note: only SURNAME is checked, so coauthor problems are possible as above.\n",
    "        # More complex checking could be done by looking up the name in ORCID, if available.\n",
    "        # Always report, but only match when person and department names are similar.\n",
    "        name_test_ratio = fuzz.token_set_ratio(pubmed_author['surname'], name)\n",
    "        #print(nameTestRatio, pubmed_author['surname'])\n",
    "        if name_test_ratio >= test_ratio:\n",
    "            if pubmed_author['orcid'] != '' and orcid != '':\n",
    "                # both employee and pubmed_author must have ORCIDs to do this check\n",
    "                if orcid != extract_local_name(pubmed_author['orcid']):\n",
    "                    # Reject the article if the matched surname has an inconsistent ORCID\n",
    "                    print('*** ' + pubmed_author['forename'] + ' ' + pubmed_author['surname'] + ' is NOT the same person; ORCID ' + pubmed_author['orcid'] + ' does not match.')\n",
    "                    return screen\n",
    "                # If the PubMed metadata gives an ORCID for the matched person, record it\n",
    "                else:\n",
    "                    print(pubmed_author['forename'] + ' ' + pubmed_author['surname'] + ' has matching ORCID ' + pubmed_author['orcid'])\n",
    "                    screen = True\n",
    "                    return screen # don't continue the loop since ORCIDs match\n",
    "\n",
    "            # If there is an affiliation, display it. \n",
    "            # If the department name matches the affiliation, call it a match\n",
    "            if pubmed_author['affiliation'] != '': \n",
    "                for affiliation in affiliations:\n",
    "                    set_ratio = fuzz.token_set_ratio(affiliation, pubmed_author['affiliation'])\n",
    "                    print('Affiliation test: ', set_ratio, pubmed_author['affiliation'])\n",
    "                    if set_ratio >= department_test_ratio:\n",
    "                        print('*** pubmed_author/affiliation match!')\n",
    "                        screen = True\n",
    "                        return screen # don't continue the loop (look up pubmed_author) since it's an affiliation match\n",
    "\n",
    "    return screen\n",
    "\n",
    "# ------------------------\n",
    "# Complex functions\n",
    "# ------------------------\n",
    "\n",
    "def disambiguate_authors(doi, authors, pmid):\n",
    "    filename = 'researchers.csv'\n",
    "    researchers = read_dicts_from_csv(file_path + filename)\n",
    "\n",
    "    filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "    altnames = read_dicts_from_csv(file_path + filename)\n",
    "\n",
    "    filename = 'departments.csv'\n",
    "    departments = read_dicts_from_csv(file_path + filename)\n",
    "\n",
    "    filename = 'department_labels.csv'\n",
    "    department_labels = read_dicts_from_csv(file_path + filename)\n",
    "\n",
    "    max_pmids_to_check = 10\n",
    "    # If there is a PubMed ID for the article, retrieve the author info\n",
    "    if pmid != '':\n",
    "        pubmed_author_info = retrieve_pubmed_data(pmid)\n",
    "        print('retrieved data from PubMed ID', pmid)\n",
    "        for author_index in range(len(pubmed_author_info)):\n",
    "            pubmed_author_info[author_index]['name'] = pubmed_author_info[author_index]['forename'] + ' ' + pubmed_author_info[author_index]['surname']\n",
    "    else:\n",
    "        print('no PubMed data')\n",
    "\n",
    "    #crossref_results['pmid'] = pmid\n",
    "\n",
    "    # Augment CrossRef data with PubMed data. Typically the PubMed data is more likely to have the affiliations\n",
    "    # Names are generally very similar, but vary with added or missing periods on initials and suffixes\n",
    "    if pmid != '':\n",
    "        for author_index in range(len(authors)):\n",
    "            found = False\n",
    "            crossref_name = authors[author_index]['givenName'] + ' ' + authors[author_index]['familyName']\n",
    "            #print(crossref_name)\n",
    "            for pubmed_author in pubmed_author_info:\n",
    "                ratio = fuzz.ratio(pubmed_author['name'], crossref_name)\n",
    "                #print(ratio, pubmed_author['name'])\n",
    "                if ratio > 87: # had to drop down to this level because some people with missing \"Jr\" weren't matching\n",
    "                    found = True\n",
    "                    result_string = 'fuzzy label match: ' + str(ratio) + pubmed_author['name'] + ' / ' + crossref_name\n",
    "                    #print(result_string)\n",
    "                    break\n",
    "            if not found:\n",
    "                print('Did not find a match in the PubMed data for', crossref_name)\n",
    "            else:\n",
    "                #print(pubmed_author)\n",
    "                #print(authors[author_index])\n",
    "\n",
    "                # If there is a PubMed affiliation and no affiliation in the CrossRef data, add the PubMed affiliation\n",
    "                if pubmed_author['affiliation'] != '':\n",
    "                    if len(authors[author_index]['affiliation']) == 0:\n",
    "                        authors[author_index]['affiliation'].append(pubmed_author['affiliation'])\n",
    "\n",
    "                # If there is an ORCID in PubMed and no ORCID in the CrossRef data, add the ORCID to CrossRef data\n",
    "                # Not sure how often this happens since I think maybe usually of one has it, the other does, too.\n",
    "                if pubmed_author['orcid'] != '':\n",
    "                    if authors[author_index]['orcid'] == '':\n",
    "                        authors[author_index]['orcid'] = pubmed_author['orcid']\n",
    "\n",
    "                #print(authors[author_index])\n",
    "\n",
    "            #print()\n",
    "    #print(json.dumps(pubmed_author_info, indent=2))\n",
    "\n",
    "\n",
    "    # screens.json is a configuration file that defines the kinds of screens to be performed on potential Q ID matches from Wikidata\n",
    "    screens = load_json_into_data_struct('screens.json')\n",
    "\n",
    "    # Perform screening operations on authors to try to determine their Q IDs\n",
    "    found_qid_values = []\n",
    "    not_found_author_list = []\n",
    "    author_count = 1\n",
    "    for author in authors:\n",
    "        print(author_count)\n",
    "        found = False\n",
    "        \n",
    "        # First eliminate the case where all of the name pieces are empty\n",
    "        if (author['givenName'] + ' ' + author['familyName']).strip() == '':\n",
    "            break\n",
    "        # screen for exact match to Wikidata labels\n",
    "        for researcher in researchers:\n",
    "            if researcher['label_en'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                found = True\n",
    "                result_string = 'researcher exact label match: ' + researcher['qid'] + ' ' + researcher['label_en']\n",
    "                name = researcher['label_en']\n",
    "                qid = researcher['qid']\n",
    "                break\n",
    "        if not found:\n",
    "            # screen for exact match to alternate names\n",
    "            for altname in altnames:\n",
    "                if altname['altLabel'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                    found = True\n",
    "                    result_string = 'researcher altname match: ' + altname['qid'] + ' ' + altname['altLabel']\n",
    "                    name = altname['altLabel']\n",
    "                    qid = altname['qid']\n",
    "                    break\n",
    "            if not found:\n",
    "                # If the researcher has an ORCID, see if it's at Wikidata\n",
    "                if author['orcid'] != '':\n",
    "                    hit = searchWikidataForQIdByOrcid(author['orcid'])\n",
    "                    if hit != {}:\n",
    "                        found = True\n",
    "                        result_string = 'Wikidata ORCID search: ' + hit['qid'] + ' ' + hit['label'] + ' / ' + hit['description']\n",
    "                        name = hit['label']\n",
    "                        qid = hit['qid']\n",
    "\n",
    "                if not found:\n",
    "                    # screen for fuzzy match to Wikidata-derived labels\n",
    "                    for researcher in researchers:\n",
    "                        # Require the surname to match the label surname exactly\n",
    "                        split_names = find_surname_givens(researcher['label_en']) # returns False if no family name\n",
    "                        if split_names: # skip names that don't have 2 parts !!! also misses non-English labels!\n",
    "                            if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                                #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                                if w_ratio > 90:\n",
    "                                    found = True\n",
    "                                    result_string = 'fuzzy label match: ' + str(w_ratio) + ' ' + researcher['qid'] + ' ' + researcher['label_en'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                    name = researcher['label_en']\n",
    "                                    qid = researcher['qid']\n",
    "                                    break\n",
    "                    if not found:\n",
    "                        # screen for fuzzy match to alternate names\n",
    "                        for altname in altnames:\n",
    "                            split_names = find_surname_givens(altname['altLabel'])\n",
    "                            if split_names: # skip names that don't have 2 parts\n",
    "                                if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                    w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                                    #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                                    if w_ratio > 90:\n",
    "                                        found = True\n",
    "                                        result_string = 'researcher altname fuzzy match: ' + str(w_ratio) + ' ' + altname['qid'] + ' ' + altname['altLabel'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                        name = altname['altLabel']\n",
    "                                        qid = altname['qid']\n",
    "                                        break\n",
    "                        if not found:\n",
    "                            name = author['givenName'] + ' ' + author['familyName']\n",
    "                            print('Searching Wikidata for', name)\n",
    "                            print('researcher known affiliations: ', author['affiliation'])\n",
    "                            print()\n",
    "                            hits = search_name_at_wikidata(name)\n",
    "                            #print(hits)\n",
    "\n",
    "                            qids = []\n",
    "                            for hit in hits:\n",
    "                                qids.append(hit['qid'])\n",
    "                            return_list = screen_qids(qids, screens)\n",
    "                            #print(return_list)\n",
    "\n",
    "                            for hit in return_list:\n",
    "\n",
    "                                split_names = find_surname_givens(hit['label'])\n",
    "\n",
    "                                # Require the surname to match the Wikidata label surname exactly\n",
    "                                # This prevents a high fraction of fuzzy matches where the last names are similar but not the same\n",
    "                                if split_names: # skip names that don't have 2 parts\n",
    "                                    if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                        #print(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                        #print(hit)\n",
    "                                        w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                        #print('w_ratio:', w_ratio)\n",
    "                                        #ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                        #print('ratio:', ratio)\n",
    "                                        #partial_ratio = fuzz.partial_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                        #print('partial_ratio:', partial_ratio)\n",
    "                                        #token_sort_ratio = fuzz.token_sort_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                        #print('token_sort_ratio:', token_sort_ratio)\n",
    "                                        #token_set_ratio = fuzz.token_set_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                        #print('token_set_ratio:', token_set_ratio)\n",
    "\n",
    "                                        # This screen requires a high degree of similarity between the overall ORCID names and Wikidata labels\n",
    "                                        if w_ratio > 80:\n",
    "                                            print('Wikidata search fuzzy match:', w_ratio, author['givenName'] + ' ' + author['familyName'], ' / ', 'https://www.wikidata.org/wiki/'+ hit['qid'], hit['label'])\n",
    "                                            print('Wikidata description: ', hit['description'])\n",
    "\n",
    "                                            # Here we need to check Wikidata employer and affiliation and fuzzy match against known affiliations\n",
    "                                            occupations, employers, affilliations = search_wikidata_occ_emp_aff(hit['qid'])\n",
    "                                            print('occupations:', occupations)\n",
    "                                            print('employers:', employers)\n",
    "                                            print('affilliations', affilliations)\n",
    "                                            print()\n",
    "\n",
    "                                            # Perform a check of the employer to make sure we didn't miss somebody in the earlier\n",
    "                                            # string matching\n",
    "                                            for employer in employers:\n",
    "                                                if 'Vanderbilt University' in employer: # catch university and med center\n",
    "                                                    found = True\n",
    "                                                    result_string = 'Match Vanderbilt employer in Wikidata: ' + hit['qid'] + ' ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                                    qid = hit['qid']\n",
    "\n",
    "                                            # If the author doesn't have any known affiliations, there is no point in checking PubMed\n",
    "                                            if author['affiliation'] != []:\n",
    "                                                # Search Wikidata for articles written by this match\n",
    "                                                articles_in_wikidata = search_wikidata_article(hit['qid'])\n",
    "                                                #print(articles_in_wikidata)\n",
    "\n",
    "                                                # Step through articles with PubMed IDs found in Wikidata and see if the author affiliation or ORCID matches any of the articles\n",
    "                                                check = 0\n",
    "                                                for article_in_wikidata in articles_in_wikidata:\n",
    "                                                    if article_in_wikidata['pmid'] != '':\n",
    "                                                        check += 1\n",
    "                                                        if check > max_pmids_to_check:\n",
    "                                                            print('More articles, but stopping after checking', max_pmids_to_check)\n",
    "                                                            break # break out of article-checking loop\n",
    "                                                        print('Checking article, PMID:', article_in_wikidata['pmid'], article_in_wikidata['title'])\n",
    "                                                        pubmed_match = identified_in_pubmed(article_in_wikidata['pmid'], author['givenName'] + ' ' + author['familyName'], author['affiliation'], author['orcid'])\n",
    "                                                        if not pubmed_match:\n",
    "                                                            #print('no match')\n",
    "                                                            print()\n",
    "                                                        else:\n",
    "                                                            found = True\n",
    "                                                            result_string = 'PubMed affilation match: ' + hit['qid'] + ' ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                                            qid = hit['qid']\n",
    "                                                            break # break out of article-checking loop\n",
    "\n",
    "                                            if found:\n",
    "                                                break # break out of hit list loop\n",
    "                                            print()\n",
    "\n",
    "        if not found:\n",
    "            not_found_author_list.append({'author_string': author['givenName'] + ' ' + author['familyName'], 'series_ordinal': author_count})\n",
    "            print('not found:', author['givenName'] + ' ' + author['familyName'])\n",
    "\n",
    "        else:\n",
    "            found_qid_values.append({'qid': qid, 'stated_as': author['givenName'] + ' ' + author['familyName'], 'series_ordinal': author_count})\n",
    "            print(result_string)\n",
    "            for department in departments:\n",
    "                if qid == department['qid']:\n",
    "                    for department_label in department_labels:\n",
    "                        if department_label['qid'] == department['affiliation']:\n",
    "                            print(department_label['label_en'])\n",
    "                            break\n",
    "        print()\n",
    "        author_count += 1\n",
    "\n",
    "    print()\n",
    "    return found_qid_values, not_found_author_list\n",
    "\n",
    "def retrieve_spreadsheet_data():\n",
    "    # Spreadsheet has only one row\n",
    "    filename = 'spreadsheet_input_article.csv'\n",
    "    articles = read_dicts_from_csv(filename)\n",
    "    data = articles[0]\n",
    "\n",
    "    filename = 'spreadsheet_input_author.csv'\n",
    "    authors = read_dicts_from_csv(filename)\n",
    "\n",
    "    article_dict = {}\n",
    "    author_list = []\n",
    "    \n",
    "    # alt_reference is used in the case where the DOI doesn't exist or won't dereference\n",
    "    if 'alt_reference' in data:\n",
    "        alt_reference = data['alt_reference']\n",
    "    else:\n",
    "        alt_reference = ''\n",
    "\n",
    "    if 'doi' in data:\n",
    "        article_dict['doi'] = data['doi']\n",
    "    else:\n",
    "        article_dict['doi'] = ''\n",
    "        \n",
    "    for author in authors:\n",
    "        authorDict = {}\n",
    "        if 'orcid' in author:\n",
    "            authorDict['orcid'] = author['orcid']\n",
    "        else:\n",
    "            authorDict['orcid'] = ''\n",
    "        if 'sequence' in author:\n",
    "            authorDict['sequence'] = author['sequence']\n",
    "        else:\n",
    "            authorDict['sequence'] = ''\n",
    "        if 'givenName' in author:\n",
    "            authorDict['givenName'] = author['givenName']\n",
    "        else:\n",
    "            authorDict['givenName'] = ''\n",
    "        if 'familyName' in author:\n",
    "            authorDict['familyName'] = author['familyName']\n",
    "        else:\n",
    "            authorDict['familyName'] = ''\n",
    "        affiliationList = []\n",
    "        if 'affiliation' in author: # only one affiliation in table, but output as list to be compatible with DOI data\n",
    "            affiliationList.append(author['affiliation'])\n",
    "        else:\n",
    "            affiliationList.append('')\n",
    "        # if there aren't any affiliations, the list will remain empty\n",
    "        authorDict['affiliation'] = affiliationList\n",
    "        author_list.append(authorDict)\n",
    "    article_dict['authors'] = author_list\n",
    "    '''\n",
    "    if 'issued' in data:\n",
    "        issued = data['issued']['date-parts'][0]\n",
    "        issued_date = str(issued[0])\n",
    "        if len(issued) > 1:\n",
    "            if len(str(issued[1])) == 1:\n",
    "                issued_date += '-0'+ str(issued[1])\n",
    "            else:\n",
    "                issued_date += '-'+ str(issued[1])\n",
    "            if len(issued) > 2:                \n",
    "                if len(str(issued[2])) == 1:\n",
    "                    issued_date += '-0'+ str(issued[2])\n",
    "                else:\n",
    "                    issued_date += '-'+ str(issued[2])\n",
    "        article_dict['published'] = issued_date\n",
    "    else:\n",
    "        article_dict['published'] = ''\n",
    "    '''\n",
    "    if 'published' in data:\n",
    "        article_dict['published'] = data['published']\n",
    "    else:\n",
    "        article_dict['published'] = ''            \n",
    "    if 'volume' in data:\n",
    "        article_dict['volume'] = data['volume']\n",
    "    else:\n",
    "        article_dict['volume'] = ''\n",
    "    if 'issue' in data:\n",
    "        article_dict['issue'] = data['issue']\n",
    "    else:\n",
    "        article_dict['issue'] = ''\n",
    "    if 'page' in data:\n",
    "        article_dict['page'] = data['page']\n",
    "    else:\n",
    "        article_dict['page'] = ''\n",
    "    if 'journal_issn' in data:\n",
    "        article_dict['journal_issn'] = [data['journal_issn']]\n",
    "    else:\n",
    "        article_dict['journal_issn'] = []\n",
    "    if 'label_en' in data:\n",
    "        article_dict['title_' + default_language] = data['label_en']\n",
    "        article_dict['label_' + default_language] = data['label_en']\n",
    "    else:\n",
    "        article_dict['title_' + default_language] = ''\n",
    "        article_dict['label_' + default_language] = ''\n",
    "    if 'journal_title' in data:\n",
    "        article_dict['journal_title'] = data['journal_title']\n",
    "    else:\n",
    "        article_dict['journal_title'] = ''\n",
    "\n",
    "    if 'work_type' in data:\n",
    "        found = False\n",
    "        for work_type in work_types:\n",
    "            if data['work_type'] == work_type['crossref_type_string']:\n",
    "                found = True\n",
    "                article_dict['instance_of'] = work_type['qid']\n",
    "                article_dict['description_' + default_language] = work_type['description']\n",
    "        if not found:\n",
    "            article_dict['instance_of'] = ''\n",
    "            article_dict['description_' + default_language] = ''\n",
    "    else:\n",
    "        article_dict['instance_of'] = ''\n",
    "        article_dict['description_' + default_language] = ''\n",
    "\n",
    "    return article_dict, alt_reference\n",
    "\n",
    "# ------------------------\n",
    "# Top level composite functions\n",
    "# ------------------------\n",
    "\n",
    "def create_article_record(doi, articles_list, stored_retrieved_authors):\n",
    "    doi = doi.upper()\n",
    "    today = generate_utc_date()\n",
    "    data_available = False\n",
    "\n",
    "    # Retrieve data from file - the data is used for cases where there is no DOI to be used \n",
    "    # to acquire the info from CrossRef\n",
    "    alt_reference = '' # Default to no alternative reference, i.e. use the DOI IRI\n",
    "    if doi == '':\n",
    "        crossref_results, alt_reference = retrieve_spreadsheet_data()\n",
    "        crossref_results['doi'] = crossref_results['doi'].upper()\n",
    "        doi = crossref_results['doi']\n",
    "        data_available = True\n",
    "\n",
    "    # Retrieve data from CrossRef\n",
    "    else:\n",
    "        # Retrieve CrossRef data using DOI\n",
    "        crossref_results = retrieve_crossref_data(doi)\n",
    "        if crossref_results == {}:\n",
    "            print('No data available from CrossRef for', doi)\n",
    "        else:\n",
    "            print('retrieved data from CrossRef for', doi)\n",
    "            # screen for missing data that would crash the script and log problems\n",
    "            if not 'authors' in crossref_results:\n",
    "                print(doi + ' no author data')\n",
    "                print(doi + ' no author data\\n', file=log_object) # log to file\n",
    "            else:\n",
    "                if not 'label_' + default_language in crossref_results or crossref_results['label_' + default_language] == '':\n",
    "                    print(doi + ' title missing')\n",
    "                    print(doi + ' title missing\\n', file=log_object) # log to file\n",
    "                else:\n",
    "                    if len(crossref_results['label_' + default_language]) > 250:\n",
    "                        print(doi + ' label too long')\n",
    "                        print(doi + ' label too long\\n', file=log_object) # log to file              \n",
    "                    else:\n",
    "                        data_available = True\n",
    "\n",
    "        #print(json.dumps(crossref_results, indent = 2))\n",
    "\n",
    "    if data_available:        \n",
    "        # Determine the PubMed ID if there is one\n",
    "        pmid = retrieve_pubmed_id(doi)\n",
    "        crossref_results['pmid'] = pmid\n",
    "\n",
    "        # Extract metadata and create a record for the article\n",
    "        primary_metadata = extract_doi_metadata(crossref_results, doi, pmid, today, alt_reference)\n",
    "        articles_list.append(primary_metadata)\n",
    "        #print(articles_list)\n",
    "\n",
    "        # Writ the data to the file after every lookup in case the script crashes\n",
    "        fieldnames = list(articles_list[0].keys()) # get field names from first dict in list\n",
    "        write_dicts_to_csv(articles_list, file_path + 'articles.csv', fieldnames)\n",
    "\n",
    "        # Store the author data for this article.\n",
    "        # It will be retrieved from the file after the basic article metadata are writtend and the Q ID is available\n",
    "        stored_retrieved_authors.append({'doi': crossref_results['doi'], 'authors': json.dumps(crossref_results['authors'])})\n",
    "        write_dicts_to_csv(stored_retrieved_authors, file_path + 'stored_retrieved_authors.csv', ['doi', 'authors'])\n",
    "        \n",
    "    return articles_list, stored_retrieved_authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve DOI data from CrossRef to create new article items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved data from CrossRef for 10.1371/JOURNAL.PBIO.3001417\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This script does not (yet) check the DOI list for duplicates! They only get caught when there is an error \n",
    "# reported from the Wikidata API when VanderBot tries to upload a duplicate label/description combination.\n",
    "\n",
    "# *** For now, hard-code logging to crossref_errors.txt\n",
    "log_path = 'crossref_errors.txt'\n",
    "log_object = open(log_path, 'wt', encoding='utf-8')\n",
    "\n",
    "# *** For now, hard-code data file locations since it's a Jupyter notebook\n",
    "#file_path = '/users/baskausj/github/vandycite/divinity_law/'\n",
    "# Test users should comment out the line above and uncomment the following line:\n",
    "file_path = ''\n",
    "\n",
    "# This file contains the DOIs to be processed. The file can contain any columns, but the one with the Q IDs\n",
    "# must have the column header \"qid\"\n",
    "doi_source = read_dicts_from_csv(file_path + 'doi_source.csv')\n",
    "\n",
    "# This file contains any existing article items from previous work. \n",
    "articles_list = read_dicts_from_csv(file_path + 'articles.csv')\n",
    "\n",
    "# Create a list to store author data retrieved from CrossRef. \n",
    "# Don't need to retrieve the existing list as that will result in creating duplicate author item lines.\n",
    "#stored_retrieved_authors = read_dicts_from_csv('stored_retrieved_authors.csv')\n",
    "stored_retrieved_authors = []\n",
    "\n",
    "for doi_dict in doi_source:\n",
    "    doi = doi_dict['doi'].strip() # remove any leading or trailing whitespace\n",
    "    if doi_in_wikidata(doi):\n",
    "        print(doi, 'article already in Wikidata')\n",
    "        print(doi + ' article already in Wikidata\\n', file=log_object) # log to file\n",
    "    else:\n",
    "        # The article data and retrieved author data get passed to the function, then returned with the record for\n",
    "        # the DOI appended to the end\n",
    "        articles_list, stored_retrieved_authors = create_article_record(doi, articles_list, stored_retrieved_authors)\n",
    "\n",
    "# Potentially there may need to be logging done during the author writing stage, but we'll need to be able\n",
    "# to look at the log before writing the articles anyway.\n",
    "log_object.close()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interlude\n",
    "\n",
    "After running the code above, the VanderBot script must be run on the `articles.csv` file.\n",
    "\n",
    "The following code must be run, and then run the VanderBot script again to add the author and author string data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10.1371/JOURNAL.PBIO.3001417\n",
      "retrieved data from PubMed ID 34699520\n",
      "1\n",
      "Searching Wikidata for Asia K. Miller\n",
      "researcher known affiliations:  ['Vanderbilt University, Department of Biological Sciences, Nashville, Tennessee, United States of America.']\n",
      "\n",
      "Wikidata search fuzzy match: 86 Asia K. Miller  /  https://www.wikidata.org/wiki/Q3600368 A. Miller\n",
      "Wikidata description:  British association football player\n",
      "occupations: ['association football player']\n",
      "employers: []\n",
      "affilliations []\n",
      "\n",
      "\n",
      "Wikidata search fuzzy match: 86 Asia K. Miller  /  https://www.wikidata.org/wiki/Q89611330 A Miller\n",
      "Wikidata description:  researcher\n",
      "occupations: ['researcher']\n",
      "employers: ['Staffordshire University']\n",
      "affilliations []\n",
      "\n",
      "Checking article, PMID: 29231092 Test-retest reliability of the irrational performance beliefs inventory.\n",
      "Affiliation test:  56 a School of Life Sciences , Staffordshire University , Stoke on Trent , UK.\n",
      "\n",
      "\n",
      "not found: Asia K. Miller\n",
      "\n",
      "2\n",
      "Searching Wikidata for Camille S. Westlake\n",
      "researcher known affiliations:  ['Vanderbilt University, Department of Biological Sciences, Nashville, Tennessee, United States of America.']\n",
      "\n",
      "not found: Camille S. Westlake\n",
      "\n",
      "3\n",
      "Wikidata ORCID search: Q77516238 Karissa Cross / biology researcher\n",
      "\n",
      "4\n",
      "Searching Wikidata for Brittany A. Leigh\n",
      "researcher known affiliations:  ['Vanderbilt University, Department of Biological Sciences, Nashville, Tennessee, United States of America.']\n",
      "\n",
      "Wikidata search fuzzy match: 95 Brittany A. Leigh  /  https://www.wikidata.org/wiki/Q77516371 Brittany Leigh\n",
      "Wikidata description:  biology researcher\n",
      "occupations: ['researcher']\n",
      "employers: ['Vanderbilt University']\n",
      "affilliations ['Vanderbilt Department of Biological Sciences']\n",
      "\n",
      "Checking article, PMID: 32975515 Symbiont-mediated cytoplasmic incompatibility: what have we learned in 50 years?\n",
      "Affiliation test:  100 Department of Biological Sciences, Vanderbilt University, Nashville, United States.\n",
      "*** pubmed_author/affiliation match!\n",
      "PubMed affilation match: Q77516371 Brittany A. Leigh\n",
      "\n",
      "5\n",
      "researcher exact label match: Q45943775 Seth R. Bordenstein\n",
      "Vanderbilt Department of Biological Sciences\n",
      "Vanderbilt Institute for Infection, Immunology, and Inflammation\n",
      "Vanderbilt University School of Medicine\n",
      "\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# NOTE: currently only DOI data is being used here. alternative references will eventually need to be supported\n",
    "alt_reference =''\n",
    "\n",
    "today = generate_utc_date()\n",
    "\n",
    "# Load existing data if any (primarily if script crashes and has to be rerun)\n",
    "authors_list = read_dicts_from_csv(file_path + 'authors.csv')\n",
    "author_strings_list = read_dicts_from_csv(file_path + 'author_strings.csv')\n",
    "\n",
    "# Open the file containing the stored data about authors retrieved from CrossRef\n",
    "stored_retrieved_authors = read_dicts_from_csv(file_path + 'stored_retrieved_authors.csv')\n",
    "\n",
    "# Open the article items file after upload in order to get the Q IDs for the newly written articles\n",
    "articles = read_dicts_from_csv(file_path + 'articles.csv')\n",
    "\n",
    "for article in articles:\n",
    "    qid = article['qid']\n",
    "    doi = article['doi']\n",
    "    print(qid, doi)\n",
    "    pmid = article['pmid']\n",
    "    \n",
    "    found = False\n",
    "    for article_authors in stored_retrieved_authors:\n",
    "        if article['doi'] == article_authors['doi']:\n",
    "            found = True\n",
    "            authors = json.loads(article_authors['authors'])\n",
    "            break\n",
    "    if found:\n",
    "        # Disambiguate authors against existing Wikidata people items\n",
    "        found_author_qids, author_name_strings = disambiguate_authors(doi, authors, pmid)\n",
    "\n",
    "        for author in found_author_qids:\n",
    "            out_dict = {}\n",
    "            out_dict['qid'] = qid\n",
    "            out_dict['label_en'] = article['label_en']\n",
    "            out_dict['author_uuid'] = ''\n",
    "            out_dict['author'] = author['qid']\n",
    "            out_dict['author_series_ordinal'] = author['series_ordinal']\n",
    "            out_dict['author_stated_as'] = author['stated_as']\n",
    "            out_dict['author_ref1_hash'] = ''\n",
    "            if alt_reference == '':\n",
    "                out_dict['author_ref1_referenceUrl'] = 'http://doi.org/' + doi\n",
    "            else:\n",
    "                out_dict['author_ref1_referenceUrl'] = alt_reference\n",
    "            out_dict['author_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict['author_ref1_retrieved_val'] = today\n",
    "            out_dict['author_ref1_retrieved_prec'] = ''\n",
    "            authors_list.append(out_dict)\n",
    "        #print(authors_list)\n",
    "        \n",
    "        if len(authors_list) > 0:\n",
    "            fieldnames = list(authors_list[0].keys()) \n",
    "            write_dicts_to_csv(authors_list, file_path + 'authors.csv', fieldnames)\n",
    "\n",
    "        for author in author_name_strings:\n",
    "            out_dict = {}\n",
    "            out_dict['qid'] = qid\n",
    "            out_dict['label_en'] = article['label_en']\n",
    "            out_dict['author_string_uuid'] = ''\n",
    "            out_dict['author_string'] = author['author_string']\n",
    "            out_dict['author_string_series_ordinal'] = author['series_ordinal']\n",
    "            out_dict['author_string_ref1_hash'] = ''\n",
    "            if alt_reference == '':\n",
    "                out_dict['author_string_ref1_referenceUrl'] = 'http://doi.org/' + doi\n",
    "            else:\n",
    "                out_dict['author_string_ref1_referenceUrl'] = alt_reference\n",
    "            out_dict['author_string_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict['author_string_ref1_retrieved_val'] = today\n",
    "            out_dict['author_string_ref1_retrieved_prec'] = ''\n",
    "            author_strings_list.append(out_dict)\n",
    "\n",
    "        #print(author_strings_list)\n",
    "        if len(author_strings_list) > 0:\n",
    "            fieldnames = list(author_strings_list[0].keys()) \n",
    "            write_dicts_to_csv(author_strings_list, file_path + 'author_strings.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test code\n",
    "\n",
    "The code cells below were used to build and the production code. They don't need to be run and have been retained for historical purposes.\n",
    "\n",
    "## Retrieve DOI data for existing work records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'articles.csv'\n",
    "articles = read_dicts_from_csv(filename)\n",
    "for article in articles[50:100]:\n",
    "    if article['doi'] != '':\n",
    "        crossref_results = retrieve_crossref_data(article['doi'])\n",
    "        if crossref_results != {}:        \n",
    "            crossref_results = extract_journal_qid(crossref_results)\n",
    "\n",
    "if log_path != '':\n",
    "    log_object.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of invoking PMID and Wikidata test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = '10.1111/rec3.12158'\n",
    "\n",
    "doi = doi.upper()\n",
    "print('pmid:', retrieve_pubmed_id(doi))\n",
    "print('in Wikidata?', doi_in_wikidata(doi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests of  name searches at Wikidata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name = '尼可罗·马基亚维利'\n",
    "#name = 'Nicolás Maquiavelo'\n",
    "name = 'Никколо Макиавелли'\n",
    "#generate_name_alternatives(name)\n",
    "search_name_at_wikidata(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = '10.1186/S13643-020-01393-8'\n",
    "crossref_results = retrieve_crossref_data(doi)\n",
    "for author in crossref_results['authors']:\n",
    "    name = author['givenName'] + ' ' + author['familyName']\n",
    "    hit = search_name_at_wikidata(name)\n",
    "    print(name)\n",
    "    print(hit)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests of searches for names at Vanderbilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'researchers.csv'\n",
    "researchers = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "altnames = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'departments.csv'\n",
    "departments = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'department_labels.csv'\n",
    "department_labels = read_dicts_from_csv(filename)\n",
    "\n",
    "#print(researchers[0])\n",
    "#print()\n",
    "#print(altnames[0])\n",
    "print()\n",
    "print(departments[0])\n",
    "print()\n",
    "print(department_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test query looking for coauthor matches\n",
    "\n",
    "The idea here was that potential author Wikidata items could be tested by seeing if they were coauthors with any of the existing authors.\n",
    "\n",
    "Tested the query but it didn't seem to ever find matches and I didn't test or debug thoroughly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to look for situations where one of the unlinked resarchers are coauthors of identified ones from Vanderbilt\n",
    "print('Looking for potential coauthor matches')\n",
    "query_string = '''\n",
    "select distinct ?coauthor ?label where {\n",
    "  VALUES ?researcher\n",
    "  {\n",
    "  ''' + found_qid_values + '''}\n",
    "?publication wdt:P50 ?researcher.\n",
    "?publication wdt:P50 ?coauthor.\n",
    "?coauthor rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "FILTER(?researcher != ?coauthor)\n",
    "  }\n",
    "'''\n",
    "#print(query_string)\n",
    "results = send_sparql_query(query_string)\n",
    "for author in not_found_author_list:\n",
    "    print(author['givenName'] + ' ' + author['familyName'])\n",
    "    for result in results:\n",
    "        w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], result['label'])\n",
    "        if w_ratio > 90:\n",
    "            print('fuzzy match: ' + str(w_ratio) + ' ' + result['coauthor'] + ' ' + result['label'] + ' / ' + author['givenName'] + ' ' + author['familyName'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data used in development of Q ID-screening function\n",
    "\n",
    "The hard-coded screens below can be used with the test Q IDs to screen out \"George Washington\"s of different sorts. They are an alternative to loading the screens via:\n",
    "\n",
    "```\n",
    "screens = load_json_into_data_struct('screens.json')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = ['Q586680', 'Q1406257', 'Q1508517', 'Q10288976', 'Q20539851', 'Q23', 'Q79483233', 'Q103915646', 'Q5545912']\n",
    "\n",
    "screens1 = [\n",
    "    [\n",
    "        {\n",
    "            'property': 'P31',\n",
    "            'entity': 'Q5', # use empty string if any value is allowed, or if filtering value strings\n",
    "            'lang': '',\n",
    "            'position': 'object',\n",
    "            'require': 'exclude', # options: include, exclude\n",
    "            'filter_type': '', # options: in, <, >\n",
    "            'filter_string': ''\n",
    "        },\n",
    "        {\n",
    "            'property': 'P170',\n",
    "            'entity': '',\n",
    "            'lang': '',\n",
    "            'position': 'subject',\n",
    "            'require': 'exclude', # options: include, exclude\n",
    "            'filter_type': '', # options: in, <, >\n",
    "            'filter_string': ''\n",
    "        }\n",
    "    ],[\n",
    "        {\n",
    "            'property': 'description',\n",
    "            'entity': 'American jazz trombonist',\n",
    "            'lang': 'en',\n",
    "            'position': 'object',\n",
    "            'require': 'include', # options: include, exclude\n",
    "            'filter_type': '', # options: in, <, >\n",
    "            'filter_string': ''\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "screens2 = [\n",
    "    [\n",
    "        {\n",
    "            'property': 'P31',\n",
    "            'entity': 'Q3305213', # paintings\n",
    "            'lang': '',\n",
    "            'position': 'object',\n",
    "            'require': 'include', # options: include, exclude\n",
    "            'filter_type': '', # options: in, <, >\n",
    "            'filter_string': ''\n",
    "        }\n",
    "    ],[\n",
    "        {\n",
    "            'property': 'P31',\n",
    "            'entity': 'Q179700', # sculptures\n",
    "            'lang': '',\n",
    "            'position': 'object',\n",
    "            'require': 'include', # options: include, exclude\n",
    "            'filter_type': '', # options: in, <, >\n",
    "            'filter_string': ''\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "screens = load_json_into_data_struct('screens.json')\n",
    "return_list = screen_qids(qids, screens)\n",
    "print(json.dumps(return_list, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NCBI ID converter\n",
    "```\n",
    "https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/?ids=10.1016/j.ajhg.2010.04.006&format=json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_name = ''\n",
    "last_name = ''\n",
    "if (first_name + ' ' + last_name).strip() == '':\n",
    "    print('nothing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
