{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to pull metadata from Wikidata\n",
    "\n",
    "Throughout the script I refer to \"Wikidata\" but this could be used for any Wikibase instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration section\n",
    "\n",
    "Import modules, set values, and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire-wikidata-metadata.ipynb This is part of the VandyCite project https://www.wikidata.org/wiki/Wikidata:WikiProject_VandyCite\n",
    "# (c) 2020 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf 2020-09-08\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "manage_labels_descriptions = True\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "#data_path = home + '/divinity-law/'\n",
    "data_path = '../../../vandycite/journals/'\n",
    "item_source_csv = 'journal-div-qids.csv' # put an empty string here to get the QIDs from the query rather than a file\n",
    "\n",
    "# insert any graph pattern that will screen for the Q IDs you are interested in. Must use \"?qid\" as the variable.\n",
    "item_query = '''select distinct ?qid where {\n",
    "  ?qid wdt:P195 wd:Q18563658.\n",
    "  }'''\n",
    "\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# NOTE: the following qualifier value types are currently not supported: globecoordinate, quantity, monolingualtext\n",
    "# supported types are: item, date, and string-like types (uri, string)\n",
    "\n",
    "# settings for multiprop\n",
    "label_description_language_list = []\n",
    "output_file_name = 'journals-multiprop-test.csv'\n",
    "prop_list = [\n",
    "    #{'pid': 'P108', 'variable': 'official_website', 'value_type': 'uri','qual': [{'pid': 'P580', 'variable': 'work_language', 'value_type': 'date'}]}\n",
    "    {'pid': 'P856', 'variable': 'official_website', 'value_type': 'uri','qual': [{'pid': 'P407', 'variable': 'work_language', 'value_type': 'item'}]}\n",
    "]\n",
    "\n",
    "'''\n",
    "# settings for labels/title metadata\n",
    "label_description_language_list = ['en', 'zh', 'zh-hans', 'zh-hant', 'de', 'fr']\n",
    "output_file_name = 'journals-title.csv'\n",
    "prop_list = [\n",
    "    {'pid': 'P1476', 'variable': 'title_en', 'value_type': 'monolingualtext', 'language': 'en'},\n",
    "    {'pid': 'P1476', 'variable': 'title_de', 'value_type': 'monolingualtext', 'language': 'de'},\n",
    "    {'pid': 'P1476', 'variable': 'title_la', 'value_type': 'monolingualtext', 'language': 'la'},\n",
    "    {'pid': 'P1476', 'variable': 'title_fr', 'value_type': 'monolingualtext', 'language': 'fr'},\n",
    "    {'pid': 'P1476', 'variable': 'title_it', 'value_type': 'monolingualtext', 'language': 'it'},\n",
    "    {'pid': 'P1476', 'variable': 'title_pt', 'value_type': 'monolingualtext', 'language': 'pt'},\n",
    "    {'pid': 'P1476', 'variable': 'title_es', 'value_type': 'monolingualtext', 'language': 'es'},\n",
    "    {'pid': 'P1476', 'variable': 'title_sk', 'value_type': 'monolingualtext', 'language': 'sk'},\n",
    "    {'pid': 'P1476', 'variable': 'title_hu', 'value_type': 'monolingualtext', 'language': 'hu'},\n",
    "    {'pid': 'P1476', 'variable': 'title_nb', 'value_type': 'monolingualtext', 'language': 'nb'},\n",
    "    {'pid': 'P1476', 'variable': 'title_ko', 'value_type': 'monolingualtext', 'language': 'ko'},\n",
    "    {'pid': 'P1476', 'variable': 'title_af', 'value_type': 'monolingualtext', 'language': 'af'}\n",
    "]\n",
    "'''\n",
    "\n",
    "'''\n",
    "# official website requires qualifier language of work or name (item)\n",
    "prop_list = [\n",
    "    {'pid': 'P495', 'variable': 'country_of_origin', 'value_type': 'item','qual': []},\n",
    "    {'pid': 'P571', 'variable': 'inception', 'value_type': 'date','qual': []},\n",
    "    {'pid': 'P2669', 'variable': 'discontinued_date', 'value_type': 'date','qual': []},\n",
    "    {'pid': 'P856', 'variable': 'official_website', 'value_type': 'uri','qual': [{'pid': 'P407', 'variable': 'work_language', 'value_type': 'item'}]},\n",
    "    {'pid': 'P155', 'variable': 'follows', 'value_type': 'item','qual': []},\n",
    "    {'pid': 'P156', 'variable': 'followed_by', 'value_type': 'item','qual': []},\n",
    "    {'pid': 'P2896', 'variable': 'publication_interval', 'value_type': 'quantity','qual': []}\n",
    "]\n",
    "'''\n",
    "# The following properties can contain multiple values per item, so need to be managed in separate CSVs.\n",
    "# The script needs to be rerun with each one as a single item on the prop_list.\n",
    "\n",
    "#prop_list = [\n",
    "#    {'pid': 'P123', 'variable': 'publisher', 'value_type': 'item'}\n",
    "#    {'pid': 'P1476', 'variable': 'title', 'value_type': 'monolingualtext', 'language': 'en'}\n",
    "#    {'pid': 'P31', 'variable': 'instance_of', 'value_type': 'item'}\n",
    "#    {'pid': 'P407', 'variable': 'language_of_work', 'value_type': 'item'}\n",
    "#    {'pid': 'P236', 'variable': 'issn', 'value_type': 'string'}\n",
    "#    {'pid': 'P921', 'variable': 'main_subject', 'value_type': 'item'}\n",
    "#]\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderDiv/0.1 (https://github.com/HeardLibrary/linked-data/tree/master/publications/divinity-law; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts the UUID and qId from a statement IRI\n",
    "def extract_statement_uuid(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/statement/Q7552806-8B88E0CA-BCC8-49D5-9AC2-F1755464F1A2\n",
    "    pieces = iri.split('/')\n",
    "    statement_id = pieces[5]\n",
    "    pieces = statement_id.split('-')\n",
    "    return pieces[1] + '-' + pieces[2] + '-' + pieces[3] + '-' + pieces[4] + '-' + pieces[5], pieces[0]\n",
    "\n",
    "# ----------------\n",
    "# Specialty functions\n",
    "# ----------------\n",
    "\n",
    "# function to add variables related to a property to the select clause and graph pattern of the SPARQL query\n",
    "def sparql_append_property(prop, select_prefix, graph_pattern_prefix):\n",
    "    variable_name = prop['variable']\n",
    "    prop_id = prop['pid']\n",
    "    \n",
    "    # Create the variables for the select clause of the query\n",
    "    select_prefix += ' ?' + variable_name + '_uuid '\n",
    "    if prop['value_type'] == 'date':\n",
    "        select_prefix += '?' + variable_name + '_nodeId ?' + variable_name + '_val ?' + variable_name + '_prec'\n",
    "    elif prop['value_type'] == 'quantity':\n",
    "        select_prefix += '?' + variable_name + '_nodeId ?' + variable_name + '_val ?' + variable_name + '_unit'\n",
    "    elif prop['value_type'] == 'globecoordinate':\n",
    "        select_prefix += '?' + variable_name + '_nodeId ?' + variable_name + '_val ?' + variable_name + '_long ?' + variable_name + '_prec'\n",
    "    else:\n",
    "        select_prefix += '?' + variable_name\n",
    "    select_prefix += ' ?' + variable_name + '_ref1_hash ?' + variable_name + '_ref1_statedIn ?' + variable_name + '_ref1_referenceUrl ?' + variable_name + '_ref1_retrieved_nodeId ?' + variable_name + '_ref1_retrieved_val ?' + variable_name + '_ref1_retrieved_prec '\n",
    "    for qualifier in prop['qual']:\n",
    "        if qualifier['value_type'] == 'date':\n",
    "            select_prefix += '?' + variable_name + '_' + qualifier['variable'] + '_nodeId ?' + variable_name + '_' + qualifier['variable'] + '_val ?' + variable_name + '_' + qualifier['variable'] + '_prec '\n",
    "        else:\n",
    "            select_prefix += '?' + variable_name + '_' + qualifier['variable'] + ' '\n",
    "\n",
    "    # Create the graph pattern for the query\n",
    "    graph_pattern_prefix += '''optional{\n",
    "?qid p:''' + prop['pid'] + ' ?' + variable_name + '_uuid.\\n'\n",
    "\n",
    "    if prop['value_type'] == 'date':\n",
    "        graph_pattern_prefix += '?' + variable_name + '_uuid psv:' + prop['pid'] + ' ?' + variable_name + '''_nodeId.\n",
    "?''' + variable_name + '_nodeId wikibase:timeValue ?' + variable_name + '''_val.\n",
    "?''' + variable_name + '_nodeId wikibase:timePrecision ?' + variable_name + '_prec.\\n'\n",
    "        \n",
    "    elif prop['value_type'] == 'quantity':\n",
    "        graph_pattern_prefix += '?' + variable_name + '_uuid psv:' + prop['pid'] + ' ?' + variable_name + '''_nodeId.\n",
    "?''' + variable_name + '_nodeId wikibase:quantityAmount ?' + variable_name + '''_val.\n",
    "?''' + variable_name + '_nodeId wikibase:quantityUnit ?' + variable_name + '_unit.\\n'\n",
    "        \n",
    "    elif prop['value_type'] == 'globecoordinate':\n",
    "        graph_pattern_prefix += '?' + variable_name + '_uuid psv:' + prop['pid'] + ' ?' + variable_name + '''_nodeId.\n",
    "?''' + variable_name + '_nodeId wikibase:geoLatitude ?' + variable_name + '''_val.\n",
    "?''' + variable_name + '_nodeId wikibase:geoLongitude ?' + variable_name + '''_long.\n",
    "?''' + variable_name + '_nodeId wikibase:geoPrecision ?' + variable_name + '_prec.\\n'\n",
    "        \n",
    "    elif prop['value_type'] == 'monolingualtext':\n",
    "        graph_pattern_prefix += '?' + variable_name + '_uuid ps:' + prop['pid'] + ' ?' + variable_name + '''.\n",
    "filter(lang(?''' + variable_name + ')=\"' + prop['language'] + '\")\\n'\n",
    "        \n",
    "    else:\n",
    "        graph_pattern_prefix += '?' + variable_name + '_uuid ps:' + prop['pid'] + ' ?' + variable_name + '.\\n'\n",
    "\n",
    "    graph_pattern_prefix += '''\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid prov:wasDerivedFrom ?''' + variable_name + '''_ref1_hash.\n",
    "?''' + variable_name + '_ref1_hash pr:P248 ?' + variable_name + '''_ref1_statedIn.\n",
    "}\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid prov:wasDerivedFrom ?''' + variable_name + '''_ref1_hash.\n",
    "?''' + variable_name + '_ref1_hash pr:P854 ?' + variable_name + '''_ref1_referenceUrl.\n",
    "}\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid prov:wasDerivedFrom ?''' + variable_name + '''_ref1_hash.\n",
    "?''' + variable_name + '_ref1_hash prv:P813 ?' + variable_name + '''_ref1_retrieved_nodeId.\n",
    "?''' + variable_name + '_ref1_retrieved_nodeId wikibase:timeValue ?' + variable_name + '''_ref1_retrieved_val.\n",
    "?''' + variable_name + '_ref1_retrieved_nodeId wikibase:timePrecision ?' + variable_name + '''_ref1_retrieved_prec.\n",
    "}\n",
    "'''\n",
    "    for qualifier in prop['qual']:\n",
    "        if qualifier['value_type'] == 'date':\n",
    "            graph_pattern_prefix += '''\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid pqv:''' + qualifier['pid'] + ' ?' + variable_name + '_' + qualifier['variable'] + '''_nodeId.\n",
    "?''' + variable_name + '_' + qualifier['variable'] + '_nodeId wikibase:timeValue ?' + variable_name + '_' + qualifier['variable'] + '''_val.\n",
    "?''' + variable_name + '_' + qualifier['variable'] + '_nodeId wikibase:timePrecision ?' + variable_name + '_' + qualifier['variable'] + '''_prec.\n",
    "}'''\n",
    "        else:\n",
    "            graph_pattern_prefix += '''\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid pq:'''+ qualifier['pid'] + ''' ?''' + variable_name + '_' + qualifier['variable'] +'''.\n",
    "}\n",
    "'''\n",
    "    graph_pattern_prefix += '}\\n'\n",
    "    return select_prefix, graph_pattern_prefix\n",
    "\n",
    "# function to add columns to column list for CSV header\n",
    "def csv_header_append(prop, header_list):\n",
    "    variable_name = prop['variable']\n",
    "\n",
    "    header_list.append(variable_name + '_uuid')\n",
    "    if prop['value_type'] == 'date':\n",
    "        header_list.append(variable_name + '_nodeId')\n",
    "        header_list.append(variable_name + '_val')\n",
    "        header_list.append(variable_name + '_prec')\n",
    "    elif prop['value_type'] == 'quantity':\n",
    "        header_list.append(variable_name + '_nodeId')\n",
    "        header_list.append(variable_name + '_val')\n",
    "        header_list.append(variable_name + '_unit')\n",
    "    elif prop['value_type'] == 'globecoordinate':\n",
    "        header_list.append(variable_name + '_nodeId')\n",
    "        header_list.append(variable_name + '_val')\n",
    "        header_list.append(variable_name + '_long')\n",
    "        header_list.append(variable_name + '_prec')\n",
    "    else:\n",
    "        header_list.append(variable_name)\n",
    "    header_list.append(variable_name + '_ref1_hash')\n",
    "    header_list.append(variable_name + '_ref1_statedIn')\n",
    "    header_list.append(variable_name + '_ref1_referenceUrl')\n",
    "    header_list.append(variable_name + '_ref1_retrieved_nodeId')\n",
    "    header_list.append(variable_name + '_ref1_retrieved_val')\n",
    "    header_list.append(variable_name + '_ref1_retrieved_prec')\n",
    "    for qualifier in prop['qual']:\n",
    "        if qualifier['value_type'] == 'date':\n",
    "            header_list.append(variable_name + '_' + qualifier['variable'] + '_nodeId')\n",
    "            header_list.append(variable_name + '_' + qualifier['variable'] + '_val')\n",
    "            header_list.append(variable_name + '_' + qualifier['variable'] + '_prec')\n",
    "        else:\n",
    "            header_list.append(variable_name + '_' + qualifier['variable'])\n",
    "\n",
    "    return header_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of items from file (or generate by query) and construct Q ID list for query\n",
    "\n",
    "The CSV has a header row with column headers: `qid` and `label`. The `qid` column contains the Wikidata Q identifiers for each item. The `label` column contains the label, which isn't necessarily the label in Wikidata and isn't use for anything in the script. It does provide a way for humans to recognize the item when looking at the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if item_source_csv == '':\n",
    "    # send request to Wikidata Query Service\n",
    "    print('querying SPARQL endpoint')\n",
    "    response = requests.post(endpoint, data=item_query, headers=requestheader)\n",
    "    #print(response.text)\n",
    "    data = response.json()\n",
    "    print('results returned')\n",
    "\n",
    "    # extract the values from the response JSON\n",
    "    items = data['results']['bindings']\n",
    "    #print(results)\n",
    "else:\n",
    "    # Load item data from csv\n",
    "    print('loading item data from file')\n",
    "    filename = data_path + item_source_csv\n",
    "    items = read_dict(filename)\n",
    "    print('done loading')\n",
    "\n",
    "# Create VALUES list for items\n",
    "item_qids = ''\n",
    "for item in items:\n",
    "    item_qids += 'wd:' + item['qid'] + '\\n'\n",
    "# remove trailing newline\n",
    "item_qids = item_qids[:len(item_qids)-1]\n",
    "\n",
    "#print(item_qids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SPARQL query to get the property statements for the items on the Q IDs list\n",
    "select_variable_list = ''\n",
    "graph_pattern = ''\n",
    "for prop in prop_list:\n",
    "    select_variable_list, graph_pattern = sparql_append_property(prop, select_variable_list, graph_pattern)\n",
    "query = '''\n",
    "select distinct ?qid '''\n",
    "\n",
    "# note: dashes not allowed in SPARQL variable names, so replace with underscores\n",
    "for label_description_language in label_description_language_list:\n",
    "    query +='?label_' + label_description_language.replace('-', '_') + ' ?description_' + label_description_language.replace('-', '_') + ' '\n",
    "query += select_variable_list + 'where {'\n",
    "\n",
    "query += '''\n",
    "  VALUES ?qid\n",
    "{\n",
    "''' + item_qids + '''\n",
    "}\n",
    "'''\n",
    "# made label and description optional since some don't have in English\n",
    "for label_description_language in label_description_language_list:\n",
    "    query += '''\n",
    "optional {\n",
    "?qid rdfs:label ?label_''' + label_description_language.replace('-', '_') + '''.\n",
    "filter(lang(?label_''' + label_description_language.replace('-', '_') + ')=\"' + label_description_language + '''\")\n",
    "}\n",
    "optional {\n",
    "?qid schema:description ?description_''' + label_description_language.replace('-', '_') + '''.\n",
    "filter(lang(?description_''' + label_description_language.replace('-', '_') + ')=\"' + label_description_language + '''\")\n",
    "}\n",
    "'''\n",
    "\n",
    "query += graph_pattern + '''\n",
    "}'''\n",
    "\n",
    "#print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request to Wikidata Query Service\n",
    "print('querying SPARQL endpoint')\n",
    "response = requests.post(endpoint, data=query, headers=requestheader)\n",
    "#print(response.text)\n",
    "data = response.json()\n",
    "\n",
    "# extract the values from the response JSON\n",
    "results = data['results']['bindings']\n",
    "\n",
    "print('done retrieving data')\n",
    "# print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract results\n",
    "metadata_list = []\n",
    "for result in results:\n",
    "    row_dict = {}\n",
    "    row_dict['qid'] = extract_qnumber(result['qid']['value'])\n",
    "    for label_description_language in label_description_language_list:\n",
    "        try:\n",
    "            row_dict['label_' + label_description_language] = result['label_' + label_description_language.replace('-', '_')]['value']\n",
    "        except:\n",
    "            row_dict['label_' + label_description_language] = ''\n",
    "        if manage_labels_descriptions:\n",
    "            try:\n",
    "                row_dict['description_' + label_description_language] = result['description_' + label_description_language.replace('-', '_')]['value']\n",
    "            except:\n",
    "                row_dict['description_' + label_description_language] = ''           \n",
    "    for property in prop_list:\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_uuid'], trash = extract_statement_uuid(result[property['variable'] + '_uuid']['value'])\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_uuid'] = ''\n",
    "        try:\n",
    "            if property['value_type'] == 'item':\n",
    "                row_dict[property['variable']] = extract_qnumber(result[property['variable']]['value'])\n",
    "            elif property['value_type'] == 'date':\n",
    "                row_dict[property['variable'] + '_nodeId'] = extract_qnumber(result[property['variable'] + '_nodeId']['value'])\n",
    "                row_dict[property['variable'] + '_val'] = result[property['variable'] + '_val']['value']\n",
    "                row_dict[property['variable'] + '_prec'] = result[property['variable'] + '_prec']['value']\n",
    "            elif property['value_type'] == 'quantity':\n",
    "                row_dict[property['variable'] + '_nodeId'] = extract_qnumber(result[property['variable'] + '_nodeId']['value'])\n",
    "                row_dict[property['variable'] + '_val'] = result[property['variable'] + '_val']['value']\n",
    "                row_dict[property['variable'] + '_unit'] = result[property['variable'] + '_unit']['value']\n",
    "            elif property['value_type'] == 'globecoordinate':\n",
    "                row_dict[property['variable'] + '_nodeId'] = extract_qnumber(result[property['variable'] + '_nodeId']['value'])\n",
    "                row_dict[property['variable'] + '_val'] = result[property['variable'] + '_val']['value']\n",
    "                row_dict[property['variable'] + '_long'] = result[property['variable'] + '_long']['value']\n",
    "                row_dict[property['variable'] + '_prec'] = result[property['variable'] + '_long']['_prec']\n",
    "            else:\n",
    "                row_dict[property['variable']] = result[property['variable']]['value']\n",
    "        except:\n",
    "            if property['value_type'] == 'date':\n",
    "                row_dict[property['variable'] + '_nodeId'] = ''\n",
    "                row_dict[property['variable'] + '_val'] = ''\n",
    "                row_dict[property['variable'] + '_prec'] = ''\n",
    "            elif property['value_type'] == 'quantity':\n",
    "                row_dict[property['variable'] + '_nodeId'] = ''\n",
    "                row_dict[property['variable'] + '_val'] = ''\n",
    "                row_dict[property['variable'] + '_unit'] = ''\n",
    "            elif property['value_type'] == 'globecoordinate':\n",
    "                row_dict[property['variable'] + '_nodeId'] = ''\n",
    "                row_dict[property['variable'] + '_val'] = ''\n",
    "                row_dict[property['variable'] + '_long'] = ''\n",
    "                row_dict[property['variable'] + '_prec'] = ''\n",
    "            else:\n",
    "                row_dict[property['variable']] = ''\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_ref1_hash'] = extract_qnumber(result[property['variable'] + '_ref1_hash']['value'])\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_ref1_hash'] = ''\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_ref1_statedIn'] = extract_qnumber(result[property['variable'] + '_ref1_statedIn']['value'])\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_ref1_statedIn'] = ''\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_ref1_referenceUrl'] = result[property['variable'] + '_ref1_referenceUrl']['value']\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_ref1_referenceUrl'] = ''\n",
    "        try:\n",
    "            # Note: the form of the node ID is http://www.wikidata.org/value/0a8f688406e3fc53d0119eafcd2c0396\n",
    "            # so the extract_qnumber() function can be used on it.\n",
    "            row_dict[property['variable'] + '_ref1_retrieved_nodeId'] = extract_qnumber(result[property['variable'] + '_ref1_retrieved_nodeId']['value'])\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_ref1_retrieved_nodeId'] = ''\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_ref1_retrieved_val'] = result[property['variable'] + '_ref1_retrieved_val']['value']\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_ref1_retrieved_val'] = ''\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_ref1_retrieved_prec'] = result[property['variable'] + '_ref1_retrieved_prec']['value']\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_ref1_retrieved_prec'] = ''\n",
    "            \n",
    "        for qualifier in property['qual']:\n",
    "            try:\n",
    "                if qualifier['value_type'] == 'date':\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable'] + '_nodeId'] = extract_qnumber(result[property['variable'] + '_' + qualifier['variable'] + '_nodeId']['value'])\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable'] + '_val'] = result[property['variable'] + '_' + qualifier['variable'] + '_val']['value']\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable'] + '_prec'] = result[property['variable'] + '_' + qualifier['variable'] + '_prec']['value']\n",
    "                elif qualifier['value_type'] == 'item':\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable']] = extract_qnumber(result[property['variable'] + '_' + qualifier['variable']]['value'])\n",
    "                else:\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable']] = result[property['variable'] + '_' + qualifier['variable']]['value']\n",
    "            except:\n",
    "                if qualifier['value_type'] == 'date':\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable'] + '_nodeId'] = ''\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable'] + '_val'] = ''\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable'] + '_prec'] = ''\n",
    "                else:\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable']] = ''\n",
    "    \n",
    "    metadata_list.append(row_dict)\n",
    "    \n",
    "# print(json.dumps(metadata_list, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of column headers\n",
    "fieldnames = ['qid']\n",
    "if manage_labels_descriptions:\n",
    "    # The schema generator puts all of the labels first, then the descriptions\n",
    "    for label_description_language in label_description_language_list:\n",
    "        fieldnames.append('label_' + label_description_language.replace('-', '_'))          \n",
    "    for label_description_language in label_description_language_list:\n",
    "        fieldnames.append('description_' + label_description_language.replace('-', '_'))          \n",
    "for prop in prop_list:\n",
    "    fieldnames = csv_header_append(prop, fieldnames)\n",
    "if not(manage_labels_descriptions):\n",
    "    fieldnames.append('label_' + label_description_language)          \n",
    "# print(fieldnames)\n",
    "\n",
    "# write the data to a CSV file\n",
    "print('writing data to file')\n",
    "write_dicts_to_csv(metadata_list, data_path + output_file_name, fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
