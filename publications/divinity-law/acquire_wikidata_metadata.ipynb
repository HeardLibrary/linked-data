{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to pull metadata from Wikidata\n",
    "\n",
    "Throughout the script I refer to \"Wikidata\" but this could be used for any Wikibase instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration section\n",
    "\n",
    "Import modules, set values, and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire-wikidata-metadata.ipynb This is part of the VandyCite project https://www.wikidata.org/wiki/Wikidata:WikiProject_VandyCite\n",
    "# (c) 2020 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf 2020-09-06\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "data_path = home + '/divinity-law/'\n",
    "item_source_csv = 'identified-journals.csv'\n",
    "output_file_name = 'test-output.csv'\n",
    "\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderDiv/0.1 (https://github.com/HeardLibrary/linked-data/tree/master/publications/divinity-law; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts the UUID and qId from a statement IRI\n",
    "def extract_statement_uuid(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/statement/Q7552806-8B88E0CA-BCC8-49D5-9AC2-F1755464F1A2\n",
    "    pieces = iri.split('/')\n",
    "    statement_id = pieces[5]\n",
    "    pieces = statement_id.split('-')\n",
    "    return pieces[1] + '-' + pieces[2] + '-' + pieces[3] + '-' + pieces[4] + '-' + pieces[5], pieces[0]\n",
    "\n",
    "# ----------------\n",
    "# Specialty functions\n",
    "# ----------------\n",
    "\n",
    "# old version of function\n",
    "def sparql_append_properties(variable_name, select_prefix, graph_pattern_prefix):\n",
    "    select_prefix += '?' + variable_name + ' ?' + variable_name + '_uuid ?' + variable_name + '_ref1_hash ?' + variable_name + '_ref1_statedIn ?' + variable_name + '_ref1_referenceUrl ?' + variable_name + '_ref1_retrieved_nodeid ?' + variable_name + '_ref1_retrieved_val ?' + variable_name + '_ref1_retrieved_prec '\n",
    "    graph_pattern_prefix += '''optional{\n",
    "?''' + variable_name + '''_uuid prov:wasDerivedFrom ?''' + variable_name + '''_ref1_hash.\n",
    "?''' + variable_name + '_ref1_hash pr:P248 ?' + variable_name + '''_ref1_statedIn.\n",
    "}\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid prov:wasDerivedFrom ?''' + variable_name + '''_ref1_hash.\n",
    "?''' + variable_name + '_ref1_hash pr:P854 ?' + variable_name + '''_ref1_referenceUrl.\n",
    "}\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid prov:wasDerivedFrom ?''' + variable_name + '''_ref1_hash.\n",
    "?''' + variable_name + '_ref1_hash prv:P813 ?' + variable_name + '''_ref1_retrieved_nodeid.\n",
    "?''' + variable_name + '_ref1_retrieved_nodeid wikibase:timeValue ?' + variable_name + '''_ref1_retrieved_val.\n",
    "?''' + variable_name + '_ref1_retrieved_nodeid wikibase:timePrecision ?' + variable_name + '''_ref1_retrieved_prec.\n",
    "}\n",
    "'''\n",
    "    return select_prefix, graph_pattern_prefix\n",
    "\n",
    "# new version of function\n",
    "def sparql_append_property(prop, select_prefix, graph_pattern_prefix):\n",
    "    variable_name = prop['variable']\n",
    "    prop_id = prop['pid']\n",
    "    select_prefix += '?' + variable_name + ' ?' + variable_name + '_uuid ?' + variable_name + '_ref1_hash ?' + variable_name + '_ref1_statedIn ?' + variable_name + '_ref1_referenceUrl ?' + variable_name + '_ref1_retrieved_nodeid ?' + variable_name + '_ref1_retrieved_val ?' + variable_name + '_ref1_retrieved_prec '\n",
    "    graph_pattern_prefix += '''optional{\n",
    "?qid p:''' + prop['pid'] + ' ?' + prop['variable'] + '''_uuid.\n",
    "?qid wdt:''' + prop['pid'] + ' ?' + prop['variable'] + '''.\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid prov:wasDerivedFrom ?''' + variable_name + '''_ref1_hash.\n",
    "?''' + variable_name + '_ref1_hash pr:P248 ?' + variable_name + '''_ref1_statedIn.\n",
    "}\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid prov:wasDerivedFrom ?''' + variable_name + '''_ref1_hash.\n",
    "?''' + variable_name + '_ref1_hash pr:P854 ?' + variable_name + '''_ref1_referenceUrl.\n",
    "}\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid prov:wasDerivedFrom ?''' + variable_name + '''_ref1_hash.\n",
    "?''' + variable_name + '_ref1_hash prv:P813 ?' + variable_name + '''_ref1_retrieved_nodeid.\n",
    "?''' + variable_name + '_ref1_retrieved_nodeid wikibase:timeValue ?' + variable_name + '''_ref1_retrieved_val.\n",
    "?''' + variable_name + '_ref1_retrieved_nodeid wikibase:timePrecision ?' + variable_name + '''_ref1_retrieved_prec.\n",
    "}}\n",
    "'''\n",
    "    return select_prefix, graph_pattern_prefix\n",
    "\n",
    "# function to add columns to column list for CSV header\n",
    "def csv_header_append(prop, header_list):\n",
    "    variable_name = prop['variable']\n",
    "\n",
    "    header_list.append(variable_name)\n",
    "    header_list.append(variable_name + '_uuid')\n",
    "    header_list.append(variable_name + '_ref1_hash')\n",
    "    header_list.append(variable_name + '_ref1_statedIn')\n",
    "    header_list.append(variable_name + '_ref1_referenceUrl')\n",
    "    header_list.append(variable_name + '_ref1_retrieved_nodeid')\n",
    "    header_list.append(variable_name + '_ref1_retrieved_val')\n",
    "    header_list.append(variable_name + '_ref1_retrieved_prec')\n",
    "\n",
    "    return header_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load list of items from file\n",
    "\n",
    "The CSV has a header row with column headers: `qid` and `label`. The `qid` column contains the Wikidata Q identifiers for each item. The `label` column contains the label, which isn't necessarily the label in Wikidata, but provides a way for humans to recognize the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load item data from csv\n",
    "print('loading item data from file')\n",
    "filename = data_path + item_source_csv\n",
    "items = read_dict(filename)\n",
    "\n",
    "# Create VALUES list for journals\n",
    "item_qids = ''\n",
    "for item in items:\n",
    "    item_qids += 'wd:' + item['qid'] + '\\n'\n",
    "# remove trailing newline\n",
    "item_qids = item_qids[:len(item_qids)-1]\n",
    "\n",
    "# create properties dictionary\n",
    "prop_list = [\n",
    "    {'pid': 'P31', 'variable': 'instance_of', 'value_type': 'item'},\n",
    "    {'pid': 'P1476', 'variable': 'title', 'value_type': 'string'},\n",
    "    {'pid': 'P407', 'variable': 'language_of_work', 'value_type': 'item'},\n",
    "    {'pid': 'P495', 'variable': 'country_of_origin', 'value_type': 'item'},\n",
    "    {'pid': 'P123', 'variable': 'publisher', 'value_type': 'item'},\n",
    "    {'pid': 'P571', 'variable': 'inception', 'value_type': 'date'},\n",
    "    {'pid': 'P2669', 'variable': 'discontinued_date', 'value_type': 'item'},\n",
    "    {'pid': 'P856', 'variable': 'official_website', 'value_type': 'uri'},\n",
    "    {'pid': 'P155', 'variable': 'follows', 'value_type': 'item'},\n",
    "    {'pid': 'P156', 'variable': 'followed_by', 'value_type': 'item'},\n",
    "    {'pid': 'P921', 'variable': 'main_subject', 'value_type': 'item'},\n",
    "    {'pid': 'P2896', 'variable': 'publication_interval', 'value_type': 'decimal'},\n",
    "    {'pid': 'P236', 'variable': 'issn', 'value_type': 'string'}\n",
    "]\n",
    "\n",
    "#print(item_qids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create query to get the statement UUIDs and values\n",
    "\n",
    "SPARQL query to be sent to the Wikidata Query Service (WDQS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property = prop_list[12]\n",
    "\n",
    "# create a string for the query\n",
    "query = '''\n",
    "select distinct ?qid '''\n",
    "\n",
    "query += '?' + property['variable'] + '_value '\n",
    "query += '?' + property['variable'] + '_statement '\n",
    "\n",
    "query += 'where {'\n",
    "\n",
    "query += '''\n",
    "  VALUES ?qid\n",
    "{\n",
    "''' + item_qids + '''\n",
    "}\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "query += '?qid wdt:' + property['pid'] + ' ?' + property['variable'] + '_value.\\n'\n",
    "query += '?qid p:' + property['pid'] + ' ?' + property['variable'] + '_statement.\\n'\n",
    "query += '}'\n",
    "\n",
    "#print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the query to the WDQS and extract the results from the returned JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send SPARQL query to the Wikidata Query Service\n",
    "print('retrieving data from Wikidata')\n",
    "\n",
    "# send request to Wikidata Query Service\n",
    "response = requests.post(endpoint, data=query, headers=requestheader)\n",
    "data = response.json()\n",
    "\n",
    "# extract the values from the response JSON\n",
    "results = data['results']['bindings']\n",
    "#print(json.dumps(results, indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the qids, values, and statement UUIDs from the response data and store in a list of dicts (`statements_list`). Note: the `extract_statement_uuid()` function returns `uuid`,`qid` extracted from the statement IRI, so they have to be re-assembled to make the statement identifier: *qid*-*UUID*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_list = [] # create list of data for statements, not really used for anything yet\n",
    "statement_uuids = '' # create VALUES list for statmements\n",
    "for result in results:\n",
    "    row_dict = {}\n",
    "    row_dict = {'qid': extract_qnumber(result['qid']['value'])}\n",
    "\n",
    "    if property['value_type'] == 'item':\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        value = extract_qnumber(result[property['variable'] + '_value']['value'])\n",
    "    else:\n",
    "        value = result[property['variable'] + '_value']['value']\n",
    "    row_dict[property['variable'] + '_value'] = value\n",
    "    uuid, temp_qid = extract_statement_uuid(result[property['variable'] + '_statement']['value'])\n",
    "    row_dict[property['variable'] + '_statement'] = uuid\n",
    "    statement_uuids += 'wds:' + temp_qid + '-' + uuid + '\\n'\n",
    "    statements_list.append(row_dict)\n",
    "\n",
    "# remove trailing newline\n",
    "statement_uuids = statement_uuids[:len(statement_uuids)-1]\n",
    "\n",
    "#print(statement_uuids)\n",
    "#print(statements_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create query to retrieve the reference metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_prefix = ''\n",
    "graph_pattern_prefix = ''\n",
    "prop = property['variable']\n",
    "\n",
    "add_select, add_graph_pattern = sparql_append_properties(prop, select_prefix, graph_pattern_prefix)\n",
    "\n",
    "# create a string for the query\n",
    "query = 'select distinct ?label '\n",
    "\n",
    "query += add_select + '''where {\n",
    "'''\n",
    "\n",
    "query += '  VALUES ?' + property['variable'] + '''_uuid\n",
    "{\n",
    "''' + statement_uuids + '''\n",
    "}\n",
    "\n",
    "'''\n",
    "\n",
    "query += '''\n",
    "?''' + property['variable'] + '_uuid ps:' + property['pid'] + ' ?' + property['variable'] + '''.\n",
    "?qid p:''' + property['pid'] + ' ?' + property['variable'] + '''_uuid.\n",
    "?qid rdfs:label ?label.\n",
    "filter(lang(?label)='en')\n",
    "'''\n",
    "\n",
    "query += add_graph_pattern + '}'\n",
    "\n",
    "# print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the reference metadata from Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request to Wikidata Query Service\n",
    "response = requests.post(endpoint, data=query, headers=requestheader)\n",
    "print(response.text)\n",
    "data = response.json()\n",
    "\n",
    "# extract the values from the response JSON\n",
    "results = data['results']['bindings']\n",
    "print('done retrieving reference data')\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the reference metadata from the JSON returned from the endpoint into flat JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract results\n",
    "references_list = []\n",
    "for result in results:\n",
    "    row_dict = {}\n",
    "    row_dict[property['variable'] + '_uuid'], row_dict['qid'] = extract_statement_uuid(result[property['variable'] + '_uuid']['value'])\n",
    "    row_dict[property['variable']] = result[property['variable']]['value']\n",
    "    row_dict['label'] = result['label']['value']\n",
    "    try:\n",
    "        row_dict[property['variable'] + '_ref1_hash'] = extract_qnumber(result[property['variable'] + '_ref1_hash']['value'])\n",
    "    except:\n",
    "        row_dict[property['variable'] + '_ref1_hash'] = ''\n",
    "    try:\n",
    "        row_dict[property['variable'] + '_ref1_statedIn'] = extract_qnumber(result[property['variable'] + '_ref1_statedIn']['value'])\n",
    "    except:\n",
    "        row_dict[property['variable'] + '_ref1_statedIn'] = ''\n",
    "    try:\n",
    "        row_dict[property['variable'] + '_ref1_referenceUrl'] = result[property['variable'] + '_ref1_referenceUrl']['value']\n",
    "    except:\n",
    "        row_dict[property['variable'] + '_ref1_referenceUrl'] = ''\n",
    "    try:\n",
    "        # Note: the form of the node ID is http://www.wikidata.org/value/0a8f688406e3fc53d0119eafcd2c0396\n",
    "        # so the extract_qnumber() function can be used on it.\n",
    "        row_dict[property['variable'] + '_ref1_retrieved_nodeid'] = extract_qnumber(result[property['variable'] + '_ref1_retrieved_nodeid']['value'])\n",
    "    except:\n",
    "        row_dict[property['variable'] + '_ref1_retrieved_nodeid'] = ''\n",
    "    try:\n",
    "        row_dict[property['variable'] + '_ref1_retrieved_val'] = result[property['variable'] + '_ref1_retrieved_val']['value']\n",
    "    except:\n",
    "        row_dict[property['variable'] + '_ref1_retrieved_val'] = ''\n",
    "    try:\n",
    "        row_dict[property['variable'] + '_ref1_retrieved_prec'] = result[property['variable'] + '_ref1_retrieved_prec']['value']\n",
    "    except:\n",
    "        row_dict[property['variable'] + '_ref1_retrieved_prec'] = ''\n",
    "    references_list.append(row_dict)\n",
    "    \n",
    "# print(json.dumps(references_list, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the flattened JSON into CSV and write to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of column headers\n",
    "fieldnames = ['qid', property['variable'] + '_uuid', property['variable'], property['variable'] + '_ref1_hash', property['variable'] + '_ref1_statedIn', property['variable'] + '_ref1_referenceUrl', property['variable'] + '_ref1_retrieved_nodeid', property['variable'] + '_ref1_retrieved_val', property['variable'] + '_ref1_retrieved_prec', 'label']\n",
    "\n",
    "# write the data to a CSV file\n",
    "print('writing data to file')\n",
    "write_dicts_to_csv(references_list, data_path + output_file_name, fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined script to acquire metadata for multiple properties at once\n",
    "\n",
    "This duplicates many of the cells above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load item data from csv\n",
    "print('loading item data from file')\n",
    "filename = data_path + item_source_csv\n",
    "items = read_dict(filename)\n",
    "\n",
    "# Create VALUES list for journals\n",
    "item_qids = ''\n",
    "for item in items:\n",
    "    item_qids += 'wd:' + item['qid'] + '\\n'\n",
    "# remove trailing newline\n",
    "item_qids = item_qids[:len(item_qids)-1]\n",
    "\n",
    "# create properties dictionary\n",
    "prop_list = [\n",
    "    {'pid': 'P495', 'variable': 'country_of_origin', 'value_type': 'item'},\n",
    "    {'pid': 'P571', 'variable': 'inception', 'value_type': 'date'},\n",
    "    {'pid': 'P2669', 'variable': 'discontinued_date', 'value_type': 'item'},\n",
    "    {'pid': 'P856', 'variable': 'official_website', 'value_type': 'uri'},\n",
    "    {'pid': 'P155', 'variable': 'follows', 'value_type': 'item'},\n",
    "    {'pid': 'P156', 'variable': 'followed_by', 'value_type': 'item'},\n",
    "    {'pid': 'P2896', 'variable': 'publication_interval', 'value_type': 'decimal'},\n",
    "]\n",
    "\n",
    "#prop_list = [\n",
    "#    {'pid': 'P123', 'variable': 'publisher', 'value_type': 'item'},\n",
    "#    {'pid': 'P1476', 'variable': 'title', 'value_type': 'string'},\n",
    "#    {'pid': 'P31', 'variable': 'instance_of', 'value_type': 'item'},\n",
    "#    {'pid': 'P407', 'variable': 'language_of_work', 'value_type': 'item'},\n",
    "#    {'pid': 'P236', 'variable': 'issn', 'value_type': 'string'}\n",
    "#    {'pid': 'P921', 'variable': 'main_subject', 'value_type': 'item'},\n",
    "#]\n",
    "\n",
    "#print(item_qids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SPARQL query to get the property statements for the items on the Q IDs list\n",
    "select_variable_list = ''\n",
    "graph_pattern = ''\n",
    "for prop in prop_list:\n",
    "    select_variable_list, graph_pattern = sparql_append_property(prop, select_variable_list, graph_pattern)\n",
    "query = '''\n",
    "select distinct ?qid ?label ''' + select_variable_list + 'where {'\n",
    "\n",
    "query += '''\n",
    "  VALUES ?qid\n",
    "{\n",
    "''' + item_qids + '''\n",
    "}\n",
    "'''\n",
    "\n",
    "query += '''\n",
    "?qid rdfs:label ?label.\n",
    "filter(lang(?label)='en')\n",
    "'''\n",
    "\n",
    "query += graph_pattern + '''\n",
    "}'''\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send request to Wikidata Query Service\n",
    "response = requests.post(endpoint, data=query, headers=requestheader)\n",
    "#print(response.text)\n",
    "data = response.json()\n",
    "\n",
    "# extract the values from the response JSON\n",
    "results = data['results']['bindings']\n",
    "print('done retrieving reference data')\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract results\n",
    "metadata_list = []\n",
    "for result in results:\n",
    "    row_dict = {}\n",
    "    row_dict['qid'] = extract_qnumber(result['qid']['value'])\n",
    "    row_dict['label_en'] = result['label']['value']\n",
    "    for property in prop_list:\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_uuid'], trash = extract_statement_uuid(result[property['variable'] + '_uuid']['value'])\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_uuid'] = ''\n",
    "        try:\n",
    "            row_dict[property['variable']] = result[property['variable']]['value']\n",
    "        except:\n",
    "            row_dict[property['variable']] = ''\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_ref1_hash'] = extract_qnumber(result[property['variable'] + '_ref1_hash']['value'])\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_ref1_hash'] = ''\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_ref1_statedIn'] = extract_qnumber(result[property['variable'] + '_ref1_statedIn']['value'])\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_ref1_statedIn'] = ''\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_ref1_referenceUrl'] = result[property['variable'] + '_ref1_referenceUrl']['value']\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_ref1_referenceUrl'] = ''\n",
    "        try:\n",
    "            # Note: the form of the node ID is http://www.wikidata.org/value/0a8f688406e3fc53d0119eafcd2c0396\n",
    "            # so the extract_qnumber() function can be used on it.\n",
    "            row_dict[property['variable'] + '_ref1_retrieved_nodeid'] = extract_qnumber(result[property['variable'] + '_ref1_retrieved_nodeid']['value'])\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_ref1_retrieved_nodeid'] = ''\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_ref1_retrieved_val'] = result[property['variable'] + '_ref1_retrieved_val']['value']\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_ref1_retrieved_val'] = ''\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_ref1_retrieved_prec'] = result[property['variable'] + '_ref1_retrieved_prec']['value']\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_ref1_retrieved_prec'] = ''\n",
    "    \n",
    "    metadata_list.append(row_dict)\n",
    "    \n",
    "print(json.dumps(metadata_list, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of column headers\n",
    "fieldnames = ['qid', 'label_en']\n",
    "for prop in prop_list:\n",
    "    fieldnames = csv_header_append(prop, fieldnames)\n",
    "# print(fieldnames)\n",
    "\n",
    "# write the data to a CSV file\n",
    "print('writing data to file')\n",
    "write_dicts_to_csv(metadata_list, data_path + output_file_name, fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
