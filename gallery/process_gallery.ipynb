{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine arts gallery data processing script\n",
    "\n",
    "The script starts with a dump from jstor. The Excel file must then be saved as a CSV. NOTE: Save the CSV file with an `.txt` file extension (gallery_works0.txt) and don't open it without using the import routine where you can set the accession number to be text. Failure to do this will result in the loss of trailing zeros and item mismatches in the future. \n",
    "\n",
    "It is best to avoid opening the CSV files for manual editing and to just let the script do the work. The script reliably opens the file without corrupting the accession number.\n",
    "\n",
    "## Function section\n",
    "\n",
    "This needs to be run before any of the other cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from copy import deepcopy\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "import datetime\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# Calculate the reference date retrieved value for all statements\n",
    "whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "dateZ = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "ref_retrieved = dateZ + 'T00:00:00Z' # form 2019-12-05T00:00:00Z as provided by Wikidata, without leading +\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# find non-redundant values for a column or simple list\n",
    "def non_redundant(table, column_key):\n",
    "    non_redundant_list = []\n",
    "    for row in table:\n",
    "        found = False\n",
    "        for test_item in non_redundant_list:\n",
    "            if column_key == '':\n",
    "                if row == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            else:\n",
    "                if row[column_key] == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            if column_key == '':\n",
    "                non_redundant_list.append(row)\n",
    "            else:\n",
    "                non_redundant_list.append(row[column_key])\n",
    "    return non_redundant_list\n",
    "\n",
    "# function to use in sort of simple list\n",
    "def sort_funct(row):\n",
    "    return row\n",
    "\n",
    "# function to use in sort last_first names\n",
    "def sort_last_first(row):\n",
    "    return row['last_first']\n",
    "\n",
    "# function to use in sort by match score\n",
    "def sort_score(row):\n",
    "    return row['score']\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# search label and alias\n",
    "# For whatever reason, if I use the graph pattern\n",
    "\n",
    "# wd:Q21 wdt:P31 ?class.\n",
    "\n",
    "# England is not Q6256 (country)\n",
    "# But if I use the graph pattern\n",
    "\n",
    "#   wd:Q21 p:P31 ?statement.\n",
    "#  ?statement ps:P31 ?class.\n",
    "\n",
    "# it is ??!!\n",
    "def searchLabelsAtWikidata(string, class_list):\n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id '\n",
    "    query += '''where {\n",
    "  {?id rdfs:label \"''' + string + '''\"@en.}\n",
    "  union\n",
    "  {?id skos:altLabel \"''' + string + '''\"@en.}\n",
    "  '''\n",
    "    for class_index in range(len(class_list)):\n",
    "        if class_index == 0:\n",
    "            query += '''{?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "        else:\n",
    "            query += '''union\n",
    "  {?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "    query += '''}'''\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    # r = requests.get(endpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['id']['value'])\n",
    "        return_value.append(qid)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def retrieve_gallery_classes():\n",
    "    # create a string for the query\n",
    "    # use Metropolitan Museum of Art because there are too many collections to not specify the collection.\n",
    "    query = '''select distinct ?class ?label where \n",
    "      {\n",
    "      ?item wdt:P195 wd:Q160236.\n",
    "      ?item wdt:P31 ?class.\n",
    "      ?class rdfs:label ?label.\n",
    "      filter(lang(?label) = 'en')\n",
    "      }\n",
    "      order by ?label'''\n",
    "\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    print('sending query')\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    print('results returned')\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['class']['value'])\n",
    "        label = result['label']['value']\n",
    "        return_value.append({'label': label, 'qid': qid})\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        nameVersion = ''\n",
    "        for pieceNumber in range(0, len(pieces)-1):\n",
    "            nameVersion += pieces[pieceNumber] + ' '\n",
    "        nameVersion += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        # get rid of quotes, which will break the query\n",
    "        alternative = alternative.replace('\"', '')\n",
    "        alternative = alternative.replace(\"'\", '')\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    # r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = vbc.extract_qnumber(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def name_variant_testing(name, variant):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.','')\n",
    "    variant = variant.replace('.','')\n",
    "    \n",
    "    # create first names\n",
    "    name_pieces = name.split(' ')\n",
    "    variant_pieces = variant.split(' ')\n",
    "    last_name = name_pieces[len(name_pieces)-1]\n",
    "    last_variant = variant_pieces[len(variant_pieces)-1]\n",
    "    if len(name_pieces) > 1:\n",
    "        first_names = name[0:-(len(last_name)+1)]\n",
    "    else:\n",
    "        first_names = name     \n",
    "    if len(variant_pieces) > 1:\n",
    "        first_variants = variant[0:-(len(last_variant)+1)]\n",
    "    else:\n",
    "        first_variants = variant      \n",
    "    #print(first_names)\n",
    "    #print(first_variants)\n",
    "    \n",
    "    # compare first names\n",
    "    # I experimented with the different ratios and I think fuzz might be best.\n",
    "    ratio = fuzz.ratio(first_names, first_variants)\n",
    "    #partial_ratio = fuzz.partial_ratio(first_names, first_variants)\n",
    "    #sort_ratio = fuzz.token_sort_ratio(first_names, first_variants)\n",
    "    #set_ratio = fuzz.token_set_ratio(first_names, first_variants)\n",
    "    # print('name similarity ratio', ratio)\n",
    "    #print('partial ratio', partial_ratio)\n",
    "    #print('sort_ratio', sort_ratio)\n",
    "    #print('set_ratio', set_ratio)\n",
    "\n",
    "    return(ratio)\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', '')\n",
    "    name = name.replace(',', '')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def remove_parens(string):\n",
    "    name_string = string.split('(')[0]\n",
    "    return name_string.strip()\n",
    "\n",
    "def remove_description(string):\n",
    "    try:\n",
    "        right_string = string.split('(')[1]\n",
    "        left_string = right_string.split(')')[0]\n",
    "        result = left_string.strip()\n",
    "    except:\n",
    "        result = ''\n",
    "    return result\n",
    "\n",
    "def reverse_names(string):\n",
    "    pieces = string.split(',')\n",
    "    return pieces[1].strip() + ' ' + pieces[0].strip()\n",
    "\n",
    "# Screens for Wikidata items that are potential matches\n",
    "\n",
    "import vb_common_code as vbc\n",
    "retrieve_class_list_query = vbc.Query(pid='P31', uselabel=False, sleep=sparql_sleep)\n",
    "retrieve_birth_date_query = vbc.Query(isitem=False, pid='P569', sleep=sparql_sleep)\n",
    "retrieve_death_date_query = vbc.Query(isitem=False, pid='P570', sleep=sparql_sleep)\n",
    "\n",
    "def human(qId):\n",
    "    screen = True\n",
    "    wdClassList = retrieve_class_list_query.single_property_values_for_item(qId)\n",
    "    # if there is a class property, check if it's a human\n",
    "    if len(wdClassList) != 0:\n",
    "        # if it's not a human\n",
    "        if wdClassList[0] != 'Q5':\n",
    "            #print('*** This item is not a human!')\n",
    "            screen = False\n",
    "    return screen\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return resultsDict\n",
    "\n",
    "def determine_era(string):\n",
    "    # dates with no CE or BCE, including empty string\n",
    "    if 'CE' not in string:\n",
    "        value = string\n",
    "        era = 'unknown'      \n",
    "    else:\n",
    "        if 'BCE' in string:\n",
    "            value = string[0:len(string)-3].strip()\n",
    "            era = 'BCE'\n",
    "        else: # string ends with CE\n",
    "            value = string[0:len(string)-2].strip()\n",
    "            era = 'CE'\n",
    "    return value, era\n",
    "\n",
    "def determine_zeros(date):\n",
    "    zero_count = 0\n",
    "    for char_number in range(len(date), 0, -1):\n",
    "        if date[char_number-1] == '0':\n",
    "            zero_count += 1\n",
    "        else:\n",
    "            return zero_count\n",
    "\n",
    "def pad_zeros_left(date_string):\n",
    "    length = len(date_string)\n",
    "    pad = 4-length\n",
    "    return '0' * pad + date_string\n",
    "\n",
    "def sign(era):\n",
    "    if era == 'BCE':\n",
    "        return '-'\n",
    "    elif era == 'CE':\n",
    "        return ''\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def detect_language(string):\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    return lang, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial file processing\n",
    "\n",
    "NOTE: When opening the files, be sure to pay attention to the file import dialog. That allows the accession number column to be imported as a string rather than as a number. Importing as a number causes trailing zeros to be dropped.\n",
    "\n",
    "This section simplifies the column headers and writes a copy of the data to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gallery_works0.csv'\n",
    "old_works = read_dict(filename)\n",
    "fieldnames = list(old_works[0].keys())\n",
    "\n",
    "# print(fieldnames)\n",
    "\n",
    "ssid_field_name = 'SSID'\n",
    "# need to compensate for Bit Order Mark (BOM) from first character of header.\n",
    "if fieldnames[0][0] == '\\ufeff':\n",
    "    ssid_field_name = '\\ufeff' + ssid_field_name\n",
    "works = []\n",
    "for old_work in old_works:\n",
    "    work = {}\n",
    "    work['ssid'] = old_work[ssid_field_name]\n",
    "    work['filename'] = old_work['Filename']\n",
    "    title = old_work['Title[637073]'].strip()\n",
    "    title = title.replace('\\n', ' ') # replace embedded hard returns with spaces.\n",
    "    work['title'] = title\n",
    "    creator = old_work['Creator[637071]'].strip()\n",
    "    creator = creator.replace('\\n', ' ') # replace embedded hard returns with spaces.\n",
    "    work['creator_string'] = creator\n",
    "    work['date'] = old_work['Date[637076]']\n",
    "    work['classification'] = old_work['Classification[637103]']\n",
    "    work['medium'] = old_work['Medium[637080]']\n",
    "    work['measurements'] = old_work['Measurements[637081]']\n",
    "    work['style_period'] = old_work['Style/Period[637079]']\n",
    "    work['country_culture'] = old_work['Country/Culture[637072]']\n",
    "    work['seals_inscriptions'] = old_work['Seals & Inscriptions[637104]'].strip()\n",
    "    work['signature'] = old_work['Signature[637105]']\n",
    "    work['description'] = old_work['Description[637092]']\n",
    "    work['publications'] = old_work['Publications[637106]']\n",
    "    work['exhibitions'] = old_work['Exhibitions[637107]']\n",
    "    work['accession_number'] = old_work['Accession Number[637085]'].strip()\n",
    "    work['date_acquired'] = old_work['Date Acquired[637109]']\n",
    "    work['gift_of'] = old_work['Gift of[637110]']\n",
    "    work['purchased_from'] = old_work['Purchased from[637111]']\n",
    "    work['credit_line'] = old_work['Credit Line[637112]']\n",
    "    work['provenance'] = old_work['Provenance[637113]']\n",
    "    work['collection'] = old_work['Collection[637114]']\n",
    "    work['last_change'] = old_work['Last Change[637115]']\n",
    "    work['notes'] = old_work['Notes[637116]']\n",
    "    work['rights'] = old_work['Rights[637099]']\n",
    "    work['media_url'] = old_work['Media URL']\n",
    "    # License and the Artstor fields are not populated\n",
    "        \n",
    "    works.append(work)\n",
    "        \n",
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_renamed1.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse dimensions data\n",
    "\n",
    "This processes the values of the measurements column and separates them into length and width or length/width/height quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for work_index in range(len(works)):\n",
    "#for work_index in range(0,1):\n",
    "    string = works[work_index]['measurements'].strip()\n",
    "    if string == '':\n",
    "        # no value; set all variables to empty string\n",
    "        height = ''\n",
    "        width = ''\n",
    "        depth = ''\n",
    "        diameter = ''\n",
    "    elif ' x ' not in string:\n",
    "        # one dimensional or improperly formatted\n",
    "        pieces = string.split(' ')\n",
    "        try:\n",
    "            value = float(pieces[0])\n",
    "            if pieces[1] != 'in.':\n",
    "                # second part of string not \"in.\"\n",
    "                print(works[work_index]['accession_number'], string)\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                diameter = ''\n",
    "            else:\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                diameter = str(value)\n",
    "        except:\n",
    "            # improperly formatted            \n",
    "            # set all variables to empty string\n",
    "            print(works[work_index]['accession_number'], string)\n",
    "            height = ''\n",
    "            width = ''\n",
    "            depth = ''\n",
    "            diameter = ''\n",
    "    else:\n",
    "        # the string has an x in it, so it's multidimensional\n",
    "        pieces = string.split('x')\n",
    "        # split the string and get rid of leading and trailing whitespace\n",
    "        for piece_index in range(len(pieces)):\n",
    "            pieces[piece_index] = pieces[piece_index].strip()\n",
    "        # remove the \"in.\" and any spaces from the last piece\n",
    "        pieces[len(pieces)-1] = pieces[len(pieces)-1].split('in')[0].strip()\n",
    "        if len(pieces) == 2:\n",
    "            # two-dimensional work\n",
    "            try:\n",
    "                height = str(float(pieces[0]))\n",
    "                width = str(float(pieces[1]))\n",
    "                depth = ''\n",
    "                diameter = ''\n",
    "            except:\n",
    "                print(works[work_index]['accession_number'], string)\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                diameter = ''                \n",
    "        else:\n",
    "            # three-dimensional work\n",
    "            try:\n",
    "                height = str(float(pieces[0]))\n",
    "                width = str(float(pieces[1]))\n",
    "                depth = str(float(pieces[2]))\n",
    "                diameter = ''\n",
    "            except:\n",
    "                print(works[work_index]['accession_number'], string)\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                diameter = ''\n",
    "    works[work_index]['height'] = height\n",
    "    works[work_index]['width'] = width\n",
    "    works[work_index]['depth'] = depth\n",
    "    works[work_index]['diameter'] = diameter\n",
    "\n",
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_with_dim2.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse inception dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gallery_works_with_dim2.csv'\n",
    "works = read_dict(filename)\n",
    "# fieldnames = list(works[0].keys())\n",
    "# print(fieldnames)\n",
    "\n",
    "for work_index in range(len(works)):\n",
    "    string = works[work_index]['date']\n",
    "    \n",
    "    # handle ideosyncratic date values\n",
    "    if string == 'not dated':\n",
    "        string = ''\n",
    "    if string == 'Unknown':\n",
    "        string = ''\n",
    "    if string[0:5] == 'late ':\n",
    "        string = string[5:]\n",
    "    if string[0:4] == 'mid ':\n",
    "        string = string[4:]\n",
    "    # drop parenthetical comments after the dates\n",
    "    if '(' in string:\n",
    "        pieces = string.split('(')\n",
    "        string = pieces[0].strip()\n",
    "    # fix bad century designation\n",
    "    if 'th CE' in string:\n",
    "        pieces = string.split('th CE')\n",
    "        string = pieces[0] + 'th century CE'\n",
    "        \n",
    "\n",
    "    # split dates\n",
    "    date_list = ['', '', ''] # 0 is single date, 1 is beginning of range, 2 is end of range\n",
    "    if '-' in string:\n",
    "        date_list[0] = ''\n",
    "        date_list[1] = string.split('-')[0].strip()\n",
    "        date_list[2] = string.split('-')[1].strip()\n",
    "    elif 'to' in string:\n",
    "        date_list[0] = ''\n",
    "        date_list[1] = string.split('to')[0].strip()\n",
    "        date_list[2] = string.split('to')[1].strip()\n",
    "        # handle special case of \"late ... to early ...\"\n",
    "        if date_list[2][0:6] == 'early ':\n",
    "            date_list[2] = date_list[2][6:]\n",
    "    else:\n",
    "        date_list[0] = string.strip()\n",
    "        date_list[1] = ''\n",
    "        date_list[2] = ''\n",
    "\n",
    "    # extract CE and BCE\n",
    "    for date_index in range(len(date_list)):\n",
    "        date_dict = {}\n",
    "        date_dict['value'], date_dict['era'] = determine_era(date_list[date_index])\n",
    "        date_list[date_index] = date_dict\n",
    "    \n",
    "    # If last date in range has a designation and the first one doesn't, assign it to the first date.\n",
    "    if date_list[1]['value'] != '' and date_list[1]['era'] == 'unknown' and date_list[2]['era'] != 'unknown':\n",
    "        date_list[1]['era'] = date_list[2]['era']\n",
    "        \n",
    "    # For dates with no specified era, assign CE\n",
    "    for date_index in range(len(date_list)):\n",
    "        if date_list[date_index]['value'] != '' and date_list[date_index]['era'] == 'unknown':\n",
    "            date_list[date_index]['era'] = 'CE'\n",
    "    \n",
    "    # Create a date dict to hold more information about the date format\n",
    "    date_dict = {'dates': date_list}\n",
    "    \n",
    "    # Determine if date is circa\n",
    "    date_dict['circa'] = False\n",
    "    for date_index in range(len(date_dict['dates'])):\n",
    "        if date_list[date_index]['value'][0:3] == 'ca.':\n",
    "            date_dict['circa'] = True\n",
    "            date_dict['dates'][date_index]['value'] = date_dict['dates'][date_index]['value'][3:].strip()\n",
    "    \n",
    "    # Determine if values are centuries\n",
    "    date_dict['century'] = False\n",
    "    for date_index in range(len(date_dict['dates'])):\n",
    "        if date_dict['dates'][date_index]['value'][-7:] == 'century':\n",
    "            date_dict['century'] = True\n",
    "            date_dict['dates'][date_index]['value'] = date_dict['dates'][date_index]['value'][:-7].strip()\n",
    "    if date_dict['century']: # if determined to be century values, strip off the \"th\"\n",
    "        for date_index in range(len(date_dict['dates'])):\n",
    "            date_dict['dates'][date_index]['value'] = date_dict['dates'][date_index]['value'][:-2]\n",
    "    # append date dict to works\n",
    "    works[work_index]['inception'] = date_dict\n",
    "\n",
    "# check for bad dates\n",
    "print('Dates with problems that need to be fixed manually')\n",
    "for work_index in range(len(works)):\n",
    "    for date in works[work_index]['inception']['dates']:\n",
    "        if date['value'] != '':\n",
    "            try:\n",
    "                junk = int(date['value'])\n",
    "            except:\n",
    "                print(work_index, works[work_index]['date'])\n",
    "    # check for two-digit second numbers in ranges\n",
    "    if works[work_index]['inception']['dates'][1]['value'] != '' and works[work_index]['inception']['dates'][1]['era'] == 'CE':\n",
    "        if int(works[work_index]['inception']['dates'][1]['value']) > int(works[work_index]['inception']['dates'][2]['value']):\n",
    "            print(work_index, works[work_index]['accession_number'], works[work_index]['date'])\n",
    "        \n",
    "# Process dates into form needed by Wikidata\n",
    "for work_index in range(len(works)):\n",
    "#for work_index in range(325, 330):\n",
    "    if works[work_index]['inception']['dates'][1]['value'] != '': # cases with date ranges\n",
    "        # Average ranges\n",
    "        first = works[work_index]['inception']['dates'][1]['value']\n",
    "        first_era = works[work_index]['inception']['dates'][1]['era']\n",
    "        second = works[work_index]['inception']['dates'][2]['value']\n",
    "        second_era = works[work_index]['inception']['dates'][2]['era']\n",
    "        if works[work_index]['inception']['century']:\n",
    "            first = str(int(first) * 100)\n",
    "            second = str(int(second) * 100)\n",
    "        minimum_zeros = min(determine_zeros(first), determine_zeros(second))\n",
    "        factor = 10**minimum_zeros\n",
    "        average = (float(sign(first_era) + first) + float(sign(second_era) + second))/2\n",
    "        if minimum_zeros < 2:\n",
    "            # for years and decades, round to the nearest year\n",
    "            average = int(average +.5)\n",
    "            works[work_index]['inception_prec'] = '9' # precision to year\n",
    "        else:\n",
    "            if works[work_index]['inception']['century']: # date given in centuries\n",
    "                if int(second) - int(first) == 100:\n",
    "                    # if given as \"xth to (x+1)th century\" then use the year between\n",
    "                    average = (int(sign(first_era) + '1') * (int(first) - 100) + float(sign(second_era) + second))/2\n",
    "                    works[work_index]['inception_prec'] = '9' # precision to year\n",
    "                elif int(second) - int(first) == 200:\n",
    "                    # if given as \"xth to (x+2)th century\" then use the century x+1 between\n",
    "                    average = int(average/factor)*factor\n",
    "                    works[work_index]['inception_prec'] = '7' # precision to century\n",
    "                else:\n",
    "                    # for wider ranges, just give the average year\n",
    "                    average = (int(sign(first_era) + '1') * (int(first) - 100) + float(sign(second_era) + second))/2\n",
    "                    works[work_index]['inception_prec'] = '9' # precision to year\n",
    "            else: # date give in year range\n",
    "                if int(second) - int(first) == 100:\n",
    "                    # if given as \"x00 to (x+1)00\" then use the x+1 century. This is good for cases like \"1400-1500\"\n",
    "                    average = int(average/factor + 1)*factor\n",
    "                    works[work_index]['inception_prec'] = '7' # precision to century\n",
    "                else:\n",
    "                    # for ranges like \"x00-(x+2)00\" then use the year in the middle: (x+1)00\n",
    "                    works[work_index]['inception_prec'] = '9' # precision to year\n",
    "        # remove negative sign\n",
    "        average = int(average) # remove any decimals and trailing zeros from the number\n",
    "        if average < 0:\n",
    "            number_string = str(average)[1:]\n",
    "            sign_string = '-'\n",
    "        else: # positive dates aren't stored with signs, they are added by the upload script\n",
    "            number_string = str(average)\n",
    "            sign_string = ''\n",
    "        works[work_index]['inception_val'] = sign_string + pad_zeros_left(number_string) + '-01-01T00:00:00Z'\n",
    "        \n",
    "        # Now set the earliest and latest date values\n",
    "        if works[work_index]['inception']['century']: # date given in centuries\n",
    "            works[work_index]['earliest_date_val'] = sign(works[work_index]['inception']['dates'][1]['era']) + pad_zeros_left(works[work_index]['inception']['dates'][1]['value'] + '00') + '-01-01T00:00:00Z'\n",
    "            works[work_index]['earliest_date_prec'] = '7' # precision to century\n",
    "            works[work_index]['latest_date_val'] = sign(works[work_index]['inception']['dates'][2]['era']) + pad_zeros_left(works[work_index]['inception']['dates'][2]['value'] + '00') + '-01-01T00:00:00Z'\n",
    "            works[work_index]['latest_date_prec'] = '7' # precision to century\n",
    "        else: # date given in years\n",
    "            works[work_index]['earliest_date_val'] = sign(works[work_index]['inception']['dates'][1]['era']) + pad_zeros_left(works[work_index]['inception']['dates'][1]['value']) + '-01-01T00:00:00Z'\n",
    "            works[work_index]['earliest_date_prec'] = '9' # precision to year\n",
    "            works[work_index]['latest_date_val'] = sign(works[work_index]['inception']['dates'][2]['era']) + pad_zeros_left(works[work_index]['inception']['dates'][2]['value']) + '-01-01T00:00:00Z'\n",
    "            works[work_index]['latest_date_prec'] = '9' # precision to year\n",
    "    else: # cases without date ranges\n",
    "        if works[work_index]['inception']['dates'][0]['value'] =='':\n",
    "            works[work_index]['inception_val'] = ''\n",
    "            works[work_index]['inception_prec'] = ''\n",
    "        else:\n",
    "            if works[work_index]['inception']['century']: # date given in centuries\n",
    "                works[work_index]['inception_val'] = sign(works[work_index]['inception']['dates'][0]['era']) + pad_zeros_left(works[work_index]['inception']['dates'][0]['value'] + '00') + '-01-01T00:00:00Z'\n",
    "                works[work_index]['inception_prec'] = '7' # precision to century\n",
    "            else: # date given in years\n",
    "                works[work_index]['inception_val'] = sign(works[work_index]['inception']['dates'][0]['era']) + pad_zeros_left(works[work_index]['inception']['dates'][0]['value']) + '-01-01T00:00:00Z'\n",
    "                works[work_index]['inception_prec'] = '9' # precision to year\n",
    "        \n",
    "        works[work_index]['earliest_date_val'] = ''\n",
    "        works[work_index]['earliest_date_prec'] = ''\n",
    "        works[work_index]['latest_date_val'] = ''\n",
    "        works[work_index]['latest_date_prec'] = ''\n",
    "    \n",
    "    # add statement for sourcing circumstances qualifier P1480 when \"circa\" (Q5727902)\n",
    "    if works[work_index]['inception']['circa']:\n",
    "        works[work_index]['sourcing_circumstances'] = 'Q5727902'\n",
    "    else:\n",
    "        works[work_index]['sourcing_circumstances'] = ''\n",
    "    \n",
    "    '''  \n",
    "    print('date:', works[work_index]['date'])\n",
    "    print('inception:', works[work_index]['inception_val'], works[work_index]['inception_prec'])            \n",
    "    print('earliest date:', works[work_index]['earliest_date_val'], works[work_index]['earliest_date_prec'])\n",
    "    print('latest date:', works[work_index]['latest_date_val'], works[work_index]['latest_date_prec'])\n",
    "    print()\n",
    "    '''\n",
    "\n",
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_with_dates3.csv', fieldnames)\n",
    "\n",
    "'''\n",
    "# output test table for dates for testing, not needed since script is now working.\n",
    "out_table = []\n",
    "for work in works:\n",
    "    out_dict = {}\n",
    "    out_dict['string'] = work['date']\n",
    "    out_dict['singe_date'] = work['inception']['dates'][0]['value']\n",
    "    out_dict['singe_era'] = work['inception']['dates'][0]['era']\n",
    "    out_dict['first_date'] = work['inception']['dates'][1]['value']\n",
    "    out_dict['first_era'] = work['inception']['dates'][1]['era']\n",
    "    out_dict['second_date'] = work['inception']['dates'][2]['value']\n",
    "    out_dict['second_era'] = work['inception']['dates'][2]['era']\n",
    "    out_dict['circa'] = work['inception']['circa']\n",
    "    out_dict['century'] = work['inception']['century']\n",
    "    out_table.append(out_dict)\n",
    "\n",
    "fieldnames = list(out_table[0].keys())\n",
    "write_dicts_to_csv(out_table, 'test_dates.csv', fieldnames)\n",
    "'''\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up acquisition dates\n",
    "\n",
    "This script cleans up the date acquired field. These are mostly years, but there are a few non-year values. However, for old works, the date is when it was acquired by the Peabody gallery. So the actual values for VU should be taken from the accession number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gallery_works_with_dates3.csv'\n",
    "works = read_dict(filename)\n",
    "\n",
    "# This section was used to develop the cleanup routine with a non-redundant list\n",
    "'''\n",
    "for work_index in range(len(works)):\n",
    "    if works[work_index]['date_acquired'].strip() != '':\n",
    "        works[work_index]['date_acquired'] = works[work_index]['date_acquired'].strip()\n",
    "acquisition_dates = non_redundant(works, 'date_acquired')\n",
    "acquisition_dates.sort(key = sort_funct)\n",
    "'''\n",
    "\n",
    "for work_index in range(len(works)):\n",
    "    if works[work_index]['date_acquired'].strip() != '':\n",
    "        date = works[work_index]['date_acquired'].strip()\n",
    "    else:\n",
    "        date = ''\n",
    "\n",
    "    # remove commas\n",
    "    date = date.replace(',', '')\n",
    "    if '/' in date:\n",
    "        pieces = date.split('/')\n",
    "    else:\n",
    "        pieces = date.split(' ')\n",
    "    year = ''\n",
    "    circa = False\n",
    "    for piece in pieces:\n",
    "        if piece == 'ca.':\n",
    "            circa = True\n",
    "        try:\n",
    "            number = int(piece)\n",
    "            if number > 1000:\n",
    "                year = str(number)\n",
    "        except:\n",
    "            pass\n",
    "    if year != '':\n",
    "        works[work_index]['acquired_cleaned'] = year\n",
    "        \n",
    "    # adate['circa'] = circa # don't really know what to do with circa, not an appropriae qualifier for collection\n",
    "    year = works[work_index]['accession_number'][0:4]\n",
    "\n",
    "    if works[work_index]['accession_number'] != '':\n",
    "        year = works[work_index]['accession_number'][0:4]\n",
    "        works[work_index]['collection_start_time_val'] = year + '-01-01T00:00:00Z'\n",
    "        works[work_index]['collection_start_time_prec'] = '9'\n",
    "    else:\n",
    "        works[work_index]['collection_start_time_val'] = ''\n",
    "        works[work_index]['collection_start_time_prec'] = ''\n",
    "        \n",
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_acquisition4.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build output file for Wikidata upload\n",
    "\n",
    "The data here needs to be reconciled against data already downloaded from Wikidata using another script. Those downloaded data are in the `works_multiprop.csv` file. The data generated here will be added to that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open source files\n",
    "\n",
    "filename = 'works_multiprop.csv'\n",
    "items = read_dict(filename)\n",
    "fieldnames = list(items[0].keys())\n",
    "# print(fieldnames)\n",
    "\n",
    "data_columns = []\n",
    "for fieldname in fieldnames:\n",
    "    if '_ref1_referenceUrl' in fieldname:\n",
    "        data_column = {}\n",
    "        data_column['prefix'] = fieldname[:len(fieldname)-18]\n",
    "        if data_column['prefix'] in fieldnames:\n",
    "            data_column['name'] = data_column['prefix']\n",
    "        else: # value node values don't have a column name that is the prefix\n",
    "            data_column['name'] = data_column['prefix'] + '_val'\n",
    "        data_columns.append(data_column)\n",
    "#print(data_columns)\n",
    "\n",
    "filename = 'gallery_works_acquisition4.csv'\n",
    "works = read_dict(filename)\n",
    "\n",
    "# fieldnames = list(works[0].keys())\n",
    "# print(fieldnames)\n",
    "\n",
    "filename = 'creators.csv'\n",
    "creators_raw = read_dict(filename)\n",
    "creators = []\n",
    "for creator in creators_raw:\n",
    "    strings = json.loads(creator['creator_string'])\n",
    "    for string in strings:\n",
    "        creators.append({'qid': creator['qid'], 'name': creator['name'], 'string': string})\n",
    "\n",
    "filename = 'classification_mappings.csv'\n",
    "classifications = read_dict(filename)\n",
    "\n",
    "filename = 'country_mappings.csv'\n",
    "countries = read_dict(filename)\n",
    "\n",
    "filename = 'creator-multi.csv'\n",
    "creator2 = read_dict(filename)\n",
    "\n",
    "# set up error logs\n",
    "missing_creators = []\n",
    "missing_classifications = []\n",
    "missing_countries = []\n",
    "\n",
    "output = deepcopy(items)\n",
    "\n",
    "count = 0\n",
    "for work in works:\n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    dic = {}\n",
    "    \n",
    "    # Screening section. Several screens are applied to suppress a record from being written\n",
    "    suppressed = False\n",
    "    \n",
    "    # Prevent existing items from being modified by data from Artstor\n",
    "    found = False\n",
    "    for item in items:\n",
    "        if item['inventory_number'] == work['accession_number']:\n",
    "            found = True\n",
    "            break\n",
    "    # If the accession number of the work matches an existing item, at this point we will just skip it.\n",
    "    # At some point in the future, we would want to check for corrected or updated information.\n",
    "    if found:\n",
    "        suppressed = True\n",
    "    \n",
    "    # Suppress works with labels that are too long\n",
    "    if len(work['title']) > 150:\n",
    "        suppressed = True\n",
    "    \n",
    "    # Supress works that aren't classified with a genre since they won't have instance_of\n",
    "    if work['classification'] == '':\n",
    "        suppressed = True\n",
    "    \n",
    "    # For now, suppress portrait aspect works\n",
    "    if work['height'] != '' and float(work['height']) > float(work['width']):\n",
    "        suppressed = True\n",
    "    \n",
    "    # For now, suppress all works with \"diameters\" because a lot of them are lengths\n",
    "    if work['diameter'] != '':\n",
    "        suppressed = True\n",
    "        \n",
    "    dic['inventory_number'] = work['accession_number']\n",
    "\n",
    "        # For now, suppress all works with missing creators\n",
    "    found = False\n",
    "    for creator in creators:\n",
    "        if work['creator_string'] == creator['string']:\n",
    "            artist_name = creator['name']\n",
    "            dic['creator'] = creator['qid']\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        suppressed = True\n",
    "        missing_creators.append({'inventory_number': \"#\" + dic['inventory_number'], 'string': work['creator_string']})\n",
    "        artist_name = ''\n",
    "        dic['creator'] = ''\n",
    "    \n",
    "    if creator['qid'] == '':\n",
    "        suppressed = True\n",
    "    \n",
    "    # Generate the lines for all non-suppressed works\n",
    "    if not suppressed:\n",
    "        dic['inventory_number_collection'] = 'Q18563658' # Fine Arts Gallery\n",
    "        dic['label_en'] = work['title']\n",
    "        \n",
    "        # title\n",
    "        # The title column is hard-coded as English, so suspected non-English titles \n",
    "        # should be suppressed\n",
    "        lang, prec = detect_language(work['title'])\n",
    "        if lang == 'en' and prec > 0.99:\n",
    "            dic['title'] = work['title']\n",
    "        else:\n",
    "            dic['title'] = ''\n",
    "        dic['title_subtitle'] = ''\n",
    "        \n",
    "        # find second creator if there is one; only used for the description. 2nd creator item will be added\n",
    "        # as part of a second table\n",
    "        found = False\n",
    "        for creator in creator2:\n",
    "            if work['creator_string'] == json.loads(creator['creator_string'])[0]:\n",
    "                second_artist_name = creator['name']\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            missing_creators.append({'inventory_number': \"#\" + dic['inventory_number'], 'string': work['creator_string']})\n",
    "            second_artist_name = ''\n",
    "        \n",
    "        # instance of\n",
    "        found = False\n",
    "        for classification in classifications:\n",
    "            if work['classification'] == classification['string']:\n",
    "                genre_string = classification['label']\n",
    "                dic['instance_of'] = classification['qid']\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            missing_classifications.append({'inventory_number': \"#\" + dic['inventory_number'], 'string': work['classification']})\n",
    "            genre_string = ''\n",
    "            dic['instance_of'] = ''\n",
    "            \n",
    "        if second_artist_name == '':\n",
    "            if 'attributed to' in artist_name:\n",
    "                dic['description_en'] = genre_string + artist_name\n",
    "            else:\n",
    "                dic['description_en'] = genre_string + ' by ' + artist_name\n",
    "        else:\n",
    "            dic['description_en'] = genre_string + ' by ' + artist_name + ' and ' + second_artist_name\n",
    "        dic['inception_val'] = work['inception_val']\n",
    "        dic['inception_prec'] = work['inception_prec']\n",
    "        dic['inception_earliest_date_val'] = work['earliest_date_val']\n",
    "        dic['inception_earliest_date_prec'] = work['earliest_date_prec']\n",
    "        dic['inception_latest_date_val'] = work['latest_date_val']\n",
    "        dic['inception_latest_date_prec'] = work['latest_date_prec']\n",
    "        dic['inception_sourcing_circumstances'] = work['sourcing_circumstances']\n",
    "        \n",
    "        # country\n",
    "        found = False\n",
    "        for country in countries:\n",
    "            if work['country_culture'] == country['string']:\n",
    "                country_culture = work['country_culture']\n",
    "                dic['country'] = country['qid']\n",
    "                dic['country_of_origin'] = country['qid']\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            missing_countries.append({'inventory_number': \"#\" + dic['inventory_number'], 'string': work['country_culture']})\n",
    "            country_culture = ''\n",
    "            dic['country'] = ''\n",
    "            dic['country_of_origin'] = ''\n",
    "        \n",
    "        dic['height_val'] = work['height']\n",
    "        if work['height'] != '':\n",
    "            dic['height_unit'] = 'Q218593'\n",
    "        else:\n",
    "            dic['height_unit'] = ''\n",
    "            \n",
    "        dic['width_val'] = work['width']\n",
    "        if work['width'] != '':\n",
    "            dic['width_unit'] = 'Q218593'\n",
    "        else:\n",
    "            dic['width_unit'] = ''\n",
    "            \n",
    "        dic['thickness_val'] = work['depth']\n",
    "        if work['depth'] != '':\n",
    "            dic['thickness_unit'] = 'Q218593'\n",
    "        else:\n",
    "            dic['thickness_unit'] = ''\n",
    "        \n",
    "        dic['diameter_val'] = work['diameter']\n",
    "        if work['diameter'] != '':\n",
    "            dic['diameter_unit'] = 'Q218593'\n",
    "        else:\n",
    "            dic['diameter_unit'] = ''\n",
    "        \n",
    "        dic['collection'] = 'Q18563658' # Fine Arts Gallery\n",
    "        # !!! Can we get this from the inventory number if missing?\n",
    "        dic['collection_start_time_val'] = work['collection_start_time_val']\n",
    "        dic['collection_start_time_prec'] = work['collection_start_time_prec']\n",
    "        dic['location'] = 'Q18563658' # Vanderbilt University Fine Arts Gallery\n",
    "\n",
    "        # generate references\n",
    "        for column in data_columns:\n",
    "            try: # some columns are passed through without values and will generate errors\n",
    "                if dic[column['name']] != '':\n",
    "                    dic[column['prefix'] + '_ref1_referenceUrl'] = 'https://library.artstor.org/#/asset/' + work['ssid']\n",
    "                    dic[column['prefix'] + '_ref1_retrieved_val'] =  ref_retrieved\n",
    "                    dic[column['prefix'] + '_ref1_retrieved_prec'] =  '11'\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        output.append(dic)\n",
    "    count += 1\n",
    "\n",
    "# output data\n",
    "fieldnames = list(output[0].keys())\n",
    "write_dicts_to_csv(output, 'gallery_works_to_write_dup.csv', fieldnames)\n",
    "\n",
    "# write error logs\n",
    "fieldnames = ['inventory_number', 'string']\n",
    "write_dicts_to_csv(missing_creators, 'missing_creators.csv', fieldnames)\n",
    "write_dicts_to_csv(missing_classifications, 'missing_classifications.csv', fieldnames)\n",
    "write_dicts_to_csv(missing_countries, 'missing_countries.csv', fieldnames)\n",
    "\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicate label/description combinations\n",
    "\n",
    "Wikidata does not allow writing any records if their label/description combinations are the same as an existing item. So we need to not try to write records that are duplicates locally. This cell eliminates local duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = 'gallery_works_to_write_dup.csv'\n",
    "dup_works = read_dict(filename)\n",
    "#for work in dup_works:\n",
    "#    print(work['label_en'])\n",
    "#print()\n",
    "\n",
    "works = []\n",
    "\n",
    "count = 0\n",
    "while len(dup_works) > 0:\n",
    "    remaining = []\n",
    "    if count % 10 == 0: # make something show up so we know it's working\n",
    "        print(count)\n",
    "    match = False\n",
    "    for work_index in range(len(dup_works)-1, 0, -1):\n",
    "        #print(count, dup_works[work_index]['label_en'], '|', dup_works[0]['label_en'])\n",
    "        if dup_works[work_index]['label_en'] == dup_works[0]['label_en'] and dup_works[work_index]['description_en'] == dup_works[0]['description_en']:\n",
    "            match = True\n",
    "            #print('match')\n",
    "            del dup_works[work_index]\n",
    "        else:\n",
    "            remaining.append(dup_works[work_index])\n",
    "    \n",
    "    if not match:\n",
    "        #print('did not match')\n",
    "        works.append(dup_works[0])\n",
    "    dup_works = deepcopy(remaining)\n",
    "    count += 1\n",
    "        \n",
    "#print()\n",
    "#for work in works:\n",
    "#    print(work['label_en'])\n",
    "# output data\n",
    "\n",
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_to_write.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell checks items against Wikidata labels and descriptions (hacked from vb5_check_labels_descriptions.py)\n",
    "\n",
    "Probably good to do a sort by qid, label, description before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = 'gallery_works_to_write.csv'\n",
    "works = read_dict(filename)\n",
    "\n",
    "#for work_index in range(41,42):\n",
    "for work_index in range(len(works)):\n",
    "    if work_index % 10 == 0: # make something show up so we know it's working\n",
    "        print(work_index)\n",
    "    if works[work_index]['qid'] == '':\n",
    "        # Have to do really weird stuff with quotes to avoid problems with strings that contain them\n",
    "        # Still could have problems if any label starts or ends with a single quote.\n",
    "        query = \"\"\"select distinct ?item where {\n",
    "          ?item rdfs:label '''\"\"\" + works[work_index]['label_en'] + \"\"\"'''@en.\n",
    "          ?item schema:description '''\"\"\" + works[work_index]['description_en'] + \"\"\"'''@en.\n",
    "          }\"\"\"\n",
    "\n",
    "        #print('Checking label: \"' + works[work_index]['label_en'] + '\", description: \"' + works[work_index]['description_en'] + '\"')\n",
    "        \n",
    "        r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "        try:\n",
    "            data = r.json()\n",
    "            results = data['results']['bindings']\n",
    "            #print(results)\n",
    "\n",
    "            if len(results) > 0:\n",
    "                match = extract_qnumber(results[0]['item']['value'])\n",
    "                print('Warning! Row ' + str(work_index + 2) + ' is the same as ' + match)\n",
    "        except:\n",
    "            print(r.text)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------\n",
    "\n",
    "# STOP HERE\n",
    "\n",
    "# --------------\n",
    "\n",
    "The following scripts are for one-time use and have already been run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate classes\n",
    "\n",
    "This section first gets all of the classes (values for P31 instanceOf) from the Met's collection. Then it tries to match the labels of those items to the values in the classification column of the gallery data. The source file for the `works` list can really be any of the series because the `classification` field isn't pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all classes in the Met\n",
    "work_classes = retrieve_gallery_classes()\n",
    "# print(work_classes)\n",
    "\n",
    "filename = 'gallery_works_with_dim2.txt'\n",
    "works = read_dict(filename)\n",
    "\n",
    "values = non_redundant(works, 'classification')\n",
    "values.sort(key = sort_funct)\n",
    "if values[0] == '':\n",
    "    values.remove('')\n",
    "print(values)\n",
    "\n",
    "mappings = []\n",
    "for value in values:\n",
    "    found = False\n",
    "    for work_class in work_classes:\n",
    "        if value.lower() == work_class['label'].lower():\n",
    "            found = True\n",
    "            mappings.append({'string': value, 'qid': work_class['qid'], 'label': work_class['label']})\n",
    "    if not found:\n",
    "        mappings.append({'string': value, 'qid': '', 'label': ''})\n",
    "write_dicts_to_csv(mappings, 'classification_mappings.csv', ['string', 'qid', 'label'])\n",
    "print()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate country table\n",
    "\n",
    "Match labels in the country_culture column with labels and aliases in Wikidata for various country-like items. NOTE: this requires hand-processing after generation, so it shouldn't be re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = non_redundant(works, 'country_culture')\n",
    "values.sort(key = sort_funct)\n",
    "if values[0] == '':\n",
    "    values.remove('')\n",
    "print(values)\n",
    "\n",
    "mappings = []\n",
    "for value in values:\n",
    "    result_list = searchLabelsAtWikidata(value, ['Q6256','Q3624078','Q6266'])\n",
    "    print('|' + value + '|', result_list)\n",
    "    if len(result_list) == 1:\n",
    "        qid = result_list[0]\n",
    "    elif len(result_list) > 1:\n",
    "        qid = result_list\n",
    "    else:\n",
    "        qid = ''\n",
    "    mappings.append({'string': value, 'qid': qid})\n",
    "write_dicts_to_csv(mappings, 'country_mappings.csv', ['string', 'qid'])\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process medium field\n",
    "\n",
    "The medium field could form the basis of the description field, but is also used to generate the material used (P186) values.\n",
    "\n",
    "Currently, the `medium.csv` file isn't really used for anything, but it could be used in conjunction with the materials dictionary to describe the materials in the object. This is an area for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't really matter which source file is used, since there is no pre-processing done on this column\n",
    "filename = 'gallery_works_with_dates.txt'\n",
    "works = read_dict(filename)\n",
    "fieldnames = list(works[0].keys())\n",
    "# for field in fieldnames:\n",
    "#    print(field)\n",
    "\n",
    "for work_index in range(len(works)):\n",
    "    if works[work_index]['medium'].strip() != '':\n",
    "        works[work_index]['medium'] = works[work_index]['medium'].strip()\n",
    "medium_strings = non_redundant(works, 'medium')\n",
    "medium_strings.sort(key = sort_funct)\n",
    "out_table = []\n",
    "materials_list = []\n",
    "for string in medium_strings:\n",
    "    #print(string)\n",
    "    out_dict = {}\n",
    "    out_dict['string'] = string\n",
    "    out_dict['material'] = []\n",
    "\n",
    "    pieces = string.split(' ')\n",
    "    if len(pieces) == 1:\n",
    "        out_dict['material1'] = string\n",
    "        out_dict['material'].append(string)\n",
    "        out_dict['material2'] = ''\n",
    "        out_dict['material'].append('')\n",
    "    elif not ' on ' in string and ' and ' in string:\n",
    "        pieces = string.split(' and ')\n",
    "        out_dict['material1'] = pieces[0]\n",
    "        out_dict['material'].append(pieces[0])\n",
    "        out_dict['material2'] = pieces[1]\n",
    "        out_dict['material'].append(pieces[1])\n",
    "    else:\n",
    "        out_dict['material1'] = ''\n",
    "        out_dict['material'].append('')\n",
    "        out_dict['material2'] = ''\n",
    "        out_dict['material'].append('')\n",
    "        \n",
    "    if ' on ' in string:\n",
    "        pieces = string.split(' on ')\n",
    "        out_dict['medium'] = pieces[0]\n",
    "        out_dict['material'].append(pieces[0])\n",
    "        out_dict['surface'] = pieces[1]\n",
    "        out_dict['material'].append(pieces[1])\n",
    "    else:\n",
    "        out_dict['medium'] = ''\n",
    "        out_dict['material'].append('')\n",
    "        out_dict['surface'] = ''\n",
    "        out_dict['material'].append('')\n",
    "        \n",
    "    #print(out_dict['material'])\n",
    "    #print()\n",
    "    out_table.append(out_dict)\n",
    "    for material in out_dict['material']:\n",
    "        materials_list.append(material.lower().strip())\n",
    "\n",
    "fieldnames = list(out_table[0].keys())\n",
    "write_dicts_to_csv(out_table, 'medium.csv', fieldnames)\n",
    "\n",
    "materials_list = non_redundant(materials_list, '')\n",
    "materials_list.sort(key = sort_funct)\n",
    "for material in materials_list:\n",
    "    print(material)\n",
    "\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a one-time script to generate a non-redundant list of material strings. It later gets hand-edited, so don't re-run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and process materials list derived from above. The materials.csv file was manually created by copy and paste\n",
    "# of the list at the end of the previous script.\n",
    "filename = 'materials.csv'\n",
    "creator_data = read_dict(filename)\n",
    "fieldnames = list(creator_data[0].keys())\n",
    "\n",
    "# This is a hack of the creators disambiguation routine below\n",
    "for creator_index in range(len(creator_data)):\n",
    "#for creator_index in range(5):\n",
    "    if creator_data[creator_index]['searched'] == '':\n",
    "        match = False\n",
    "        print(creator_data[creator_index]['material'])\n",
    "        print()\n",
    "        results = searchNameAtWikidata(creator_data[creator_index]['material'])\n",
    "        if len(results) == 0:\n",
    "            print('No results')\n",
    "            print()\n",
    "            creator_data[creator_index]['matches'] = 'no'\n",
    "        elif len(results) == 1:\n",
    "            print(results)\n",
    "            print('match with', searchWikidataDescription(results[0]['qId'])['description'])\n",
    "            creator_data[creator_index]['qid'] = results[0]['qId']\n",
    "        else:\n",
    "            creator_data[creator_index]['matches'] = 'yes'\n",
    "            display_strings = []\n",
    "            for result_index in range(len(results)):\n",
    "                wikidata_descriptions = searchWikidataDescription(results[result_index]['qId'])\n",
    "                description = wikidata_descriptions['description']\n",
    "\n",
    "                similarity_score = name_variant_testing(creator_data[creator_index]['material'], results[result_index]['name'])\n",
    "                # if there is an exact dates match and high name similarity, just assign Q ID\n",
    "\n",
    "                result_name = results[result_index]['name']\n",
    "                result_qid = results[result_index]['qId']\n",
    "                display_strings.append({'qid': result_qid, 'name': result_name, 'description': description, 'score': similarity_score})\n",
    "\n",
    "            display_strings.sort(key = sort_score, reverse = True)\n",
    "            for index in range(len(display_strings)):\n",
    "                print(index, display_strings[index]['score'], display_strings[index]['name'], 'https://www.wikidata.org/wiki/' + display_strings[index]['qid'])\n",
    "                print('Description:', display_strings[index]['description'])\n",
    "                print()\n",
    "            match = input('number of match or Enter for no match')\n",
    "            if match != '':\n",
    "                creator_data[creator_index]['qid'] = results[result_index]['qId']\n",
    "        creator_data[creator_index]['searched'] = 'yes'\n",
    "        write_dicts_to_csv(creator_data, 'materials.csv', fieldnames)\n",
    "        print()\n",
    "        print()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguate creators\n",
    "\n",
    "The following functions are names-related ones from vb3_match_wikidata.py\n",
    "\n",
    "The following cell creates a non-redundant list of processed names by extracting them from the creator field, then reversing them to given name first.\n",
    "\n",
    "Note: don't run this again when there is a creators.csv file already because it will overwrite any data that have been processed by the next script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator_strings = non_redundant(works, 'creator_string')\n",
    "print(len(creator_strings))creator_data = []\n",
    "for creator_string in creator_strings:\n",
    "    creator_datum = {}\n",
    "    creator_datum['last_first'] = remove_parens(creator_string)\n",
    "    creator_datum['description'] = remove_description(creator_string)\n",
    "    if ',' in creator_datum['last_first']:\n",
    "        creator_datum['name'] = reverse_names(creator_datum['last_first'])\n",
    "    else:\n",
    "        creator_datum['name'] = creator_datum['last_first']\n",
    "    creator_datum['creator_string'] = json.dumps([creator_string], ensure_ascii=False)\n",
    "    creator_data.append(creator_datum)\n",
    "    \n",
    "creator_data.sort(key = sort_last_first)\n",
    "\n",
    "fieldnames = list(creator_data[0].keys())\n",
    "write_dicts_to_csv(creator_data, 'creators.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script isn't being maintained any more because there is a stand-alone file, `screen_creators.py`, which is run from the command line. It has edits that aren't found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator_data = read_dict('creators.csv')\n",
    "fieldnames = list(creator_data[0].keys())\n",
    "\n",
    "for creator_index in range(len(creator_data)):\n",
    "# for creator_index in range(22,23):\n",
    "    if creator_data[creator_index]['searched'] == '':\n",
    "        match = False\n",
    "        print(creator_data[creator_index]['name'])\n",
    "        print(creator_data[creator_index]['description'])\n",
    "        print()\n",
    "        results = searchNameAtWikidata(creator_data[creator_index]['name'])\n",
    "        if len(results) == 0:\n",
    "            print('No results')\n",
    "            print()\n",
    "            creator_data[creator_index]['matches'] = 'no'\n",
    "        else:\n",
    "            creator_data[creator_index]['matches'] = 'yes'\n",
    "            display_strings = []\n",
    "            for result_index in range(len(results)):\n",
    "                if human(results[result_index]['qId']):\n",
    "                    wikidata_descriptions = searchWikidataDescription(results[result_index]['qId'])\n",
    "                    description = wikidata_descriptions['description']\n",
    "                    if description[0:18] != 'Peerage person ID=':\n",
    "                        \n",
    "                        birthDateList = retrieve_birth_date_query.single_property_values_for_item(results[result_index]['qId'])\n",
    "                        if len(birthDateList) >= 1:\n",
    "                            birth_date = birthDateList[0][0:4]\n",
    "                        else:\n",
    "                            birth_date = ''\n",
    "                        \n",
    "                        deathDateList = retrieve_death_date_query.single_property_values_for_item(results[result_index]['qId'])\n",
    "                        if len(deathDateList) >= 1:\n",
    "                            death_date = deathDateList[0][0:4]\n",
    "                        else:\n",
    "                            death_date = ''\n",
    "                        \n",
    "                        if death_date != '' and birth_date != '':\n",
    "                            dates = birth_date + '-' + death_date\n",
    "                        elif death_date == '' and birth_date != '':\n",
    "                            dates = 'born ' + birth_date\n",
    "                        elif death_date != '' and birth_date == '':\n",
    "                            dates = 'died ' + death_date\n",
    "                        else:\n",
    "                            dates = ''\n",
    "                            \n",
    "                        similarity_score = name_variant_testing(creator_data[creator_index]['name'], results[result_index]['name'])\n",
    "                        # if there is an exact dates match and high name similarity, just assign Q ID\n",
    "                        if dates != '' and dates in creator_data[creator_index]['description'] and int(similarity_score) > 95:\n",
    "                            match = True\n",
    "                            creator_data[creator_index]['qid'] = results[result_index]['qId']\n",
    "                            print('Auto match with', results[result_index]['name'], dates, 'https://www.wikidata.org/wiki/' + results[result_index]['qId'])\n",
    "                            break # kill the results loop\n",
    "                        else:\n",
    "                            occupation = wikidata_descriptions['occupation']\n",
    "                            result_name = results[result_index]['name']\n",
    "                            result_qid = results[result_index]['qId']\n",
    "                            display_strings.append({'qid': result_qid, 'name': result_name, 'dates': dates, 'description': description, 'occupation': occupation, 'score': similarity_score})\n",
    "\n",
    "            if match:\n",
    "                pass\n",
    "            elif len(display_strings) == 0:\n",
    "                print('No results')\n",
    "                print()\n",
    "            else:\n",
    "                display_strings.sort(key = sort_score, reverse = True)\n",
    "                for index in range(len(display_strings)):\n",
    "                    print(index, display_strings[index]['score'], display_strings[index]['name'], 'https://www.wikidata.org/wiki/' + display_strings[index]['qid'])\n",
    "                    print(display_strings[index]['dates'])\n",
    "                    print('Description:', display_strings[index]['description'])\n",
    "                    print('Occupation:', display_strings[index]['occupation'])\n",
    "                    print()\n",
    "                match = input('number of match or Enter for no match')\n",
    "                if match != '':\n",
    "                    creator_data[creator_index]['qid'] = results[result_index]['qId']\n",
    "        creator_data[creator_index]['searched'] = 'yes'\n",
    "        write_dicts_to_csv(creator_data, 'creators.csv', fieldnames)\n",
    "        print()\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
