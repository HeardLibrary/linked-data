{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to pull metadata from Wikidata\n",
    "\n",
    "Throughout the script I refer to \"Wikidata\" but this could be used for any Wikibase instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration section\n",
    "\n",
    "Import modules, set values, and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire-wikidata-metadata.ipynb This is part of the VandyCite project https://www.wikidata.org/wiki/Wikidata:WikiProject_VandyCite\n",
    "# (c) 2020 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf 2020-11-15\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderDiv/0.1 (https://github.com/HeardLibrary/linked-data/tree/master/publications/divinity-law; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts the UUID and qId from a statement IRI\n",
    "def extract_statement_uuid(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/statement/Q7552806-8B88E0CA-BCC8-49D5-9AC2-F1755464F1A2\n",
    "    pieces = iri.split('/')\n",
    "    statement_id = pieces[5]\n",
    "    pieces = statement_id.split('-')\n",
    "    return pieces[1] + '-' + pieces[2] + '-' + pieces[3] + '-' + pieces[4] + '-' + pieces[5], pieces[0]\n",
    "\n",
    "# function to use in sort\n",
    "def sort_funct(row):\n",
    "    return row['qid']\n",
    "\n",
    "# ----------------\n",
    "# Specialty functions\n",
    "# ----------------\n",
    "\n",
    "# function to add variables related to a property to the select clause and graph pattern of the SPARQL query\n",
    "def sparql_append_property(prop, select_prefix, graph_pattern_prefix):\n",
    "    variable_name = prop['variable']\n",
    "    prop_id = prop['pid']\n",
    "    \n",
    "    # Create the variables for the select clause of the query\n",
    "    select_prefix += ' ?' + variable_name + '_uuid'\n",
    "    if prop['value_type'] == 'date':\n",
    "        select_prefix += ' ?' + variable_name + '_nodeId ?' + variable_name + '_val ?' + variable_name + '_prec'\n",
    "    elif prop['value_type'] == 'quantity':\n",
    "        select_prefix += ' ?' + variable_name + '_nodeId ?' + variable_name + '_val ?' + variable_name + '_unit'\n",
    "    elif prop['value_type'] == 'globecoordinate':\n",
    "        select_prefix += ' ?' + variable_name + '_nodeId ?' + variable_name + '_val ?' + variable_name + '_long ?' + variable_name + '_prec'\n",
    "    else:\n",
    "        select_prefix += ' ?' + variable_name\n",
    "\n",
    "    if len(prop['ref']) > 0:\n",
    "        select_prefix += ' ?' + variable_name + '_ref1_hash'\n",
    "        for reference in prop['ref']:\n",
    "            if reference['value_type'] == 'date':\n",
    "                select_prefix += ' ?' + variable_name + '_ref1_' + reference['variable'] + '_nodeId ?' + variable_name + '_ref1_' + reference['variable'] + '_val ?' + variable_name + '_ref1_' + reference['variable'] + '_prec'\n",
    "            else:\n",
    "                select_prefix += ' ?' + variable_name + '_ref1_' + reference['variable']\n",
    "\n",
    "\n",
    "    for qualifier in prop['qual']:\n",
    "        if qualifier['value_type'] == 'date':\n",
    "            select_prefix += ' ?' + variable_name + '_' + qualifier['variable'] + '_nodeId ?' + variable_name + '_' + qualifier['variable'] + '_val ?' + variable_name + '_' + qualifier['variable'] + '_prec'\n",
    "        else:\n",
    "            select_prefix += ' ?' + variable_name + '_' + qualifier['variable']\n",
    "\n",
    "    # Create the graph pattern for the query\n",
    "    graph_pattern_prefix += '''optional{\n",
    "?qid p:''' + prop['pid'] + ' ?' + variable_name + '_uuid.\\n'\n",
    "\n",
    "    if prop['value_type'] == 'date':\n",
    "        graph_pattern_prefix += '?' + variable_name + '_uuid psv:' + prop['pid'] + ' ?' + variable_name + '''_nodeId.\n",
    "?''' + variable_name + '_nodeId wikibase:timeValue ?' + variable_name + '''_val.\n",
    "?''' + variable_name + '_nodeId wikibase:timePrecision ?' + variable_name + '_prec.\\n'\n",
    "        \n",
    "    elif prop['value_type'] == 'quantity':\n",
    "        graph_pattern_prefix += '?' + variable_name + '_uuid psv:' + prop['pid'] + ' ?' + variable_name + '''_nodeId.\n",
    "?''' + variable_name + '_nodeId wikibase:quantityAmount ?' + variable_name + '''_val.\n",
    "?''' + variable_name + '_nodeId wikibase:quantityUnit ?' + variable_name + '_unit.\\n'\n",
    "        \n",
    "    elif prop['value_type'] == 'globecoordinate':\n",
    "        graph_pattern_prefix += '?' + variable_name + '_uuid psv:' + prop['pid'] + ' ?' + variable_name + '''_nodeId.\n",
    "?''' + variable_name + '_nodeId wikibase:geoLatitude ?' + variable_name + '''_val.\n",
    "?''' + variable_name + '_nodeId wikibase:geoLongitude ?' + variable_name + '''_long.\n",
    "?''' + variable_name + '_nodeId wikibase:geoPrecision ?' + variable_name + '_prec.\\n'\n",
    "        \n",
    "    elif prop['value_type'] == 'monolingualtext':\n",
    "        graph_pattern_prefix += '?' + variable_name + '_uuid ps:' + prop['pid'] + ' ?' + variable_name + '''.\n",
    "filter(lang(?''' + variable_name + ')=\"' + prop['language'] + '\")\\n'\n",
    "        \n",
    "    else:\n",
    "        graph_pattern_prefix += '?' + variable_name + '_uuid ps:' + prop['pid'] + ' ?' + variable_name + '.\\n'\n",
    "\n",
    "    # construct reference triple patterns\n",
    "    for reference in prop['ref']:\n",
    "        graph_pattern_prefix += '''\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid prov:wasDerivedFrom ?''' + variable_name + '''_ref1_hash.\n",
    "'''\n",
    "        if reference['value_type'] == 'date':\n",
    "            graph_pattern_prefix += '''\n",
    "?''' + variable_name + '_ref1_hash prv:' + reference['pid'] + ' ?' + variable_name + '_ref1_' + reference['variable'] + '''_nodeId.\n",
    "?''' + variable_name + '_ref1_' + reference['variable'] + '_nodeId wikibase:timeValue ?' + variable_name + '_ref1_' + reference['variable'] + '''_val.\n",
    "?''' + variable_name + '_ref1_' + reference['variable'] + '_nodeId wikibase:timePrecision ?' + variable_name + '_ref1_' + reference['variable'] + '''_prec.\n",
    "'''\n",
    "        elif reference['value_type'] == 'monolingualtext':\n",
    "            graph_pattern_prefix += '''\n",
    "?''' + variable_name + '_ref1_hash pr:' + reference['pid'] + ' ?' + variable_name + '_ref1_' + reference['variable'] + '''.\n",
    "filter(lang(?''' + variable_name + '_' + reference['variable'] + ')=\"' + reference['language'] + '''\")\n",
    "'''\n",
    "        else:\n",
    "            graph_pattern_prefix += '''\n",
    "?''' + variable_name + '_ref1_hash pr:' + reference['pid'] + ' ?' + variable_name + '_ref1_' + reference['variable'] +'''.\n",
    "'''\n",
    "        # close the optional clause\n",
    "        graph_pattern_prefix += '''}\n",
    "'''\n",
    "\n",
    "    # construct qualifiers triple patterns\n",
    "    for qualifier in prop['qual']:\n",
    "        if qualifier['value_type'] == 'date':\n",
    "            graph_pattern_prefix += '''\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid pqv:''' + qualifier['pid'] + ' ?' + variable_name + '_' + qualifier['variable'] + '''_nodeId.\n",
    "?''' + variable_name + '_' + qualifier['variable'] + '_nodeId wikibase:timeValue ?' + variable_name + '_' + qualifier['variable'] + '''_val.\n",
    "?''' + variable_name + '_' + qualifier['variable'] + '_nodeId wikibase:timePrecision ?' + variable_name + '_' + qualifier['variable'] + '''_prec.\n",
    "}'''\n",
    "        elif qualifier['value_type'] == 'monolingualtext':\n",
    "            graph_pattern_prefix += '''\n",
    "optional{\n",
    "?''' + variable_name + '_uuid pq:' + qualifier['pid'] + ' ?' + variable_name + '_' + qualifier['variable'] + '''.\n",
    "filter(lang(?''' + variable_name + '_' + qualifier['variable'] + ')=\"' + qualifier['language'] + '''\")\n",
    "}\n",
    "'''\n",
    "        else:\n",
    "            graph_pattern_prefix += '''\n",
    "optional{\n",
    "?''' + variable_name + '''_uuid pq:'''+ qualifier['pid'] + ''' ?''' + variable_name + '_' + qualifier['variable'] +'''.\n",
    "}\n",
    "'''\n",
    "\n",
    "    # close graph pattern\n",
    "    graph_pattern_prefix += '}\\n'\n",
    "    return select_prefix, graph_pattern_prefix\n",
    "\n",
    "# function to add columns to column list for CSV header\n",
    "def csv_header_append(prop, header_list):\n",
    "    variable_name = prop['variable']\n",
    "\n",
    "    header_list.append(variable_name + '_uuid')\n",
    "    if prop['value_type'] == 'date':\n",
    "        header_list.append(variable_name + '_nodeId')\n",
    "        header_list.append(variable_name + '_val')\n",
    "        header_list.append(variable_name + '_prec')\n",
    "    elif prop['value_type'] == 'quantity':\n",
    "        header_list.append(variable_name + '_nodeId')\n",
    "        header_list.append(variable_name + '_val')\n",
    "        header_list.append(variable_name + '_unit')\n",
    "    elif prop['value_type'] == 'globecoordinate':\n",
    "        header_list.append(variable_name + '_nodeId')\n",
    "        header_list.append(variable_name + '_val')\n",
    "        header_list.append(variable_name + '_long')\n",
    "        header_list.append(variable_name + '_prec')\n",
    "    else:\n",
    "        header_list.append(variable_name)\n",
    "\n",
    "    if len(prop['ref']) > 0:\n",
    "        header_list.append(variable_name + '_ref1_hash')\n",
    "        for reference in prop['ref']:\n",
    "            if reference['value_type'] == 'date':\n",
    "                header_list.append(variable_name + '_ref1_' + reference['variable'] + '_nodeId')\n",
    "                header_list.append(variable_name + '_ref1_' + reference['variable'] + '_val')\n",
    "                header_list.append(variable_name + '_ref1_' + reference['variable'] + '_prec')\n",
    "            else:\n",
    "                header_list.append(variable_name + '_ref1_' + reference['variable'])\n",
    "\n",
    "    for qualifier in prop['qual']:\n",
    "        if qualifier['value_type'] == 'date':\n",
    "            header_list.append(variable_name + '_' + qualifier['variable'] + '_nodeId')\n",
    "            header_list.append(variable_name + '_' + qualifier['variable'] + '_val')\n",
    "            header_list.append(variable_name + '_' + qualifier['variable'] + '_prec')\n",
    "        else:\n",
    "            header_list.append(variable_name + '_' + qualifier['variable'])\n",
    "\n",
    "    return header_list\n",
    "\n",
    "def remove_multiples(matching_rows, col_name, old_id):\n",
    "    matched_rows = []\n",
    "    for check_row in matching_rows:\n",
    "        if old_id == check_row[col_name]:\n",
    "            matched_rows.append(check_row)\n",
    "    return matched_rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: run only one of the two following cells!\n",
    "\n",
    "Configuration for Fine Arts Gallery works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Setup for particular data sources\n",
    "# ----------------\n",
    "\n",
    "#data_path = home + '/divinity-law/'\n",
    "data_path = '../../vandycite/gallery_works/'\n",
    "\n",
    "# put an empty string here to get the QIDs from the query rather than a file\n",
    "item_source_csv = '' \n",
    "\n",
    "# insert any graph pattern that will screen for the Q IDs you are interested in. Must use \"?qid\" as the variable.\n",
    "item_query = '''select distinct ?qid where {\n",
    "  ?qid wdt:P195 wd:Q18563658.\n",
    "  }'''\n",
    "\n",
    "# NOTE: the following qualifier value types are currently not supported: globecoordinate, quantity\n",
    "# supported types are: item, date, and string-like types (uri, string)\n",
    "# monolingualtext is a special situation. The language tag is only used for screening - the resulting column is the same as string\n",
    "\n",
    "# settings for labels/title metadata\n",
    "manage_descriptions = True\n",
    "label_description_language_list = ['en']\n",
    "output_file_name = 'works-multiprop.csv'\n",
    "\n",
    "# properties dictionary\n",
    "# NOTE: commented out properties might be desirable but aren't available from the database\n",
    "# NOTE: new items created need to have P31 instanceOf, so there will need to be a column for this.\n",
    "prop_list = [\n",
    "    {'pid': 'P18', 'variable': 'image', 'value_type': 'uri', 'qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P17', 'variable': 'country', 'value_type': 'item', 'qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    #{'pid': 'P135', 'variable': 'movement', 'value_type': 'item', 'qual': [],\n",
    "    #     'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "    #             {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "    #             {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "    #             ]},\n",
    "    {'pid': 'P170', 'variable': 'creator', 'value_type': 'item', 'qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    # Note from Kali: We don't have data on this but could base genre on medium?\n",
    "    #{'pid': 'P136', 'variable': 'genre', 'value_type': 'item', 'qual': [],\n",
    "    #     'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "    #             {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "    #             {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "    #             ]},\n",
    "    #{'pid': 'P921', 'variable': 'main_subject', 'value_type': 'item', 'qual': [],\n",
    "    #     'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "    #             {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "    #             {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "    #             ]},\n",
    "    {'pid': 'P1476', 'variable': 'title', 'value_type': 'monolingualtext', 'language': 'en',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'en'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P127', 'variable': 'owned_by', 'value_type': 'item',\n",
    "       'qual': [{'pid': 'P580', 'variable': 'start_time', 'value_type': 'date'},\n",
    "                {'pid': 'P582', 'variable': 'end_time', 'value_type': 'date'}\n",
    "               ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P571', 'variable': 'inception', 'value_type': 'date',\n",
    "       'qual': [{'pid': 'P1326', 'variable': 'latest_date', 'value_type': 'date'},\n",
    "                {'pid': 'P1319', 'variable': 'earliest_date', 'value_type': 'date'},\n",
    "                {'pid': 'P1480', 'variable': 'sourcing_circumstances', 'value_type': 'item'}\n",
    "               ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P2048', 'variable': 'height', 'value_type': 'quantity', 'qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P2049', 'variable': 'width', 'value_type': 'quantity', 'qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P495', 'variable': 'country_of_origin', 'value_type': 'item', 'qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    #{'pid': 'P528', 'variable': 'catalog_code', 'value_type': 'string',\n",
    "    #   'qual': [{'pid': 'P972', 'variable': 'catalog', 'value_type': 'item'}\n",
    "    #           ],\n",
    "    #     'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "    #             {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "    #             {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "    #             ]},\n",
    "    {'pid': 'P6216', 'variable': 'copyright_status', 'value_type': 'item',\n",
    "       'qual': [{'pid': 'P1001', 'variable': 'applies_to_jurisdiction', 'value_type': 'item'},\n",
    "                {'pid': 'P459', 'variable': 'determination_method', 'value_type': 'item'}\n",
    "               ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P2610', 'variable': 'thickness', 'value_type': 'quantity', 'qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P2596', 'variable': 'culture', 'value_type': 'item', 'qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]}\n",
    "]\n",
    "\n",
    "'''\n",
    "# The following properties can contain multiple values per item, so need to be managed in separate CSVs.\n",
    "# The script needs to be rerun with each one as a single item on the prop_list.\n",
    "prop_list = [\n",
    "    {'pid': 'P195', 'variable': 'collection', 'value_type': 'item',\n",
    "       'qual': [{'pid': 'P580', 'variable': 'start_time', 'value_type': 'date'},\n",
    "                {'pid': 'P582', 'variable': 'end_time', 'value_type': 'date'}\n",
    "               ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P186', 'variable': 'material_used', 'value_type': 'item',\n",
    "       'qual': [{'pid': 'P518', 'variable': 'applies_to_part', 'value_type': 'item'}\n",
    "               ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "# Provide constant value for location as \"Vanderbilt University\"\n",
    "    {'pid': 'P276', 'variable': 'location', 'value_type': 'item', 'qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "# See note above about providing value for instanceOf\n",
    "    {'pid': 'P31', 'variable': 'instance_of', 'value_type': 'item', 'qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P217', 'variable': 'inventory_number', 'value_type': 'string',\n",
    "       'qual': [{'pid': 'P195', 'variable': 'collection', 'value_type': 'item'}\n",
    "               ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P973', 'variable': 'described_at_URL', 'value_type': 'uri', 'qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P108', 'variable': 'depicts', 'value_type': 'item', 'qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "]\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration for Divinity/Law journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = home + '/divinity-law/'\n",
    "data_path = '../../vandycite/journals/'\n",
    "\n",
    "# put an empty string here to get the QIDs from the query rather than a file\n",
    "item_source_csv = 'journal-div-qids.csv' \n",
    "\n",
    "# insert any graph pattern that will screen for the Q IDs you are interested in. Must use \"?qid\" as the variable.\n",
    "item_query = '''select distinct ?qid where {\n",
    "  ?qid wdt:P195 wd:Q18563658.\n",
    "  }'''\n",
    "\n",
    "# NOTE: the following qualifier value types are currently not supported: globecoordinate, quantity\n",
    "# supported types are: item, date, and string-like types (uri, string)\n",
    "# monolingualtext is a special situation. The language tag is only used for screening - the resulting column is the same as string\n",
    "\n",
    "\n",
    "# settings for labels/title metadata\n",
    "'''\n",
    "manage_descriptions = True\n",
    "label_description_language_list = ['en', 'zh', 'zh-hans', 'zh-hant', 'de', 'fr']\n",
    "output_file_name = 'titles/journals-title.csv'\n",
    "prop_list = [\n",
    "    {'pid': 'P1476', 'variable': 'title_en', 'value_type': 'monolingualtext', 'language': 'en',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'en'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]\n",
    "    }\n",
    "]\n",
    "'''\n",
    "\n",
    "manage_descriptions = False\n",
    "label_description_language_list = ['en']\n",
    "output_file_name = 'titles/journals-title-de.csv'\n",
    "prop_list = [\n",
    "    {'pid': 'P1476', 'variable': 'title_de', 'value_type': 'monolingualtext', 'language': 'de',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'de'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]\n",
    "    }\n",
    "]\n",
    "                 \n",
    "'''\n",
    "    {'pid': 'P1476', 'variable': 'title_la', 'value_type': 'monolingualtext', 'language': 'la',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'la'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P1476', 'variable': 'title_fr', 'value_type': 'monolingualtext', 'language': 'fr',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'fr'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P1476', 'variable': 'title_it', 'value_type': 'monolingualtext', 'language': 'it',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'it'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P1476', 'variable': 'title_pt', 'value_type': 'monolingualtext', 'language': 'pt',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'pt'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P1476', 'variable': 'title_es', 'value_type': 'monolingualtext', 'language': 'es',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'es'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P1476', 'variable': 'title_sk', 'value_type': 'monolingualtext', 'language': 'sk',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'sk'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P1476', 'variable': 'title_hu', 'value_type': 'monolingualtext', 'language': 'hu',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'hu'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P1476', 'variable': 'title_nb', 'value_type': 'monolingualtext', 'language': 'nb',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'nb'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P1476', 'variable': 'title_ko', 'value_type': 'monolingualtext', 'language': 'ko',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'ko'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P1476', 'variable': 'title_af', 'value_type': 'monolingualtext', 'language': 'af',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext', 'language': 'af'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]}\n",
    "]\n",
    "'''\n",
    "\n",
    "'''\n",
    "# settings for multiprop\n",
    "# official website requires qualifier language of work or name (item)\n",
    "label_description_language_list = []\n",
    "output_file_name = 'journals-multiprop.csv'\n",
    "prop_list = [\n",
    "    {'pid': 'P495', 'variable': 'country_of_origin', 'value_type': 'item','qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P571', 'variable': 'inception', 'value_type': 'date','qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P2669', 'variable': 'discontinued_date', 'value_type': 'date','qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P856', 'variable': 'official_website', 'value_type': 'uri',\n",
    "         'qual': [{'pid': 'P407', 'variable': 'work_language', 'value_type': 'item'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P155', 'variable': 'follows', 'value_type': 'item',\n",
    "         'qual': [{'pid': 'P580', 'variable': 'start_time', 'value_type': 'date'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P156', 'variable': 'followed_by', 'value_type': 'item',\n",
    "         'qual': [{'pid': 'P580', 'variable': 'start_time', 'value_type': 'date'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]},\n",
    "    {'pid': 'P2896', 'variable': 'publication_interval', 'value_type': 'quantity','qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]}\n",
    "]\n",
    "\n",
    "# The following properties can contain multiple values per item, so need to be managed in separate CSVs.\n",
    "# The script needs to be rerun with each one as a single item on the prop_list.\n",
    "\n",
    "prop_list = [\n",
    "    {'pid': 'P123', 'variable': 'publisher', 'value_type': 'item',\n",
    "         'qual': [{'pid': 'P580', 'variable': 'start_time', 'value_type': 'date'},\n",
    "                  {'pid': 'P582', 'variable': 'end_time', 'value_type': 'date'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]}\n",
    "    {'pid': 'P1476', 'variable': 'title', 'value_type': 'monolingualtext', 'language': 'en',\n",
    "         'qual': [{'pid': 'P1680', 'variable': 'subtitle', 'value_type': 'monolingualtext'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]}\n",
    "    {'pid': 'P31', 'variable': 'instance_of', 'value_type': 'item','qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]}\n",
    "    {'pid': 'P407', 'variable': 'language_of_work', 'value_type': 'item','qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]}\n",
    "    {'pid': 'P236', 'variable': 'issn', 'value_type': 'string',\n",
    "         'qual': [{'pid': 'P437', 'variable': 'distribution_format', 'value_type': 'item'},\n",
    "                  {'pid': 'P1810', 'variable': 'named_as', 'value_type': 'date'}\n",
    "                 ],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]}\n",
    "    {'pid': 'P921', 'variable': 'main_subject', 'value_type': 'item','qual': [],\n",
    "         'ref': [{'pid': 'P248', 'variable': 'statedIn', 'value_type': 'item'},\n",
    "                 {'pid': 'P854', 'variable': 'referenceUrl', 'value_type': 'uri'},\n",
    "                 {'pid': 'P813', 'variable': 'retrieved', 'value_type': 'date'}\n",
    "                 ]}\n",
    "]\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Beginning of main script\n",
    "# ----------------\n",
    "\n",
    "# Determine if previous file and load if exists\n",
    "# In order to determine which redundant statements and references to keep, try to load a previous version of the file.\n",
    "\n",
    "if os.path.exists(data_path + output_file_name):\n",
    "    old_file_data = read_dict(data_path + output_file_name)\n",
    "else:\n",
    "    old_file_data = []# ----------------\n",
    "# Load list of items from file (or generate by query) and construct Q ID list for query\n",
    "# ----------------\n",
    "\n",
    "# The CSV has a header row with column headers: `qid` and `label`. The `qid` column contains the Wikidata Q \n",
    "# identifiers for each item. The `label` column contains the label, which isn't necessarily the label in \n",
    "# Wikidata and isn't use for anything in the script. It does provide a way for humans to recognize the item \n",
    "# when looking at the table.\n",
    "\n",
    "if item_source_csv == '':\n",
    "    # send request to Wikidata Query Service\n",
    "    print('querying SPARQL endpoint to acquire item QIDs')\n",
    "    response = requests.post(endpoint, data=item_query, headers=requestheader)\n",
    "    #print(response.text)\n",
    "    data = response.json()\n",
    "    print('results returned')\n",
    "\n",
    "    # extract the values from the response JSON\n",
    "    items = data['results']['bindings']\n",
    "    #print(items)\n",
    "    \n",
    "    # Create VALUES list for items\n",
    "    item_qids = ''\n",
    "    for item in items:\n",
    "        item_qids += 'wd:' + extract_qnumber(item['qid']['value']) + '\\n'\n",
    "else:\n",
    "    # Load item data from csv\n",
    "    print('loading item data from file')\n",
    "    filename = data_path + item_source_csv\n",
    "    items = read_dict(filename)\n",
    "    print('done loading')\n",
    "\n",
    "    # Create VALUES list for items\n",
    "    item_qids = ''\n",
    "    for item in items:\n",
    "        item_qids += 'wd:' + item['qid'] + '\\n'\n",
    "# remove trailing newline\n",
    "item_qids = item_qids[:len(item_qids)-1]\n",
    "\n",
    "#print(item_qids)\n",
    "\n",
    "# ----------------\n",
    "# Create the SPARQL query to get the property statements for the items on the Q IDs list\n",
    "# ----------------\n",
    "\n",
    "select_variable_list = ''\n",
    "graph_pattern = ''\n",
    "for prop in prop_list:\n",
    "    select_variable_list, graph_pattern = sparql_append_property(prop, select_variable_list, graph_pattern)\n",
    "query = '''\n",
    "select distinct ?qid '''\n",
    "\n",
    "# note: dashes not allowed in SPARQL variable names, so replace with underscores\n",
    "for label_description_language in label_description_language_list:\n",
    "    query +='?label_' + label_description_language.replace('-', '_') + ' ?description_' + label_description_language.replace('-', '_') + ' '\n",
    "query += select_variable_list + ' where {'\n",
    "\n",
    "query += '''\n",
    "  VALUES ?qid\n",
    "{\n",
    "''' + item_qids + '''\n",
    "}\n",
    "'''\n",
    "# made label and description optional since some don't have in English\n",
    "for label_description_language in label_description_language_list:\n",
    "    query += '''\n",
    "optional {\n",
    "?qid rdfs:label ?label_''' + label_description_language.replace('-', '_') + '''.\n",
    "filter(lang(?label_''' + label_description_language.replace('-', '_') + ')=\"' + label_description_language + '''\")\n",
    "}\n",
    "optional {\n",
    "?qid schema:description ?description_''' + label_description_language.replace('-', '_') + '''.\n",
    "filter(lang(?description_''' + label_description_language.replace('-', '_') + ')=\"' + label_description_language + '''\")\n",
    "}\n",
    "'''\n",
    "\n",
    "query += graph_pattern + '''\n",
    "}\n",
    "order by ?qid'''\n",
    "\n",
    "#print(query)\n",
    "\n",
    "# ----------------\n",
    "# send request to Wikidata Query Service\n",
    "# ----------------\n",
    "\n",
    "print('querying SPARQL endpoint to acquire item metadata')\n",
    "response = requests.post(endpoint, data=query, headers=requestheader)\n",
    "#print(response.text)\n",
    "data = response.json()\n",
    "\n",
    "# extract the values from the response JSON\n",
    "results = data['results']['bindings']\n",
    "\n",
    "print('done retrieving data')\n",
    "# print(json.dumps(results, indent=2))\n",
    "\n",
    "# ----------------\n",
    "# extract results\n",
    "# ----------------\n",
    "\n",
    "metadata_list = []\n",
    "for result in results:\n",
    "    row_dict = {}\n",
    "    row_dict['qid'] = extract_qnumber(result['qid']['value'])\n",
    "    for label_description_language in label_description_language_list:\n",
    "        try:\n",
    "            row_dict['label_' + label_description_language.replace('-', '_')] = result['label_' + label_description_language.replace('-', '_')]['value']\n",
    "        except:\n",
    "            row_dict['label_' + label_description_language.replace('-', '_')] = ''\n",
    "        if manage_descriptions:\n",
    "            try:\n",
    "                row_dict['description_' + label_description_language.replace('-', '_')] = result['description_' + label_description_language.replace('-', '_')]['value']\n",
    "            except:\n",
    "                row_dict['description_' + label_description_language.replace('-', '_')] = ''           \n",
    "    for property in prop_list:\n",
    "        try:\n",
    "            row_dict[property['variable'] + '_uuid'], trash = extract_statement_uuid(result[property['variable'] + '_uuid']['value'])\n",
    "        except:\n",
    "            row_dict[property['variable'] + '_uuid'] = ''\n",
    "        try:\n",
    "            if property['value_type'] == 'item':\n",
    "                row_dict[property['variable']] = extract_qnumber(result[property['variable']]['value'])\n",
    "            elif property['value_type'] == 'date':\n",
    "                row_dict[property['variable'] + '_nodeId'] = extract_qnumber(result[property['variable'] + '_nodeId']['value'])\n",
    "                row_dict[property['variable'] + '_val'] = result[property['variable'] + '_val']['value']\n",
    "                row_dict[property['variable'] + '_prec'] = result[property['variable'] + '_prec']['value']\n",
    "            elif property['value_type'] == 'quantity':\n",
    "                row_dict[property['variable'] + '_nodeId'] = extract_qnumber(result[property['variable'] + '_nodeId']['value'])\n",
    "                row_dict[property['variable'] + '_val'] = result[property['variable'] + '_val']['value']\n",
    "                row_dict[property['variable'] + '_unit'] = extract_qnumber(result[property['variable'] + '_unit']['value'])\n",
    "            elif property['value_type'] == 'globecoordinate':\n",
    "                row_dict[property['variable'] + '_nodeId'] = extract_qnumber(result[property['variable'] + '_nodeId']['value'])\n",
    "                row_dict[property['variable'] + '_val'] = result[property['variable'] + '_val']['value']\n",
    "                row_dict[property['variable'] + '_long'] = result[property['variable'] + '_long']['value']\n",
    "                row_dict[property['variable'] + '_prec'] = result[property['variable'] + '_long']['_prec']\n",
    "            else:\n",
    "                row_dict[property['variable']] = result[property['variable']]['value']\n",
    "        except:\n",
    "            if property['value_type'] == 'date':\n",
    "                row_dict[property['variable'] + '_nodeId'] = ''\n",
    "                row_dict[property['variable'] + '_val'] = ''\n",
    "                row_dict[property['variable'] + '_prec'] = ''\n",
    "            elif property['value_type'] == 'quantity':\n",
    "                row_dict[property['variable'] + '_nodeId'] = ''\n",
    "                row_dict[property['variable'] + '_val'] = ''\n",
    "                row_dict[property['variable'] + '_unit'] = ''\n",
    "            elif property['value_type'] == 'globecoordinate':\n",
    "                row_dict[property['variable'] + '_nodeId'] = ''\n",
    "                row_dict[property['variable'] + '_val'] = ''\n",
    "                row_dict[property['variable'] + '_long'] = ''\n",
    "                row_dict[property['variable'] + '_prec'] = ''\n",
    "            else:\n",
    "                row_dict[property['variable']] = ''\n",
    "        \n",
    "        if len(property['ref']) > 0:\n",
    "            try:\n",
    "                row_dict[property['variable'] + '_ref1_hash'] = extract_qnumber(result[property['variable'] + '_ref1_hash']['value'])\n",
    "            except:\n",
    "                row_dict[property['variable'] + '_ref1_hash'] = ''\n",
    "            for reference in property['ref']:\n",
    "                try:\n",
    "                    if reference['value_type'] == 'date':\n",
    "                        # Note: the form of the node ID is http://www.wikidata.org/value/0a8f688406e3fc53d0119eafcd2c0396\n",
    "                        # so the extract_qnumber() function can be used on it.\n",
    "                        row_dict[property['variable'] + '_ref1_' + reference['variable'] + '_nodeId'] = extract_qnumber(result[property['variable'] + '_ref1_' + reference['variable'] + '_nodeId']['value'])\n",
    "                        row_dict[property['variable'] + '_ref1_' + reference['variable'] + '_val'] = result[property['variable'] + '_ref1_' + reference['variable'] + '_val']['value']\n",
    "                        row_dict[property['variable'] + '_ref1_' + reference['variable'] + '_prec'] = result[property['variable'] + '_ref1_' + reference['variable'] + '_prec']['value']\n",
    "                    elif reference['value_type'] == 'item':\n",
    "                        row_dict[property['variable'] + '_ref1_' + reference['variable']] = extract_qnumber(result[property['variable'] + '_ref1_' + reference['variable']]['value'])\n",
    "                    else:\n",
    "                        row_dict[property['variable'] + '_ref1_' + reference['variable']] = result[property['variable'] + '_ref1_' + reference['variable']]['value']\n",
    "                except:\n",
    "                    if reference['value_type'] == 'date':\n",
    "                        row_dict[property['variable'] + '_ref1_' + reference['variable'] + '_nodeId'] = ''\n",
    "                        row_dict[property['variable'] + '_ref1_' + reference['variable'] + '_val'] = ''\n",
    "                        row_dict[property['variable'] + '_ref1_' + reference['variable'] + '_prec'] = ''\n",
    "                    else:\n",
    "                        row_dict[property['variable'] + '_ref1_' + reference['variable']] = ''\n",
    "\n",
    "        for qualifier in property['qual']:\n",
    "            try:\n",
    "                if qualifier['value_type'] == 'date':\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable'] + '_nodeId'] = extract_qnumber(result[property['variable'] + '_' + qualifier['variable'] + '_nodeId']['value'])\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable'] + '_val'] = result[property['variable'] + '_' + qualifier['variable'] + '_val']['value']\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable'] + '_prec'] = result[property['variable'] + '_' + qualifier['variable'] + '_prec']['value']\n",
    "                elif qualifier['value_type'] == 'item':\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable']] = extract_qnumber(result[property['variable'] + '_' + qualifier['variable']]['value'])\n",
    "                else:\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable']] = result[property['variable'] + '_' + qualifier['variable']]['value']\n",
    "            except:\n",
    "                if qualifier['value_type'] == 'date':\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable'] + '_nodeId'] = ''\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable'] + '_val'] = ''\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable'] + '_prec'] = ''\n",
    "                else:\n",
    "                    row_dict[property['variable'] + '_' + qualifier['variable']] = ''\n",
    "    \n",
    "    metadata_list.append(row_dict)\n",
    "    \n",
    "# print(json.dumps(metadata_list, indent=2))\n",
    "\n",
    "# ----------------\n",
    "# prepare acquired data for saving\n",
    "# ----------------\n",
    "\n",
    "# create the list of column headers\n",
    "fieldnames = ['qid']\n",
    "# The schema generator puts all of the labels first, then the descriptions\n",
    "for label_description_language in label_description_language_list:\n",
    "    fieldnames.append('label_' + label_description_language.replace('-', '_'))          \n",
    "if manage_descriptions:\n",
    "    for label_description_language in label_description_language_list:\n",
    "        fieldnames.append('description_' + label_description_language.replace('-', '_'))          \n",
    "for prop in prop_list:\n",
    "    fieldnames = csv_header_append(prop, fieldnames)\n",
    "# print(fieldnames)\n",
    "\n",
    "# ----------------\n",
    "# Remove redundant item lines\n",
    "# ----------------\n",
    "\n",
    "# Determine whether old and new file headers are the same\n",
    "try:\n",
    "    old_fieldnames = list(old_file_data[0].keys())\n",
    "    if old_fieldnames != fieldnames:\n",
    "        print(old_fieldnames)\n",
    "        print(fieldnames)\n",
    "        print(\"fieldnames are not the same\")\n",
    "except:\n",
    "    print('no pre=exising file')\n",
    "    \n",
    "# Remove duplicate rows that aren't found in a previous version of the table. It doesn't do anything if there \n",
    "# isn't an earlier table. \n",
    "\n",
    "# If there are duplicates in the previous version, they will both be carried forward in the new version. \n",
    "# But it's complicated...\n",
    "\n",
    "output_data = []\n",
    "# old_file_data will be an empty list if it doesn't exist and no matching will be attempted\n",
    "for old_row in old_file_data:\n",
    "    matching_rows = []\n",
    "    # find all rows from the query that match the QID of the row in the original table\n",
    "    for row in metadata_list:\n",
    "        if old_row['qid'] == row['qid']:\n",
    "            matching_rows.append(row)\n",
    "    original_matching_rows = list(matching_rows) # make a copy of the matching data\n",
    "    if len(matching_rows) == 0:\n",
    "        print('Warning! No result from the query matches item:', old_row['qid'])\n",
    "        \n",
    "    # Note: if only one row matches, there is no reason to believe that any value or reference is the same as \n",
    "    # before since no checks are done here.\n",
    "    elif len(matching_rows) == 1:\n",
    "        output_data.append(matching_rows[0])\n",
    "    \n",
    "    # There are two reasons why there could be duplicate rows:\n",
    "    # - there could be two references for the same statement.\n",
    "    # - there could be two values for the same property.\n",
    "    elif len(matching_rows) > 1: # skip if there aren't duplicate rows\n",
    "        # There can be any number of duplicate rows since the SPARQL query catches every combination\n",
    "        # of duplicate reference and statement values. But this should systematically reduce them to \n",
    "        # one if there was only one statement in the original file.\n",
    "        \n",
    "        # Remove rows whose references don't match those in the original file.\n",
    "        for prop in prop_list:\n",
    "            ref_col_name = prop['variable'] + '_ref1_hash'\n",
    "            old_ref_hash = old_row[ref_col_name]\n",
    "            if len(matching_rows) > 1:\n",
    "                matching_rows = remove_multiples(matching_rows, ref_col_name, old_ref_hash)\n",
    "        \n",
    "        # It's possible that two rows will have different values for a property but the same reference\n",
    "        # So look for the statement UUID that matches the UUID in the original file and eliminate others.\n",
    "        for prop in prop_list:\n",
    "            statement_uuid_col_name = prop['variable'] + '_uuid'\n",
    "            old_statement_uuid = old_row[statement_uuid_col_name]\n",
    "            if len(matching_rows) > 1:\n",
    "                matching_rows = remove_multiples(matching_rows, statement_uuid_col_name, old_statement_uuid)\n",
    "        \n",
    "        # It's possible that the previous value for this item doesn't have the same reference or values \n",
    "        # as any of the query results.\n",
    "        if len(matching_rows) == 0:\n",
    "            print('Multiple query results for', old_row['qid'], 'but none match previous statement values and references.')\n",
    "            print('Check values manually.\\n')\n",
    "            output_data += original_matching_rows\n",
    "        elif len(matching_rows) ==1:\n",
    "            output_data.append(matching_rows[0])\n",
    "        else:\n",
    "            print('Multiple query results for', old_row['qid'], 'have the same statement values and references.')\n",
    "            print('How did this happen? It should not. Check results manually.')\n",
    "            output_data += matching_rows\n",
    "\n",
    "# Add any new items that were added to the item list file or that resulted from the item query.\n",
    "for row in metadata_list:\n",
    "    found = False\n",
    "    for old_row in old_file_data:\n",
    "        if old_row['qid'] == row['qid']:\n",
    "            found = True\n",
    "            break # stop looking for more matches\n",
    "    if not found:\n",
    "        output_data.append(row)\n",
    "output_data.sort(key = sort_funct)\n",
    "\n",
    "# write the data to a CSV file\n",
    "print('writing data to file')\n",
    "write_dicts_to_csv(output_data, data_path + output_file_name, fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
