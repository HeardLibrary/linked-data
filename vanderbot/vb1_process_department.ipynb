{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VanderBot\n",
    "\n",
    "The scripts in this notebook are part of VanderBot, a system to write information about Vanderbilt University researchers and their works to Wikidata.  \n",
    "\n",
    "This code is freely available under a CC0 license. Steve Baskauf 2020-04-xx\n",
    "\n",
    "VanderBot v1.0 is the first stable release.   \n",
    "\n",
    "For more information, see [this page](https://github.com/HeardLibrary/linked-data/tree/master/vanderbot).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Query() class\n",
    "\n",
    "Methods of the `Query()` class sends queries to Wikibase instances. It has the following methods:\n",
    "\n",
    "`.generic_query(query)` Sends a specified query to the endpoint and returns a list of item Q IDs, item labels, or literal values. The variable to be returned must be `?entity`.\n",
    "\n",
    "`.single_property_values_for_item(qid)` Sends a subject Q ID to the endpoint and returns a list of item Q IDs, item labels, or literal values that are values of a specified property.\n",
    "\n",
    "`.labels_descriptions(qids)` Sends a list of subject Q IDs to the endpoint and returns a list of dictionaries of the form `{'qid': qnumber, 'string': string}` where `string` is either a label, description, or alias. Alternatively, an added graph pattern can be passed as `labelscreen` in lieu of the list of Q IDs. In that case, pass an empty list (`[]`) into the method. The screening graph pattern should have `?id` as its only unknown variable.\n",
    "\n",
    "`.search_statement(qids, reference_property_list)` Sends a list of Q IDs and a list of reference properties to the endpoint and returns information about statements using a property specified as the pid value. If no value is specified, the information includes the values of the statements. For each statement, the reference UUID, reference property, and reference value is returned. If the statement has more than one reference, there will be multiple results per subject. Results are in the form `{'qId': qnumber, 'statementUuid': statement_uuid, 'statementValue': statement_value, 'referenceHash': reference_hash, 'referenceValue': reference_value}`\n",
    "\n",
    "It has the following attributes:\n",
    "\n",
    "| key | description | default value | applicable method |\n",
    "|:-----|:-----|:-----|:-----|\n",
    "| `endpoint` | endpoint URL of Wikabase | `https://query.wikidata.org/sparql` | all |\n",
    "| `mediatype` | Internet media type | `application/json` | all |\n",
    "| `useragent` | User-Agent string to send | `VanderBot/0.9` etc.| all |\n",
    "| `requestheader` | request headers to send |(generated dict) | all |\n",
    "| `sleep` | seconds to delay between queries | 0.25 | all |\n",
    "| `isitem` | `True` if value is item, `False` if value a literal | `True` | `generic_query`, `single_property_values_for_item` |\n",
    "| `uselabel` | `True` for label of item value , `False` for Q ID of item value | `True` | `generic_query`, `single_property_values_for_item` | \n",
    "| `lang` | language of label | `en` | `single_property_values_for_item`, `labels_descriptions`|\n",
    "| `labeltype` | returns `label`, `description`, or `alias` | `label` | `labels_descriptions` |\n",
    "| `labelscreen` | added triple pattern | empty string | `labels_descriptions` |\n",
    "| `pid` | property P ID | `P31` | `single_property_values_for_item`, `search_statement` |\n",
    "| `vid` | value Q ID | empty string | `search_statement` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Code\n",
    "\n",
    "This code block includes import statements, function definitions, and declarations of variables that are common to the rest of the script. It needs to be run once before the other code blocks.\n",
    "\n",
    "**Note: the code in this block is found in the stand-alone file vb_common_code.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "# For a particular processing round, set a short name for the department here.\n",
    "# This name is used to generate a set of unique processing files for that department.\n",
    "testEmployer = 'Vanderbilt University' # to test against Wikidata employer property\n",
    "employerQId = 'Q29052' # Vanderbilt University\n",
    "deathDateLimit = '2000' # any death dates before this date will be assumed to not be a match\n",
    "birthDateLimit = '1920' # any birth dates before this date will be assumed to not be a match\n",
    "wikibase_instance_namespace = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "# NOTE: eventually need to test against all affiliations in cases of faculty with multiple appointments\n",
    "# Note: 2020-04-13: on most scrapes we don't have this, so it isn't possible to check.\n",
    "\n",
    "# Here is some example JSON from a departmental configuration file (department-configuration.json):\n",
    "\n",
    "'''\n",
    "{\n",
    "  \"deptShortName\": \"anthropology\",\n",
    "  \"aads\": {\n",
    "    \"categories\": [\n",
    "      \"\"\n",
    "    ],\n",
    "    \"baseUrl\": \"https://as.vanderbilt.edu/aads/people/\",\n",
    "    \"nTables\": 1,\n",
    "    \"departmentSearchString\": \"African American and Diaspora Studies\",\n",
    "    \"departmentQId\": \"Q79117444\",\n",
    "    \"testAuthorAffiliation\": \"African American Diaspora Studies Vanderbilt\",\n",
    "    \"labels\": {\n",
    "      \"source\": \"column\",\n",
    "      \"value\": \"name\"\n",
    "    },\n",
    "    \"descriptions\": {\n",
    "      \"source\": \"constant\",\n",
    "      \"value\": \"African American and Diaspora Studies scholar\"\n",
    "    }\n",
    "  },\n",
    "  \"bsci\": {\n",
    "    \"categories\": [\n",
    "      \"primary-training-faculty\",\n",
    "      \"research-and-teaching-faculty\",\n",
    "      \"secondary-faculty\",\n",
    "      \"postdoc-fellows\",\n",
    "      \"emeriti\"\n",
    "    ],\n",
    "    \"baseUrl\": \"https://as.vanderbilt.edu/biosci/people/index.php?group=\",\n",
    "    \"nTables\": 1,\n",
    "    \"departmentSearchString\": \"Biological Sciences\",\n",
    "    \"departmentQId\": \"Q78041310\",\n",
    "    \"testAuthorAffiliation\": \"Biological Sciences Vanderbilt\",\n",
    "    \"labels\": {\n",
    "      \"source\": \"column\",\n",
    "      \"value\": \"name\"\n",
    "    },\n",
    "    \"descriptions\": {\n",
    "      \"source\": \"constant\",\n",
    "      \"value\": \"biology researcher\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''\n",
    "# Note that the first key: value pair sets the department to be processed.\n",
    "\n",
    "# The default labels and descriptions can either be a column in the table or set as a constant. \n",
    "# If it's a column, the value is the column header.  If it's a constant, the value is the string to assign as the value.\n",
    "\n",
    "# The nTables value is the number of HTML tables in the page to be searched.  Currently (2020-01-19) it isn't used\n",
    "# and the script just checks all of the tables, but it could be implemented if there are tables at the end that don't \n",
    "# include employee names.\n",
    "\n",
    "# ---------------------\n",
    "# utility functions used across blocks\n",
    "# ---------------------\n",
    "\n",
    "with open('department-configuration.json', 'rt', encoding='utf-8') as fileObject:\n",
    "    text = fileObject.read()\n",
    "deptSettings = json.loads(text)\n",
    "deptShortName = deptSettings['deptShortName']\n",
    "print('Department currently set for', deptShortName)\n",
    "\n",
    "wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "degreeList = [\n",
    "    {'string': 'Ph.D.', 'value': 'Ph.D.'},\n",
    "    {'string': 'PhD', 'value': 'Ph.D.'},\n",
    "    {'string': 'D.Phil.', 'value': 'D.Phil.'},\n",
    "    {'string': 'J.D.', 'value': 'J.D.'}\n",
    "     ]\n",
    "\n",
    "# NCBI identification requirements:\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'VanderBot' # give your application a name here\n",
    "\n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generateHeaderDictionary(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/0.9 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def writeListsToCsv(fileName, array):\n",
    "    with open(fileName, 'w', newline='', encoding='utf-8') as fileObject:\n",
    "        writerObject = csv.writer(fileObject)\n",
    "        for row in array:\n",
    "            writerObject.writerow(row)\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def writeDictsToCsv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvFileObject:\n",
    "        writer = csv.DictWriter(csvFileObject, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def readDict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as fileObject:\n",
    "        dictObject = csv.DictReader(fileObject)\n",
    "        array = []\n",
    "        for row in dictObject:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts a local name from an IRI, specify the list item number for the last piece separated by slash\n",
    "def extract_from_iri(iri, number_pieces):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[number_pieces]\n",
    "\n",
    "# see https://www.wikidata.org/wiki/Property:P21 for values\n",
    "def decodeSexOrGender(code):\n",
    "    code = code.lower()\n",
    "    if code == 'm':\n",
    "        qId = 'Q6581097'\n",
    "    elif code == 'f':\n",
    "        qId = 'Q6581072'\n",
    "    elif code == 'i':\n",
    "        qId = 'Q1097630'\n",
    "    elif code == 'tf':\n",
    "        qId = 'Q1052281'\n",
    "    elif code == 'tm':\n",
    "        qId = 'Q2449503'\n",
    "    else:\n",
    "        qId = ''\n",
    "    return qId\n",
    "\n",
    "def checkOrcid(orcid):\n",
    "    namespace = 'https://orcid.org/'\n",
    "    endpointUrl = namespace + orcid\n",
    "    acceptMediaType = 'application/ld+json'\n",
    "    r = requests.get(endpointUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    code = r.status_code\n",
    "    #print(r.text)\n",
    "    data = r.json()\n",
    "    response = {'code': code, 'data': data}\n",
    "    if response['code'] != 200:\n",
    "        print('Attempt to dereference ORCID resulted in HTTP response code ', response['code'])\n",
    "        data['orcidReferenceValue'] = ''\n",
    "    else:\n",
    "        #print('Successfully retrieved')\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "    # delay a quarter second to avoid hitting the API too rapidly\n",
    "    sleep(0.25)\n",
    "    return(wholeDateZ)\n",
    "\n",
    "# query for a single variable that's an item named 'item'\n",
    "# returns a list of results\n",
    "def searchWikidataForQIdByOrcid(orcid):\n",
    "    query = '''\n",
    "select distinct ?item where {\n",
    "  ?item wdt:P496 \"''' + employees[employeeIndex]['orcid'] + '''\".\n",
    "  }\n",
    "'''\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            qNumber = extract_qnumber(wikidataIri)\n",
    "            results.append(qNumber)\n",
    "    except:\n",
    "        results = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# --------------\n",
    "# Query class definition\n",
    "# --------------\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, **kwargs):\n",
    "        # attributes for all methods\n",
    "        try:\n",
    "            self.lang = kwargs['lang']\n",
    "        except:\n",
    "            self.lang = 'en' # default to English\n",
    "        try:\n",
    "            self.mediatype = kwargs['mediatype']\n",
    "        except:\n",
    "            self.mediatype = 'application/json' # default to JSON formatted query results\n",
    "        try:\n",
    "            self.endpoint = kwargs['endpoint']\n",
    "        except:\n",
    "            self.endpoint = 'https://query.wikidata.org/sparql' # default to Wikidata endpoint\n",
    "        try:\n",
    "            self.useragent = kwargs['useragent']\n",
    "        except:\n",
    "            self.useragent = 'VanderBot/0.9 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)' \n",
    "        self.requestheader = {\n",
    "        'Accept' : self.mediatype,\n",
    "        'User-Agent': self.useragent\n",
    "        }\n",
    "        try:\n",
    "            self.pid = kwargs['pid'] # property's P ID\n",
    "        except:\n",
    "            self.pid = 'P31' # default to \"instance of\"  \n",
    "        try:\n",
    "            self.sleep = kwargs['sleep']\n",
    "        except:\n",
    "            self.sleep = 0.25 # default throtting of 0.25 seconds\n",
    "            \n",
    "        # attributes for single property values method\n",
    "        try:\n",
    "            self.isitem = kwargs['isitem']\n",
    "        except:\n",
    "            self.isitem = True # default to values are items rather than literals   \n",
    "        try:\n",
    "            self.uselabel = kwargs['uselabel']\n",
    "        except:\n",
    "            self.uselabel = True # default is to show labels of items\n",
    "            \n",
    "        # attributes for labels and descriptions method\n",
    "        try:\n",
    "            self.labeltype = kwargs['labeltype']\n",
    "        except:\n",
    "            self.labeltype = 'label' # default to \"label\". Other options: \"description\", \"alias\"\n",
    "        try:\n",
    "            self.labelscreen = kwargs['labelscreen']\n",
    "        except:\n",
    "            self.labelscreen = '' # instead of using a list of subject items, add this line to screen for items\n",
    "            \n",
    "        # attributes for search_statement method\n",
    "        try:\n",
    "            self.vid = kwargs['vid'] # Q ID of the value of a statement. \n",
    "        except:\n",
    "            self.vid = '' # default to no value (the method returns the value of the statement)\n",
    "            \n",
    "    # send a generic query and return a list of Q IDs\n",
    "    def generic_query(self, query):\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        results_list = []\n",
    "        try:\n",
    "        #if 1==1: # replace try: to let errors occur, also comment out the except: clause\n",
    "            data = r.json()\n",
    "            #print(data)\n",
    "            statements = data['results']['bindings']\n",
    "            if len(statements) > 0: # if no results, the list remains empty\n",
    "                for statement in statements:\n",
    "                    if self.isitem:\n",
    "                        if self.uselabel:\n",
    "                            result_value = statement['entity']['value']\n",
    "                        else:\n",
    "                            result_value = extract_qnumber(statement['entity']['value'])\n",
    "                    else:\n",
    "                        result_value = statement['entity']['value']\n",
    "                    results_list.append(result_value)\n",
    "        except:\n",
    "            results_list = [r.text]\n",
    "        \n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "            \n",
    "\n",
    "    # returns the value of a single property for an item by Q ID\n",
    "    def single_property_values_for_item(self, qid):\n",
    "        query = '''\n",
    "select distinct ?object where {\n",
    "    wd:'''+ qid + ''' wdt:''' + self.pid\n",
    "        if self.uselabel and self.isitem:\n",
    "            query += ''' ?objectItem.\n",
    "    ?objectItem rdfs:label ?object.\n",
    "    FILTER(lang(?object) = \"''' + self.lang +'\")'\n",
    "        else:\n",
    "            query += ''' ?object.'''            \n",
    "        query +=  '''\n",
    "    }'''\n",
    "        #print(query)\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        results_list = []\n",
    "        try:\n",
    "        #if 1==1: # replace try: to let errors occur, also comment out the except: clause\n",
    "            data = r.json()\n",
    "            #print(data)\n",
    "            statements = data['results']['bindings']\n",
    "            if len(statements) > 0: # if no results, the list remains empty\n",
    "                for statement in statements:\n",
    "                    if self.isitem:\n",
    "                        if self.uselabel:\n",
    "                            result_value = statement['object']['value']\n",
    "                        else:\n",
    "                            result_value = extract_qnumber(statement['object']['value'])\n",
    "                    else:\n",
    "                        result_value = statement['object']['value']\n",
    "                    results_list.append(result_value)\n",
    "        except:\n",
    "            results_list = [r.text]\n",
    "        \n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "    \n",
    "    # search for any of the \"label\" types: label, alias, description. qids is a list of Q IDs without namespaces\n",
    "    def labels_descriptions(self, qids):\n",
    "        # option to explicitly list subject Q IDs\n",
    "        if self.labelscreen == '':\n",
    "            # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "            alternatives = ''\n",
    "            for qid in qids:\n",
    "                alternatives += 'wd:' + qid + '\\n'\n",
    "\n",
    "        if self.labeltype == 'label':\n",
    "            predicate = 'rdfs:label'\n",
    "        elif self.labeltype == 'alias':\n",
    "            predicate = 'skos:altLabel'\n",
    "        elif self.labeltype == 'description':\n",
    "            predicate = 'schema:description'\n",
    "        else:\n",
    "            predicate = 'rdfs:label'        \n",
    "\n",
    "        # create a string for the query\n",
    "        query = '''\n",
    "select distinct ?id ?string where {'''\n",
    "        \n",
    "        # option to explicitly list subject Q IDs\n",
    "        if self.labelscreen == '':\n",
    "            query += '''\n",
    "      VALUES ?id\n",
    "    {\n",
    "''' + alternatives + '''\n",
    "    }'''\n",
    "        # option to screen for Q IDs by triple pattern\n",
    "        if self.labelscreen != '':\n",
    "            query += '''\n",
    "    ''' + self.labelscreen\n",
    "            \n",
    "        query += '''\n",
    "    ?id '''+ predicate + ''' ?string.\n",
    "    filter(lang(?string)=\"''' + self.lang + '''\")\n",
    "    }'''\n",
    "        #print(query)\n",
    "\n",
    "        results_list = []\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        data = r.json()\n",
    "        results = data['results']['bindings']\n",
    "        for result in results:\n",
    "            # remove wd: 'http://www.wikidata.org/entity/'\n",
    "            qnumber = extract_qnumber(result['id']['value'])\n",
    "            string = result['string']['value']\n",
    "            results_list.append({'qid': qnumber, 'string': string})\n",
    "\n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "\n",
    "    # Searches for statements using a particular property. If no value is set, the value will be returned.\n",
    "    def search_statement(self, qids, reference_property_list):\n",
    "        # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "        alternatives = ''\n",
    "        for qid in qids:\n",
    "            alternatives += 'wd:' + qid + '\\n'\n",
    "\n",
    "        # create a string for the query\n",
    "        query = '''\n",
    "select distinct ?id ?statement '''\n",
    "        # if no value was specified, find the value\n",
    "        if self.vid == '':\n",
    "            query += '?statementValue '\n",
    "        if len(reference_property_list) != 0:\n",
    "            query += '?reference '\n",
    "        for ref_prop_index in range(0, len(reference_property_list)):\n",
    "            query += '?refVal' + str(ref_prop_index) + ' '\n",
    "        query += '''\n",
    "    where {\n",
    "        VALUES ?id\n",
    "    {\n",
    "''' + alternatives + '''\n",
    "    }\n",
    "    ?id p:'''+ self.pid + ''' ?statement.\n",
    "    ?statement ps:'''+ self.pid\n",
    "\n",
    "        if self.vid == '': # return the value of the statement if no particular value is specified\n",
    "            query += ' ?statementValue.'\n",
    "        else:\n",
    "            query += ' wd:' + self.vid + '.' # specify the value to be searched for\n",
    "\n",
    "        if len(reference_property_list) != 0:\n",
    "            query += '''\n",
    "    optional {\n",
    "        ?statement prov:wasDerivedFrom ?reference.''' # search for references if there are any\n",
    "            for ref_prop_index in range(0, len(reference_property_list)):\n",
    "                query +='''\n",
    "        ?reference pr:''' + reference_property_list[ref_prop_index] + ' ?refVal' + str(ref_prop_index) + '.'\n",
    "            query +='''\n",
    "            }'''\n",
    "        query +='''\n",
    "      }'''\n",
    "        #print(query)\n",
    "\n",
    "        results_list = []\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        data = r.json()\n",
    "        results = data['results']['bindings']\n",
    "        # NOTE: There may be more than one reference per statement.\n",
    "        # This results in several results with the same subject qNumber.\n",
    "        # There may also be more than one value for a property.\n",
    "        # These situations are handled in the code, which only records one statement and one reference per employee.\n",
    "        for result in results:\n",
    "            # remove wd: 'http://www.wikidata.org/entity/'\n",
    "            qnumber = extract_qnumber(result['id']['value'])\n",
    "            # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "            no_domain = extract_from_iri(result['statement']['value'], 5)\n",
    "            # need to remove the qNumber that's appended in front of the UUID\n",
    "            pieces = no_domain.split('-')\n",
    "            last_pieces = pieces[1:len(pieces)]\n",
    "            s = \"-\"\n",
    "            statement_uuid = s.join(last_pieces)\n",
    "\n",
    "            # if no value was specified, get the value that was found in the search\n",
    "            if self.vid == '':\n",
    "                statement_value = result['statementValue']['value']\n",
    "            # extract the reference property data if any reference properties were specified\n",
    "            if len(reference_property_list) != 0:\n",
    "                if 'reference' in result:\n",
    "                    # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "                    reference_hash = extract_qnumber(result['reference']['value'])\n",
    "                else:\n",
    "                    reference_hash = ''\n",
    "                reference_values = []\n",
    "                for ref_prop_index in range(0, len(reference_property_list)):\n",
    "                    if 'refVal' + str(ref_prop_index) in result:\n",
    "                        reference_value = result['refVal' + str(ref_prop_index)]['value']\n",
    "                        # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "                        #if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                        #    referenceValue = referenceValue.split('T')[0]\n",
    "                    else:\n",
    "                        reference_value = ''\n",
    "                    reference_values.append(reference_value)\n",
    "            results_dict = {'qId': qnumber, 'statementUuid': statement_uuid}\n",
    "            # if no value was specified, get the value that was found in the search\n",
    "            if self.vid == '':\n",
    "                results_dict['statementValue'] = statement_value\n",
    "            if len(reference_property_list) != 0:\n",
    "                results_dict['referenceHash'] = reference_hash\n",
    "                results_dict['referenceValues'] = reference_values\n",
    "            results_list.append(results_dict)\n",
    "\n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query ORCID for Vanderbilt University people\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/orcid/orcid-get-json.ipynb\n",
    "\n",
    "Retrieves results 100 at a time, then processes them by extracting desired information.  **NOTE: takes hours to run.**\n",
    "\n",
    "Saves results in a file and the alternative names in a second file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [['orcid', 'givenNames', 'familyName', 'startDate', 'endDate', 'department', 'organization']]\n",
    "otherNameList = [['orcid', 'altName']]\n",
    "\n",
    "# use the API to search for people associated with Vanderbilt University\n",
    "# First search is for only one record, just to get the number of hits found\n",
    "searchUri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start=1&rows=1'\n",
    "acceptMediaType = 'application/json'\n",
    "response = requests.get(searchUri, headers = generateHeaderDictionary(acceptMediaType))\n",
    "data = response.json()\n",
    "#print(data)\n",
    "numberResults = data[\"num-found\"]\n",
    "print(data[\"num-found\"])\n",
    "numberPages = math.floor(numberResults/100)\n",
    "#print(numberPages)\n",
    "remainder = numberResults - 100*numberPages\n",
    "#print(remainder)\n",
    "\n",
    "for pageCount in range(0, numberPages+1):  # the remainder will be caught when pageCount = numberPages\n",
    "    print('page: ', pageCount)\n",
    "    searchUri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start='+str(pageCount*100+1)\n",
    "    response = requests.get(searchUri, headers={'Accept' : 'application/json'})\n",
    "    print(response.url)\n",
    "    data = response.json()\n",
    "    orcidsDictsList = data['result']\n",
    "\n",
    "    # extract the identifier strings from the data structure\n",
    "    orcids = []\n",
    "    for orcidDict in orcidsDictsList:\n",
    "        dictionary = {'id': orcidDict['orcid-identifier']['path'], 'iri': orcidDict['orcid-identifier']['uri']}\n",
    "        orcids.append(dictionary)\n",
    "\n",
    "    for orchidIndex in range(0, len(orcids)):\n",
    "        response = requests.get(orcids[orchidIndex]['iri'], headers={'Accept' : 'application/json'})\n",
    "        data = response.json()\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        orcidId = data['orcid-identifier']['path']\n",
    "        #print(orcidId)\n",
    "        # if there isn't a name, then go on to the next ORCID\n",
    "        if not data['person']['name']:\n",
    "            continue\n",
    "        if data['person']['name']['given-names']:  \n",
    "            givenNames = data['person']['name']['given-names']['value']\n",
    "        else:\n",
    "            continue\n",
    "        if data['person']['name']['family-name']:\n",
    "            familyName = data['person']['name']['family-name']['value']\n",
    "        # This has been a big pain when people don't have surnames.\n",
    "        # It causes matches with everyone who has the same first name!\n",
    "        else:\n",
    "            continue\n",
    "        #print(givenNames, ' ', familyName)\n",
    "        otherNames = data['person']['other-names']['other-name']\n",
    "        for otherName in otherNames:\n",
    "            #print(otherName['content'])\n",
    "            otherNameList.append([orcidId, otherName['content']])\n",
    "\n",
    "        affiliations = data['activities-summary']['employments']['affiliation-group']\n",
    "        #print(json.dumps(affiliations, indent = 2))\n",
    "        for affiliation in affiliations:\n",
    "            summaries = affiliation['summaries']\n",
    "            #print(summaries)\n",
    "            #print()\n",
    "            for summary in summaries:\n",
    "                employment = summary['employment-summary']\n",
    "                #print(json.dumps(employment, indent = 2))\n",
    "                startDate = ''\n",
    "                if employment['start-date']:\n",
    "                    if employment['start-date']['year']:\n",
    "                        startDate += employment['start-date']['year']['value']\n",
    "                        startMonth = employment['start-date']['month']\n",
    "                        if startMonth:\n",
    "                            startDate += '-' + startMonth['value']\n",
    "                            startDay = employment['start-date']['day']\n",
    "                            if startDay:\n",
    "                                startDate += '-' + startDay['value']\n",
    "                #print('start date: ', startDate)\n",
    "                endDate = ''\n",
    "                if employment['end-date']:\n",
    "                    if employment['end-date']['year']:\n",
    "                        endDate += employment['end-date']['year']['value']\n",
    "                        endMonth = employment['end-date']['month']\n",
    "                        if endMonth:\n",
    "                            endDate += '-' + endMonth['value']\n",
    "                            endDay = employment['end-date']['day']\n",
    "                            if endDay:\n",
    "                                endDate += '-' + endDay['value']\n",
    "                #print('end date: ', endDate)\n",
    "                department = employment['department-name']\n",
    "                # if there is no value for department, set it to empty string\n",
    "                if not department:\n",
    "                    department = ''\n",
    "                #print(department)\n",
    "                if employment['organization']:\n",
    "                    organization = employment['organization']['name']\n",
    "                #print(organization)\n",
    "                if 'Vanderbilt University' in organization:\n",
    "                    print(orcidId, givenNames, familyName, startDate, endDate, department, organization)\n",
    "                    table.append([orcidId, givenNames, familyName, startDate, endDate, department, organization])\n",
    "                #print(table)\n",
    "        sleep(.25)\n",
    "\n",
    "print()\n",
    "print('Done')\n",
    "fileName = 'orcid_data.csv'\n",
    "writeListsToCsv(fileName, table)\n",
    "fileName = 'orcid_other_names.csv'\n",
    "writeListsToCsv(fileName, otherNameList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical School faculty by department\n",
    "\n",
    "This is a multi-department directory, so after it is scraped, the departments need to be sorted out using the department column.\n",
    "\n",
    "## Scrape the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_uppercase\n",
    "outputTable = [['name', 'givenName', 'surname', 'degrees', 'rank', 'department', 'url', 'date', 'letter']]\n",
    "\n",
    "for letter in ascii_uppercase:\n",
    "#if 1==1:\n",
    "    #letter = 'Q'\n",
    "    print(letter)\n",
    "    acceptMediaType = 'text/html'\n",
    "    url = 'https://wag.app.vanderbilt.edu//PublicPage/Faculty/PickLetter?letter=' + letter\n",
    "    response = requests.get(url, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    soupObject = BeautifulSoup(response.text,features=\"html5lib\")\n",
    "\n",
    "    # get the first table from the page\n",
    "    tableObject = soupObject.find_all('tbody')[0]\n",
    "\n",
    "    facultyItems = tableObject.find_all('tr')\n",
    "\n",
    "    for personRecord in facultyItems:\n",
    "        column = personRecord.find_all('td')\n",
    "        localUrl = column[0].find('a')\n",
    "        url = 'https://wag.app.vanderbilt.edu' + localUrl.get('href')\n",
    "        nameLastFirst = column[1].text.strip()\n",
    "        nameParts = nameLastFirst.split(',')\n",
    "        firstName = nameParts[1].strip()\n",
    "        lastName = nameParts[0].strip()\n",
    "        name = firstName + ' ' + lastName\n",
    "        degrees = column[2].text.strip()\n",
    "        title = column[3].text.strip()\n",
    "        department = column[4].text.strip()\n",
    "        #print(name, degrees, title, department, url)    \n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "\n",
    "\n",
    "        outputTable.append([name, firstName, lastName, degrees, title, department, url, wholeDateZ, letter])            \n",
    "\n",
    "    fileName = 'medicine-faculty.csv'\n",
    "    writeListsToCsv(fileName, outputTable)\n",
    "    sleep(0.25)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate JSON for department-configuration.json\n",
    "\n",
    "For most departments, the configurations were hand-built, but since there are a bunch I generated it from a CSV file I created in Excel that had all of the keys below as the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this only needs to be done once\n",
    "\n",
    "# generate JSON for department-configuration.json\n",
    "file_name = 'departments/medicine-source.csv'\n",
    "source_data = readDict(file_name)\n",
    "config_dict = {}\n",
    "for department in source_data:\n",
    "    department_dict = {}\n",
    "    department_dict['scrapeType'] = 0\n",
    "    department_dict['categories'] = ['']\n",
    "    department_dict['baseUrl'] = 'https://wag.app.vanderbilt.edu//PublicPage/Faculty/PickLetter?letter='\n",
    "    department_dict['nTables'] = 1\n",
    "    department_dict['departmentSearchString'] = department['search_string']\n",
    "    department_dict['departmentQId'] = department['wikidataId']\n",
    "    department_dict['testAuthorAffiliation'] = department['test_affil']\n",
    "    department_dict['labels'] = {'source': 'column', 'value': 'name'}\n",
    "    department_dict['descriptions'] = {'source': 'constant', 'value': department['description']}\n",
    "    config_dict[department['short_name']] = department_dict    \n",
    "print(json.dumps(config_dict, indent=2))\n",
    "\n",
    "# copy and paste into config file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort faculty into separate department files\n",
    "\n",
    "The resulting files are substitutes for the separate web page scrapes done on all of the other departments. The output format is the same (column headers, roles JSON, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all of the -employees.csv files for the School of Medicine at once\n",
    "file_name = 'departments/medicine-source.csv'\n",
    "source_data = readDict(file_name)\n",
    "for department in source_data:\n",
    "    deptShortName = department['short_name']\n",
    "    directory_department = department['directory_string']\n",
    "    \n",
    "    accumulationTable = [['name', 'degree', 'role', 'category']]\n",
    "    fileName = 'departments/medicine-faculty.csv'\n",
    "    data = readDict(fileName)\n",
    "    count = 0\n",
    "    for faculty in data:\n",
    "        if faculty['department'] == directory_department:\n",
    "            count += 1\n",
    "            #print(faculty)\n",
    "            name = faculty['name']\n",
    "            degree = faculty['degrees']\n",
    "            category = faculty['letter'] # The surname first letter+directory base URL will be used for the source URL\n",
    "\n",
    "            roles = []\n",
    "            role_dict = {}\n",
    "            role_dict['title'] = faculty['rank']\n",
    "            role_dict['department'] = faculty['department']\n",
    "            roles.append(role_dict)\n",
    "            roles_json = json.dumps(roles)\n",
    "\n",
    "            accumulationTable.append([name, degree, roles_json, category])\n",
    "\n",
    "    fileName = 'departments/' + deptShortName + '-employees.csv'\n",
    "    writeListsToCsv(fileName, accumulationTable)\n",
    "    print(department['short_name'] + ' ' + str(count) + ' done')\n",
    "    \n",
    "# not necessary to do an individual department scrape for any Med School department (skip next section)\n",
    "# Set deptShortName in department-configuration.json to the department you want to work on,\n",
    "# then rerun the first code cell. Also move the -employees.csv file from the departments \n",
    "# subdirectory to the active directory to start working on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape departmental website\n",
    "\n",
    "script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/scrape-bsci.ipynb\n",
    "\n",
    "This is a conglomeration of purpose-built web scrapes for the web pages of a bunch of departments. The scraping methods are very ideosyncratic based on the format of the web pages, but they all output to the same CSV format that is input into the `vb2_match_orcid.py` script. The department to be scraped is determined by the value of `deptShortName` in `department-configuration.json`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bsci_type_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    # get the tables from the page\n",
    "    tableObjects = soupObject.find_all('table')\n",
    "    for tableObject in tableObjects:  # this assumes that all tables on the page contain names\n",
    "    \n",
    "        # get the rows from the table\n",
    "        rowObjectsList = tableObject.find_all('tr')\n",
    "        for rowObject in rowObjectsList:\n",
    "            try:\n",
    "                # get the cells from each row\n",
    "                cellObjectsList = rowObject.find_all('td')\n",
    "                # picture is in cell 0, name and title is in cell 1\n",
    "                nameCell = cellObjectsList[1]\n",
    "                # the name part is bolded\n",
    "                name = nameCell('strong')[0].text\n",
    "                # remove leading and trailing whitespace, including newlines\n",
    "                name = name.strip()\n",
    "            except:\n",
    "                # if it can't find the strong tag or the second cell, give up on that row\n",
    "                pass\n",
    "            #print(name)\n",
    "\n",
    "            # check to see if the name has already been added to the list (some depts put people on two category lists)\n",
    "            found = False\n",
    "            for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                if person[0] == name:\n",
    "                    found = True\n",
    "                    break  # quit looking for the person\n",
    "            if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                # separate degrees from names\n",
    "                degree = ''\n",
    "                for testDegree in degreeList:\n",
    "                    if testDegree['string'] in name:\n",
    "                        name = name.partition(', ' + testDegree['string'])[0]\n",
    "                        # correct any malformed strings\n",
    "                        degree = testDegree['value']\n",
    "\n",
    "                try:\n",
    "                    # process the roles text\n",
    "                    dirtyText  = str(nameCell)\n",
    "                    # get rid of trailing td tag\n",
    "                    nameCellText = dirtyText.split('</td>')[0]\n",
    "                    cellLines = nameCellText.split('<br/>')\n",
    "                    roles = []\n",
    "                    for lineIndex in range(1, len(cellLines)):\n",
    "                        roleDict = {}\n",
    "                        # remove leading and trailling whitespace\n",
    "                        rawText = cellLines[lineIndex].strip()\n",
    "                        if ' of ' in rawText:\n",
    "                            pieces = rawText.split(' of ')\n",
    "                            roleDict['title'] = pieces[0]\n",
    "                            roleDict['department'] = pieces[1]\n",
    "                            roles.append(roleDict)\n",
    "                        elif ' in ' in rawText:\n",
    "                            pieces = rawText.split(' in ')\n",
    "                            roleDict['title'] = pieces[0]\n",
    "                            roleDict['department'] = pieces[1]\n",
    "                            roles.append(roleDict)\n",
    "                        else:\n",
    "                            roleDict['title'] = rawText\n",
    "                            roleDict['department'] = ''\n",
    "                            roles.append(roleDict)\n",
    "                        if ', Emeritus' in roleDict['department']:\n",
    "                            roleDict['department'] = roleDict['department'].split(', Emeritus')[0]\n",
    "                            roleDict['title'] = 'Emeritus ' + roleDict['title']\n",
    "                    rolesJson = json.dumps(roles)\n",
    "\n",
    "                except:\n",
    "                    rolesJson = ''\n",
    "                accumulationTable.append([name, degree, rolesJson, category])\n",
    "    return accumulationTable\n",
    "\n",
    "def cineart_type_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    # get the tables from the page\n",
    "    tableObjects = soupObject.find_all('table')\n",
    "    for tableObject in tableObjects:\n",
    "        rowObjectsList = tableObject.find_all('tr')\n",
    "        for rowObject in rowObjectsList:\n",
    "            try:\n",
    "                # get the cells from each row\n",
    "                cellObjectsList = rowObject.find_all('td')\n",
    "                # picture is in cell 0, name is in cell 1\n",
    "                nameCell = cellObjectsList[1]\n",
    "                # the name part is heading 4\n",
    "                name = nameCell('h4')[0].text\n",
    "                # remove leading and trailing whitespace, including newlines\n",
    "                name = name.strip()\n",
    "            except:\n",
    "                # if it can't find the strong tag or the second cell, give up on that row\n",
    "                pass\n",
    "            accumulationTable.append([name, '', '[]', category])\n",
    "    return accumulationTable\n",
    "\n",
    "def amstudies_scrape(soupObject):\n",
    "    categories = ['administrative', 'core', 'secondary', 'affiliated']\n",
    "    accumulationTable = []\n",
    "    content = soupObject.find_all('section')[0]\n",
    "    article = content('article')[0]\n",
    "    ps = soupObject.find_all('p')\n",
    "    for p in ps:\n",
    "        # ideosyncratic screen for administrators\n",
    "        found = False\n",
    "        if len(p.find_all('a')) != 0:\n",
    "            possibleName = p.find_all('a')[0] # admin faculty names are in the a tags\n",
    "            if not '@' in possibleName.text: # eliminate the email address a tags\n",
    "                if not '?' in possibleName.get('href'): # eliminate link with (c) in href value\n",
    "                    found = True\n",
    "                    name = possibleName.text\n",
    "        if found:\n",
    "            stringText = str(p)\n",
    "            role = stringText.split('<br/>')[1].strip()\n",
    "\n",
    "            accumRoles = []\n",
    "            roleDict = {}\n",
    "            roleDict['title'] = role\n",
    "            roleDict['department'] = role\n",
    "            accumRoles.append(roleDict)\n",
    "            accumulationTable.append([name, '', json.dumps(accumRoles), 'administrative'])            \n",
    "            #accumulationTable.append([name, '', '[\"title\": \"' + role + '\"]', 'administrative'])\n",
    "        # screen for core faculty\n",
    "        secondFound = False\n",
    "        if not found:\n",
    "            names = p.find_all('strong')\n",
    "            if len(names) == 1:\n",
    "                secondFound = True\n",
    "                name = names[0].text.strip()\n",
    "                category = 'core'\n",
    "        if secondFound:\n",
    "            stringText = str(p)\n",
    "            role = stringText.split('<br/>')[1].strip()\n",
    "            if role != '</strong>Program Administrator':  \n",
    "                accumRoles = []\n",
    "                roleDict = {}\n",
    "                roleDict['title'] = role\n",
    "                roleDict['department'] = role\n",
    "                accumRoles.append(roleDict)\n",
    "                accumulationTable.append([name, '', json.dumps(accumRoles), 'core'])\n",
    "                #accumulationTable.append([name, '', '[\"title\": \"' + role + '\"]', 'core'])\n",
    "    outerDivs = soupObject.find_all('div')\n",
    "    outerDiv = outerDivs[5]\n",
    "    innerDivs = outerDiv.find_all('div')\n",
    "    secondaryDiv = innerDivs[2]\n",
    "    accumulationTable = pull_amstudies_divs(secondaryDiv, 'secondary', accumulationTable)\n",
    "    affiliatedDiv = innerDivs[5]\n",
    "    accumulationTable = pull_amstudies_divs(affiliatedDiv, 'affiliated', accumulationTable)\n",
    "\n",
    "    return accumulationTable\n",
    "\n",
    "# Note: this is so ideosyncratic that the roles need to be manually edited after running\n",
    "def pull_amstudies_divs(div, category, accumulationTable):\n",
    "    if category == 'secondary':\n",
    "        p = div.find_all('div')[1]\n",
    "    else:\n",
    "        p = div\n",
    "    names = p.find_all('strong')\n",
    "    text = str(p)\n",
    "    rolesBlobs = text.split(',')\n",
    "    roles = []\n",
    "    for roleString in rolesBlobs[1:len(rolesBlobs)]:\n",
    "        role = roleString.split('<')[0].strip()\n",
    "        if role != 'Health':\n",
    "            if role != 'and Society':\n",
    "                if role != 'and Society and Anthropology':\n",
    "                    if 'of Medicine' in role:\n",
    "                        roles.append(role + ', Health, and Society')\n",
    "                    else:\n",
    "                        roles.append(role)\n",
    "    for personNumber in range(0, len(names)):\n",
    "        accumRoles = []\n",
    "        roleDict = {}\n",
    "        roleDict['title'] = roles[personNumber]\n",
    "        roleDict['department'] = roles[personNumber]\n",
    "        accumRoles.append(roleDict)\n",
    "        accumulationTable.append([names[personNumber].text, '', json.dumps(accumRoles), category])\n",
    "    return accumulationTable\n",
    "\n",
    "def art_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    divObjects = soupObject.find_all('div')\n",
    "    for div in divObjects:\n",
    "        try:\n",
    "            if div.get('class')[0] == 'row':\n",
    "                pObjects = div.find_all('p')\n",
    "                for p in pObjects:\n",
    "                    aObjects = p.find_all('a')\n",
    "                    for a in aObjects:\n",
    "                        name = a.text\n",
    "                        if not '@' in name:\n",
    "                            if name != 'CV':\n",
    "                                if name != 'F':\n",
    "                                    if name == 'arrar Hood Cusomato':\n",
    "                                        name = 'Farrar Hood Cusomato'\n",
    "\n",
    "                                    # avoid duplicate entries\n",
    "                                    found = False\n",
    "                                    for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                                        if person[0] == name:\n",
    "                                            found = True\n",
    "                                            break  # quit looking for the person\n",
    "                                    if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                                        accumulationTable.append([name, '', '[]', category])\n",
    "        except:\n",
    "            pass\n",
    "    return accumulationTable\n",
    "\n",
    "def asian_studies_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    divObjects = soupObject.find_all('div')\n",
    "    for div in divObjects:\n",
    "        try:\n",
    "            if div.get('class')[0] == 'row':\n",
    "                pObjects = div.find_all('p')\n",
    "                for p in pObjects:\n",
    "                    aObjects = p.find_all('a')\n",
    "                    if len(aObjects) >= 1:\n",
    "                        for a in aObjects:\n",
    "                            if not '@' in str(a):\n",
    "                                name = a.text.strip()\n",
    "                                if name != '':\n",
    "                                    if name != 'Alejandro':\n",
    "                                        if name == 'Acierto':\n",
    "                                            name = 'Alejandro Acierto'\n",
    "                                            \n",
    "                                        # avoid duplicate entries\n",
    "                                        found = False\n",
    "                                        for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                                            if person[0] == name:\n",
    "                                                found = True\n",
    "                                                break  # quit looking for the person\n",
    "                                        if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                                            accumulationTable.append([name, '', '[]', category])\n",
    "        except:\n",
    "            pass\n",
    "    return accumulationTable\n",
    "\n",
    "def chemistry_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    # get the tables from the page\n",
    "    tableObjects = soupObject.find_all('table')\n",
    "    for tableObject in tableObjects:\n",
    "        pObjects = tableObject.find_all('p') # first two tables (primary and secondary appointments) have p elements\n",
    "        for p in pObjects:\n",
    "            if p.text.strip() != '':\n",
    "                name = p.text.strip()\n",
    "                # avoid duplicate entries\n",
    "                found = False\n",
    "                for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                    if person[0] == name:\n",
    "                        found = True\n",
    "                        break  # quit looking for the person\n",
    "                if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                    accumulationTable.append([name, '', '[]', category])\n",
    "        if len(pObjects) == 0: # last tables (non-tenure track) don't have p elements\n",
    "            try:\n",
    "                rowObjects = tableObject.find_all('tr') # last tables (non-tenure track) have tr elements\n",
    "                for rowObject in rowObjects:\n",
    "                    columnObjects = rowObject.find_all('td')\n",
    "                    if columnObjects[0].text.strip() != 'Name':\n",
    "                        name = columnObjects[0].text.strip()\n",
    "                        # avoid duplicate entries\n",
    "                        found = False\n",
    "                        for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                            if person[0] == name:\n",
    "                                found = True\n",
    "                                break  # quit looking for the person\n",
    "                        if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                            accumulationTable.append([name, '', '[]', category])\n",
    "            except:\n",
    "                pass\n",
    "    return accumulationTable\n",
    "        \n",
    "def comsci_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    divObjects = soupObject.find_all('div')\n",
    "    for div in divObjects:\n",
    "        try:\n",
    "            if div.get('class')[0] == 'panel-body':\n",
    "                pObjects = div.find_all('a')\n",
    "                for p in pObjects:\n",
    "                    name = p.text.strip()\n",
    "                    # avoid duplicate entries\n",
    "                    found = False\n",
    "                    for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                        if person[0] == name:\n",
    "                            found = True\n",
    "                            break  # quit looking for the person\n",
    "                    if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                        accumulationTable.append([name, '', '[]', category])\n",
    "        except:\n",
    "            pass\n",
    "    return accumulationTable\n",
    "\n",
    "def communication_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    articleObjects = soupObject.find_all('article')\n",
    "    for article in articleObjects:\n",
    "        try:\n",
    "            if article.get('class')[0] == 'primary-content':\n",
    "                divObjects = article.find_all('div')\n",
    "                for divObject in divObjects:\n",
    "                    try:\n",
    "                        if divObject.get('class')[1] == 'four_fifth':\n",
    "                            aObjects = divObject.find_all('a')\n",
    "                            if aObjects[0].text.strip() != 'Stephanie Covington':\n",
    "                                name = aObjects[0].text.strip()\n",
    "                                # avoid duplicate entries\n",
    "                                found = False\n",
    "                                for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                                    if person[0] == name:\n",
    "                                        found = True\n",
    "                                        break  # quit looking for the person\n",
    "                                if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                                    accumulationTable.append([name, '', '[]', category])\n",
    "                    except:\n",
    "                        pass\n",
    "        except:\n",
    "            pass\n",
    "    return accumulationTable\n",
    "\n",
    "def europeanstudies_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    articleObjects = soupObject.find_all('article')\n",
    "    for article in articleObjects:\n",
    "        if article.get('class')[0] == 'primary-content':\n",
    "            strongObjects = article.find_all('strong')\n",
    "            for strongObject in strongObjects:\n",
    "                name = strongObject.text.strip()\n",
    "                if name[-1] == ',':\n",
    "                    name = name[0:len(name)-1]\n",
    "                accumulationTable.append([name, '', '[]', category])\n",
    "    return accumulationTable\n",
    "\n",
    "def frit_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    articleObjects = soupObject.find_all('article')\n",
    "    for article in articleObjects:\n",
    "        if article.get('class')[0] == 'primary-content':\n",
    "            pObjects = article.find_all('p')\n",
    "            for pObject in pObjects:\n",
    "                if '<a ' in str(pObject):\n",
    "                    if '<strong>' in str(pObject):\n",
    "                        aObjects = pObject.find_all('a')\n",
    "                        for aObject in aObjects:\n",
    "                            textBlob = aObject.text.strip()\n",
    "                            if textBlob != 'email':\n",
    "                                name = textBlob\n",
    "\n",
    "                                # avoid duplicate entries\n",
    "                                found = False\n",
    "                                for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                                    if person[0] == name:\n",
    "                                        found = True\n",
    "                                        break  # quit looking for the person\n",
    "                                if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                                    accumulationTable.append([name, '', '[]', category])\n",
    "    return accumulationTable\n",
    "\n",
    "def historyart_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    articleObjects = soupObject.find_all('article')\n",
    "    for article in articleObjects:\n",
    "        if article.get('class')[0] == 'primary-content':\n",
    "            pObjects = article.find_all('p')\n",
    "            for pObject in pObjects:\n",
    "                aObjects = pObject.find_all('strong')\n",
    "                for aObject in aObjects:\n",
    "                    textBlob = aObject.text.strip()\n",
    "                    if textBlob !='EMERITI':\n",
    "                        name = textBlob\n",
    "                        \n",
    "                        # avoid duplicate entries\n",
    "                        found = False\n",
    "                        for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                            if person[0] == name:\n",
    "                                found = True\n",
    "                                break  # quit looking for the person\n",
    "                        if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                            accumulationTable.append([name, '', '[]', category])\n",
    "    return accumulationTable\n",
    "\n",
    "def jewishstudies_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    articleObjects = soupObject.find_all('article')\n",
    "    for article in articleObjects:\n",
    "        if article.get('class')[0] == 'primary-content':\n",
    "            aObjects = article.find_all('a')\n",
    "            for aObject in aObjects:\n",
    "                name = aObject.text.strip()\n",
    "\n",
    "                # avoid duplicate entries\n",
    "                found = False\n",
    "                for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                    if person[0] == name:\n",
    "                        found = True\n",
    "                        break  # quit looking for the person\n",
    "                if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                    accumulationTable.append([name, '', '[]', category])\n",
    "    return accumulationTable\n",
    "\n",
    "def latinx_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    tableObjects = soupObject.find_all('table')\n",
    "    rowObjects = tableObjects[0].find_all('tr')\n",
    "    for rowObject in rowObjects:\n",
    "        pObjects = rowObject.find_all('p')\n",
    "        name = pObjects[0].text.strip()\n",
    "\n",
    "        # avoid duplicate entries\n",
    "        found = False\n",
    "        for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "            if person[0] == name:\n",
    "                found = True\n",
    "                break  # quit looking for the person\n",
    "        if not found:  # only finish extracting and saving data if there isn't a match\n",
    "            accumulationTable.append([name, '', '[]', category])\n",
    "\n",
    "def pps_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    articleObjects = soupObject.find_all('article')\n",
    "    for article in articleObjects:\n",
    "        if article.get('class')[0] == 'primary-content':\n",
    "            strongObjects = article.find_all('strong')\n",
    "            for strongObject in strongObjects:\n",
    "                found = False\n",
    "                aObjects = strongObject.find_all('a') # get all of the people with hyperlinked names\n",
    "                for aObject in aObjects:\n",
    "                    name = aObject.text.strip()\n",
    "                    found = True # found a name this way\n",
    "                if not found: # no hyperlinked name, have to parse out from full string\n",
    "                    textBlob = strongObject.text.strip()\n",
    "                    possibleName = textBlob.split(',')[0]\n",
    "                    if possibleName != 'Associate Professor NTT':\n",
    "                        name = possibleName\n",
    "\n",
    "                # avoid duplicate entries\n",
    "                found = False\n",
    "                for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                    if person[0] == name:\n",
    "                        found = True\n",
    "                        break  # quit looking for the person\n",
    "                if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                    accumulationTable.append([name, '', '[]', category])\n",
    "    return accumulationTable\n",
    "\n",
    "def religiousstudies_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    articleObjects = soupObject.find_all('article')\n",
    "    for article in articleObjects:\n",
    "        if article.get('class')[0] == 'primary-content':\n",
    "                aObjects = article.find_all('a') # get all of the people with hyperlinked names\n",
    "                for aObject in aObjects:\n",
    "                    name = aObject.text.strip()\n",
    "                    if name != '':\n",
    "                        if name != ':':\n",
    "                            if name != 'Alphabetical':\n",
    "                                if name[-1] == ':': # strip off trailing colons\n",
    "                                    name = name[0:len(name)-1]\n",
    "\n",
    "                                # avoid duplicate entries\n",
    "                                found = False\n",
    "                                for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                                    if person[0] == name:\n",
    "                                        found = True\n",
    "                                        break  # quit looking for the person\n",
    "                                if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                                    accumulationTable.append([name, '', '[]', category])\n",
    "    return accumulationTable\n",
    "\n",
    "def wgs_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    articleObjects = soupObject.find_all('article')\n",
    "    for article in articleObjects:\n",
    "        if article.get('class')[0] == 'primary-content':\n",
    "            if category == '':\n",
    "                    divObjects = article.find_all('div') # get all of the people with hyperlinked names\n",
    "                    for divObject in divObjects:\n",
    "                        try:\n",
    "                            name = divObject.find_all('strong')[0].text.strip()\n",
    "\n",
    "                            # avoid duplicate entries\n",
    "                            found = False\n",
    "                            for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                                if person[0] == name:\n",
    "                                    found = True\n",
    "                                    break  # quit looking for the person\n",
    "                            if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                                accumulationTable.append([name, '', '[]', category])\n",
    "                        except:\n",
    "                            pass\n",
    "            else:\n",
    "                pObjects = article.find_all('p')\n",
    "                for pObject in pObjects:\n",
    "                    aObjects = pObject.find_all('a')\n",
    "                    if len(aObjects) > 0:\n",
    "                        name = aObjects[0].text.strip()\n",
    "                        if ', PhD' in name:\n",
    "                            name = name[0:len(name)-5]\n",
    "\n",
    "                        # avoid duplicate entries\n",
    "                        found = False\n",
    "                        for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                            if person[0] == name:\n",
    "                                found = True\n",
    "                                break  # quit looking for the person\n",
    "                        if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                            accumulationTable.append([name, '', '[]', category])\n",
    "    return accumulationTable\n",
    "    \n",
    "def law_scrape(soupObject, category):\n",
    "    accumulationTable = []\n",
    "    tableObject = soupObject.find_all('table')[0] # the first table has the names\n",
    "    for rowObject in tableObject.find_all('tr'):\n",
    "        tdObjects = rowObject.find_all('td')\n",
    "        name = tdObjects[1].text.strip()\n",
    "        if name != 'Name': #skip the header row of the table to be scraped\n",
    "            titleString = tdObjects[2].text.strip()\n",
    "\n",
    "            cellLines = titleString.split('<br>')\n",
    "            roles = []\n",
    "            for lineIndex in range(0, len(cellLines)):\n",
    "                roleDict = {}\n",
    "                # remove leading and trailling whitespace\n",
    "                rawText = cellLines[lineIndex].strip()\n",
    "                if ' of ' in rawText:\n",
    "                    pieces = rawText.split(' of ')\n",
    "                    roleDict['title'] = pieces[0]\n",
    "                    roleDict['department'] = pieces[1]\n",
    "                    roles.append(roleDict)\n",
    "                elif ' in ' in rawText:\n",
    "                    pieces = rawText.split(' in ')\n",
    "                    roleDict['title'] = pieces[0]\n",
    "                    roleDict['department'] = pieces[1]\n",
    "                    roles.append(roleDict)\n",
    "                else:\n",
    "                    roleDict['title'] = rawText\n",
    "                    roleDict['department'] = ''\n",
    "                    roles.append(roleDict)\n",
    "                if ', Emeritus' in roleDict['department']:\n",
    "                    roleDict['department'] = roleDict['department'].split(', Emeritus')[0]\n",
    "                    roleDict['title'] = 'Emeritus ' + roleDict['title']\n",
    "            rolesJson = json.dumps(roles)\n",
    "\n",
    "            \n",
    "            # avoid duplicate entries\n",
    "            found = False\n",
    "            for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                if person[0] == name:\n",
    "                    found = True\n",
    "                    break  # quit looking for the person\n",
    "            if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                accumulationTable.append([name, '', rolesJson, category])\n",
    "    return accumulationTable\n",
    "\n",
    "def engineering_type_scrape(soupObject, category, dept_name):\n",
    "    accumulationTable = []\n",
    "    tableObject = soupObject.find_all('table')[0] # the first table has the names\n",
    "    for rowObject in tableObject.find_all('tr'):\n",
    "        tdObjects = rowObject.find_all('td')\n",
    "        try:\n",
    "            name = tdObjects[1].find('a').text.strip()\n",
    "            \n",
    "            roles = []\n",
    "            roleDict = {}\n",
    "            roleDict['title'] = 'faculty'\n",
    "            roleDict['department'] = dept_name\n",
    "            roles.append(roleDict)\n",
    "            rolesJson = json.dumps(roles)\n",
    "\n",
    "            # avoid duplicate entries\n",
    "            found = False\n",
    "            for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                if person[0] == name:\n",
    "                    found = True\n",
    "                    break  # quit looking for the person\n",
    "            if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                accumulationTable.append([name, '', rolesJson, category])\n",
    "        except:\n",
    "            pass\n",
    "    return accumulationTable\n",
    "\n",
    "def materials_science_scrape(soupObject, category, dept_name):\n",
    "    accumulationTable = []\n",
    "    tableObject = soupObject.find_all('table')[0] # the first table has the names\n",
    "    rowObjects = tableObject.find_all('tr')\n",
    "    for rowObject in rowObjects[1:len(rowObjects)]: # skip the header row\n",
    "        tdObjects = rowObject.find_all('td')\n",
    "        name = tdObjects[1].text.strip() + ' ' + tdObjects[0].text.strip() # cells contain surname, given name\n",
    "\n",
    "        roles = []\n",
    "        roleDict = {}\n",
    "        roleDict['title'] = 'faculty'\n",
    "        roleDict['department'] = dept_name\n",
    "        roles.append(roleDict)\n",
    "        rolesJson = json.dumps(roles)\n",
    "\n",
    "        # avoid duplicate entries\n",
    "        found = False\n",
    "        for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "            if person[0] == name:\n",
    "                found = True\n",
    "                break  # quit looking for the person\n",
    "        if not found:  # only finish extracting and saving data if there isn't a match\n",
    "            accumulationTable.append([name, '', rolesJson, category])\n",
    "            \n",
    "    return accumulationTable\n",
    "\n",
    "def chbe_postdoc_scrape(soupObject, category, dept_name):\n",
    "    accumulationTable = []\n",
    "    tableObject = soupObject.find_all('table')[0] # the first table has the names\n",
    "    rowObjects = tableObject.find_all('tr')\n",
    "    for rowObject in rowObjects:\n",
    "        tdObjects = rowObject.find_all('td')\n",
    "        name = tdObjects[0].text.strip()\n",
    "        print(name)\n",
    "\n",
    "        roles = []\n",
    "        roleDict = {}\n",
    "        roleDict['title'] = 'postdoc'\n",
    "        roleDict['department'] = dept_name\n",
    "        roles.append(roleDict)\n",
    "        rolesJson = json.dumps(roles)\n",
    "\n",
    "        # avoid duplicate entries\n",
    "        found = False\n",
    "        for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "            if person[0] == name:\n",
    "                found = True\n",
    "                break  # quit looking for the person\n",
    "        if not found:  # only finish extracting and saving data if there isn't a match\n",
    "            accumulationTable.append([name, '', rolesJson, category])\n",
    "            \n",
    "    return accumulationTable\n",
    "\n",
    "def cee_staff_scrape(soupObject, category, dept_name):\n",
    "    accumulationTable = []\n",
    "    tableObject = soupObject.find_all('table')[0] # the first table has the names\n",
    "    rowObjects = tableObject.find_all('tr')\n",
    "    for rowObject in rowObjects:\n",
    "        tdObjects = rowObject.find_all('td')\n",
    "        name = tdObjects[0].text.strip()\n",
    "        title = tdObjects[3].text.strip()\n",
    "        if 'Research' in title or 'Engineer' in title:\n",
    "            print(name)\n",
    "            \n",
    "            roles = []\n",
    "            roleDict = {}\n",
    "            roleDict['title'] = 'researchstaff'\n",
    "            roleDict['department'] = dept_name\n",
    "            roles.append(roleDict)\n",
    "            rolesJson = json.dumps(roles)\n",
    "\n",
    "            # avoid duplicate entries\n",
    "            found = False\n",
    "            for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                if person[0] == name:\n",
    "                    found = True\n",
    "                    break  # quit looking for the person\n",
    "            if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                accumulationTable.append([name, '', rolesJson, category])\n",
    "\n",
    "    return accumulationTable\n",
    "\n",
    "def eecs_postdoc_scrape(soupObject, category, dept_name):\n",
    "    accumulationTable = []\n",
    "    tableObject = soupObject.find_all('table')[0] # the first table has the names\n",
    "    rowObjects = tableObject.find_all('tr')\n",
    "    for rowObject in rowObjects:\n",
    "        tdObjects = rowObject.find_all('td')\n",
    "        for tdObject in tdObjects:\n",
    "            boldObjects = tdObject.find_all('strong')\n",
    "            if len(boldObjects) == 1:\n",
    "                nameString = boldObjects[0].text.strip()\n",
    "                namePieces = nameString.split(', ')\n",
    "                name = namePieces[1].strip() + ' ' + namePieces[0].strip()\n",
    "\n",
    "                roles = []\n",
    "                roleDict = {}\n",
    "                roleDict['title'] = 'postdoc'\n",
    "                roleDict['department'] = dept_name\n",
    "                roles.append(roleDict)\n",
    "                rolesJson = json.dumps(roles)\n",
    "\n",
    "                # avoid duplicate entries\n",
    "                found = False\n",
    "                for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                    if person[0] == name:\n",
    "                        found = True\n",
    "                        break  # quit looking for the person\n",
    "                if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                    accumulationTable.append([name, '', rolesJson, category])\n",
    "\n",
    "    return accumulationTable\n",
    "\n",
    "def owen_scrape(soupObject, category, dept_name):\n",
    "    accumulationTable = []\n",
    "    tableObjects = soupObject.find_all('div')\n",
    "    for table in tableObjects:\n",
    "        try:\n",
    "            if table.get('class')[0] == 'profile-listing':\n",
    "                nameObjects = table.find_all('h3')\n",
    "                for nameObject in nameObjects:\n",
    "                    name = nameObject.text.strip()\n",
    "                    # try to get rid of unprintable characters\n",
    "                    filter(lambda x: x in name.printable, name)\n",
    "                    # get rid of suffixes\n",
    "                    if 'CPA, CFE' in name:\n",
    "                        name = name[0:len(name)-9]\n",
    "                    elif 'CPA' in name:\n",
    "                        name = name[0:len(name)-4]\n",
    "                    elif 'M.D.' in name:\n",
    "                        name = name[0:len(name)-4]\n",
    "                    elif ' JD' in name:\n",
    "                        name = name[0:len(name)-3]\n",
    "                    # remove duplicate whitespace\n",
    "                    name = ' '.join(name.split())\n",
    "\n",
    "                    roles = []\n",
    "                    roleDict = {}\n",
    "                    roleDict['title'] = 'scholar'\n",
    "                    roleDict['department'] = dept_name\n",
    "                    roles.append(roleDict)\n",
    "                    rolesJson = json.dumps(roles)\n",
    "\n",
    "                    # avoid duplicate entries\n",
    "                    found = False\n",
    "                    for person in accumulationTable:  # not worrying about the header row, which shouldn't match any name\n",
    "                        if person[0] == name:\n",
    "                            found = True\n",
    "                            break  # quit looking for the person\n",
    "                    if not found:  # only finish extracting and saving data if there isn't a match\n",
    "                        accumulationTable.append([name, '', rolesJson, category])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return accumulationTable\n",
    "\n",
    "outputTable = [['name', 'degree', 'role', 'category']]\n",
    "categories = deptSettings[deptShortName]['categories']\n",
    "dept_name = deptSettings[deptShortName]['departmentSearchString']\n",
    "\n",
    "acceptMediaType = 'text/html'\n",
    "for category in categories:\n",
    "    url = deptSettings[deptShortName]['baseUrl'] + category\n",
    "    response = requests.get(url, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    soupObject = BeautifulSoup(response.text,features=\"html5lib\")\n",
    "    scrapeType = deptSettings[deptShortName]['scrapeType']\n",
    "    if scrapeType == 1:\n",
    "        temp = bsci_type_scrape(soupObject, category)\n",
    "    elif scrapeType == 2:\n",
    "        temp = cineart_type_scrape(soupObject, category)\n",
    "    elif scrapeType == 3:\n",
    "        temp = amstudies_scrape(soupObject)\n",
    "    elif scrapeType == 4:\n",
    "        temp = art_scrape(soupObject, category)\n",
    "    elif scrapeType == 5:\n",
    "        temp = asian_studies_scrape(soupObject, category)\n",
    "    elif scrapeType == 6:\n",
    "        temp = chemistry_scrape(soupObject, category)\n",
    "    elif scrapeType == 7:\n",
    "        temp = comsci_scrape(soupObject, category)\n",
    "    elif scrapeType == 8:\n",
    "        temp = communication_scrape(soupObject, category)\n",
    "    elif scrapeType == 9:\n",
    "        temp = europeanstudies_scrape(soupObject, category)\n",
    "    elif scrapeType == 10:\n",
    "        temp = frit_scrape(soupObject, category)\n",
    "    elif scrapeType == 11:\n",
    "        temp = historyart_scrape(soupObject, category)\n",
    "    elif scrapeType == 12:\n",
    "        temp = jewishstudies_scrape(soupObject, category)\n",
    "    elif scrapeType == 13:\n",
    "        temp = latinx_scrape(soupObject, category)\n",
    "    elif scrapeType == 14:\n",
    "        temp = pps_scrape(soupObject, category)\n",
    "    elif scrapeType == 15:\n",
    "        temp = religiousstudies_scrape(soupObject, category)\n",
    "    elif scrapeType == 16:\n",
    "        temp = wgs_scrape(soupObject, category)\n",
    "    elif scrapeType == 17:\n",
    "        temp = law_scrape(soupObject, category)\n",
    "    elif scrapeType == 18:\n",
    "        temp = engineering_type_scrape(soupObject, category, dept_name)\n",
    "    elif scrapeType == 19:\n",
    "        temp = materials_science_scrape(soupObject, category, dept_name)\n",
    "    elif scrapeType == 20:\n",
    "        temp = chbe_postdoc_scrape(soupObject, category, dept_name)\n",
    "    elif scrapeType == 21:\n",
    "        temp = cee_staff_scrape(soupObject, category, dept_name)\n",
    "    elif scrapeType == 22:\n",
    "        temp = eecs_postdoc_scrape(soupObject, category, dept_name)\n",
    "    elif scrapeType == 23:\n",
    "        temp = owen_scrape(soupObject, category, dept_name)\n",
    "        \n",
    "    # deduplicate any people on the new list that were already on the previous list\n",
    "    buildTable = []\n",
    "    for person in temp: # not worrying about the header row, which shouldn't match any name\n",
    "        found = False\n",
    "        for existing in outputTable:\n",
    "            if person[0] == existing[0]:\n",
    "                found = True\n",
    "                break  # quit looking for the person\n",
    "        if not found:  # only save data if there isn't a match\n",
    "            buildTable.append(person)\n",
    "    outputTable += buildTable\n",
    "\n",
    "fileName = deptShortName + '-employees.csv'\n",
    "writeListsToCsv(fileName, outputTable)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Vanderbilt people's altLabels from Wikidata\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/download-vanderbilt-people-altlabels.py\n",
    "\n",
    "These values aren't used for anything currently (2020-03-19), so running this is optional. But it will be useful in the future when we want to start collecting aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''select distinct  ?person ?altLabel where {\n",
    "  ?person p:P108 ?statement.\n",
    "  ?statement ps:P108  wd:Q29052.\n",
    "  ?person skos:altLabel ?altLabel.\n",
    "  FILTER(lang(?altLabel)=\"en\")\n",
    "}'''\n",
    "\n",
    "# The endpoint defaults to returning XML, so the Accept: header is required\n",
    "r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers={'Accept' : 'application/json'})\n",
    "\n",
    "data = r.json()\n",
    "#print(json.dumps(data,indent = 2))\n",
    "\n",
    "table = [['wikidataIri', 'altLabel']]\n",
    "items = data['results']['bindings']\n",
    "for item in items:\n",
    "    wikidataIri = item['person']['value']\n",
    "    altLabel = ''\n",
    "    if 'altLabel' in item:\n",
    "        altLabel = item['altLabel']['value']\n",
    "    table.append([wikidataIri, altLabel])\n",
    "    \n",
    "fileName = 'vanderbilt_wikidata_altlabels.csv'\n",
    "writeListsToCsv(fileName, table)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
