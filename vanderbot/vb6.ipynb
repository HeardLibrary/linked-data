{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "sparqlSleep = 0.25 # delay time between calls to SPARQL endpoint\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# function definitions\n",
    "\n",
    "def retrieveCredentials(path):\n",
    "    with open(path, 'rt') as fileObject:\n",
    "        lineList = fileObject.read().split('\\n')\n",
    "    endpointUrl = lineList[0].split('=')[1]\n",
    "    username = lineList[1].split('=')[1]\n",
    "    password = lineList[2].split('=')[1]\n",
    "    userAgent = lineList[3].split('=')[1]\n",
    "    credentials = [endpointUrl, username, password, userAgent]\n",
    "    return credentials\n",
    "\n",
    "def getLoginToken(apiUrl):    \n",
    "    parameters = {\n",
    "        'action':'query',\n",
    "        'meta':'tokens',\n",
    "        'type':'login',\n",
    "        'format':'json'\n",
    "    }\n",
    "    r = session.get(url=apiUrl, params=parameters)\n",
    "    data = r.json()\n",
    "    return data['query']['tokens']['logintoken']\n",
    "\n",
    "def logIn(apiUrl, token, username, password):\n",
    "    parameters = {\n",
    "        'action':'login',\n",
    "        'lgname':username,\n",
    "        'lgpassword':password,\n",
    "        'lgtoken':token,\n",
    "        'format':'json'\n",
    "    }\n",
    "    r = session.post(apiUrl, data=parameters)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "def getCsrfToken(apiUrl):\n",
    "    parameters = {\n",
    "        \"action\": \"query\",\n",
    "        \"meta\": \"tokens\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    r = session.get(url=apiUrl, params=parameters)\n",
    "    data = r.json()\n",
    "    return data[\"query\"][\"tokens\"][\"csrftoken\"]\n",
    "\n",
    "# read a CSV into a list of dictionaries\n",
    "def readDict(filename):\n",
    "    fileObject = open(filename, 'r', newline='', encoding='utf-8')\n",
    "    dictObject = csv.DictReader(fileObject)\n",
    "    array = []\n",
    "    for row in dictObject:\n",
    "        array.append(row)\n",
    "    fileObject.close()\n",
    "    return array\n",
    "\n",
    "# write the data to a file\n",
    "def writeToFile(tableFileName, fieldnames, tableData):\n",
    "    with open(tableFileName, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for writeRowNumber in range(0, len(tableData)):\n",
    "            writer.writerow(tableData[writeRowNumber])\n",
    "\n",
    "# gunction to get local name from an IRI\n",
    "def extractFromIri(iri, numberPieces):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[numberPieces]\n",
    "\n",
    "# search for any of the \"label\" types: label, alias, description\n",
    "def searchLabelsDescriptionsAtWikidata(qIds, labelType, language):\n",
    "    # configuration settings\n",
    "    endpointUrl = 'https://query.wikidata.org/sparql'\n",
    "    acceptMediaType = 'application/json'\n",
    "    userAgentHeader = 'VanderBot/1.3 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "    'Content-Type': 'application/sparql-query',\n",
    "    'Accept' : acceptMediaType,\n",
    "    'User-Agent': userAgentHeader\n",
    "    }\n",
    "\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    if labelType == 'label':\n",
    "        predicate = 'rdfs:label'\n",
    "    elif labelType == 'alias':\n",
    "        predicate = 'skos:altLabel'\n",
    "    elif labelType == 'description':\n",
    "        predicate = 'schema:description'\n",
    "    else:\n",
    "        predicate = 'rdfs:label'        \n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?string '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id '''+ predicate + ''' ?string.\n",
    "  filter(lang(?string)=\"''' + language + '''\")\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    # r = requests.get(endpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpointUrl, data=query, headers=requestHeaderDictionary)\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        string = result['string']['value']\n",
    "        resultsDict = {'qId': qNumber, 'string': string}\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparqlSleep)\n",
    "    \n",
    "    return returnValue\n",
    "\n",
    "# Function to convert times to the format required by Wikidata\n",
    "def convertDates(rowData, dateColumnNameRoot):\n",
    "    error = False\n",
    "    # Only do something in the case where there is a date. Missing values should be skipped.\n",
    "    if rowData[dateColumnNameRoot + '_val'] != '':\n",
    "        # Assume that if the precision column is empty that the dates need to be converted\n",
    "        if rowData[dateColumnNameRoot + '_prec'] == '':\n",
    "            #print(dateColumnNameRoot, rowData[dateColumnNameRoot + '_val'])\n",
    "\n",
    "            # set these two to default to the existing values\n",
    "            # precisionNumber = int(rowData[dateColumnNameRoot + '_prec']) # not necessary since conditional on value of ''\n",
    "            timeString = rowData[dateColumnNameRoot + '_val']\n",
    "\n",
    "            value = rowData[dateColumnNameRoot + '_val']\n",
    "            # date is YYYY-MM-DD\n",
    "            if len(value) == 10:\n",
    "                timeString = value + 'T00:00:00Z'\n",
    "                precisionNumber = 11 # precision to days\n",
    "            # date is YYYY-MM\n",
    "            elif len(value) == 7:\n",
    "                timeString = value + '-00T00:00:00Z'\n",
    "                precisionNumber = 10 # precision to months\n",
    "            # date is YYYY\n",
    "            elif len(value) == 4:\n",
    "                timeString = value + '-00-00T00:00:00Z'\n",
    "                precisionNumber = 9 # precision to years\n",
    "            # date is xsd:dateTime and doesn't need adjustment\n",
    "            elif len(value) == 20:\n",
    "                timeString = value\n",
    "                precisionNumber = 11 # assume precision to days since Wikibase doesn't support greater resolution than that\n",
    "            # date form unknown, don't adjust\n",
    "            else:\n",
    "                print('Warning: date for ' + dateColumnNameRoot + '_val:', rowData[dateColumnNameRoot + '_val'], 'does not conform to any standard format! Check manually.')\n",
    "                error = True\n",
    "            # assign the changed values back to the dict\n",
    "            rowData[dateColumnNameRoot + '_val'] = timeString\n",
    "            rowData[dateColumnNameRoot + '_prec'] = precisionNumber\n",
    "        else:\n",
    "            # a pre-existing precisionNumber must be an integer when written to the API\n",
    "            rowData[dateColumnNameRoot + '_prec'] = int(rowData[dateColumnNameRoot + '_prec'])\n",
    "\n",
    "        # If there is no UUID in the _nodeId column, generate one\n",
    "        if rowData[dateColumnNameRoot + '_nodeId'] == '':\n",
    "            rowData[dateColumnNameRoot + '_nodeId'] = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "    return rowData, error\n",
    "\n",
    "'''\n",
    "# Function to create reference value for times\n",
    "def createTimeReferenceValue(value):\n",
    "    # date is YYYY-MM-DD\n",
    "    if len(value) == 10:\n",
    "        timeString = '+' + value + 'T00:00:00Z'\n",
    "        precisionNumber = 11 # precision to days\n",
    "    # date is YYYY-MM\n",
    "    elif len(value) == 7:\n",
    "        timeString = '+' + value + '-00T00:00:00Z'\n",
    "        precisionNumber = 10 # precision to months\n",
    "    # date is YYYY\n",
    "    elif len(value) == 4:\n",
    "        timeString = '+' + value + '-00-00T00:00:00Z'\n",
    "        precisionNumber = 9 # precision to years\n",
    "    # date form unknown, don't adjust\n",
    "    else:\n",
    "        # 2020-07-15 note: Previously, the leading + was included with the the table value.\n",
    "        # However, in order for the csv2rdf schema to be valid, the + must not be included in the tabled value. So it is added here.\n",
    "        #timeString = value\n",
    "        timeString = '+' + value\n",
    "        precisionNumber = 11 # assume precision to days\n",
    "        \n",
    "    # Q1985727 is the Gregorian calendar\n",
    "    # 2020-07-15 note: Previously, the leading + was included with the the table value.\n",
    "    # However, in order for the csv2rdf schema to be valid, the + must not be included in the tabled value. So it is added here.\n",
    "    dateDict = {\n",
    "            'time': '+' + timeString,\n",
    "            'timezone': 0,\n",
    "            'before': 0,\n",
    "            'after': 0,\n",
    "            'precision': precisionNumber,\n",
    "            'calendarmodel': \"http://www.wikidata.org/entity/Q1985727\"\n",
    "            }\n",
    "    return dateDict\n",
    "'''\n",
    "\n",
    "# Find the column with the UUID for the statement\n",
    "def findPropertyUuid(propertyId, columns):\n",
    "    statementUuidColumn = '' # start value as empty string in case no UUID column\n",
    "    for column in columns:\n",
    "        if not('suppressOutput' in column):\n",
    "            # find the valueUrl in the column for which the value of the statement has the prop version of the property as its propertyUrl\n",
    "            if 'prop/' + propertyId in column['propertyUrl']:\n",
    "                temp = column['valueUrl'].partition('-{')[2]\n",
    "                statementUuidColumn = temp.partition('}')[0] # in the event of two columns with the same property ID, the last one is used\n",
    "                #print(statementUuidColumn)\n",
    "    \n",
    "    # Give a warning if there isn't any UUID column for the property\n",
    "    if statementUuidColumn == '':\n",
    "        print('Warning: No UUID column for property ' + propertyId)\n",
    "    return statementUuidColumn\n",
    "\n",
    "# Each property can have zero to many references. This function searches the column headers to find all of\n",
    "# the columns that are references for a particulary property used in statements\n",
    "def findReferencesForProperty(statementUuidColumn, columns):\n",
    "    # build up a list of dictionaries about references to associate with the property\n",
    "    referenceList = []\n",
    "\n",
    "    # Step through the columns looking for references associated with the property\n",
    "    for column in columns:\n",
    "        if not('suppressOutput' in column):\n",
    "            # check if the aboutUrl for the column has the statement subject UUID column as the about value and that the propertyUrl value is wasDerivedFrom\n",
    "            if ('prov:wasDerivedFrom' in column['propertyUrl']) and (statementUuidColumn in column['aboutUrl']):\n",
    "                temp = column['valueUrl'].partition('{')[2]\n",
    "                refHashColumn = temp.partition('}')[0]\n",
    "                #print(refHashColumn)\n",
    "\n",
    "                # These are the lists that will accumulate data about each property of the reference\n",
    "                refPropList = [] # P ID for the property\n",
    "                refValueColumnList = [] # column header string for the reference property's value\n",
    "                refEntityOrLiteral = [] # values: entity or literal, determined by presence of a valueUrl key for the column\n",
    "                refTypeList = [] # the datatype of the property's value: url, time, or string\n",
    "                refValueTypeList = [] # the specific type of a string: time or string\n",
    "                # The kind of value in the column (dateTime, string) can be retrieved directly from the column 'datatype' value\n",
    "                \n",
    "                # Now step throught the columns looking for each of the properties that are associated with the reference\n",
    "                for propColumn in columns:\n",
    "                    if not('suppressOutput' in propColumn):\n",
    "                        # Find the columns that have the refHash column name in the aboutUrl\n",
    "                        if refHashColumn in propColumn['aboutUrl']:\n",
    "                            \n",
    "                            # Determine whether the value of the reference is a value node (e.g. dates) or a direct value\n",
    "                            valueString = propColumn['propertyUrl'].partition('prop/reference/')[2]\n",
    "                            if \"value\" in valueString: # e.g. value/P813\n",
    "                                # The property IRI namespace for references with value nodes is http://www.wikidata.org/prop/reference/value/\n",
    "                                refPropList.append(valueString.partition('value/')[2])\n",
    "                                # The column title will be something like employer_ref1_retrieved_nodeId, \n",
    "                                # so get the root of the string to the left of \"_nodeId\"\n",
    "                                refValueColumnList.append(propColumn['titles'].partition('_nodeId')[0])\n",
    "\n",
    "                                # Find out what kind of value node it is. Currently supported is date; future: globe coordinate value and quantities\n",
    "                                for testColumn in columns:\n",
    "                                    try:\n",
    "                                        if propColumn['titles'] in testColumn['aboutUrl']:\n",
    "                                            if 'timeValue' in testColumn['propertyUrl']: # value is a date\n",
    "                                                refEntityOrLiteral.append('value')\n",
    "                                                refTypeList.append('time')\n",
    "                                                refValueTypeList.append('time')\n",
    "                                            elif 'geoLatitude' in testColumn['propertyUrl']: # value is a globe coordinate value\n",
    "                                                pass\n",
    "                                            elif 'quantityAmount' in testColumn['propertyUrl']: # value is a quantity\n",
    "                                                pass\n",
    "                                            else:\n",
    "                                                continue\n",
    "                                    except:\n",
    "                                        pass\n",
    "                            else: # e.g. P854\n",
    "                                # The property IRI namespace for references with direct values is http://www.wikidata.org/prop/reference/\n",
    "                                refPropList.append(valueString)\n",
    "                                # Just use the whole column title\n",
    "                                refValueColumnList.append(propColumn['titles'])\n",
    "\n",
    "                                if 'valueUrl' in propColumn:\n",
    "                                    # URIs are detected when there is a valueUrl whose value has a first character of \"{\"\n",
    "                                    if propColumn['valueUrl'][0] == '{':\n",
    "                                        refEntityOrLiteral.append('literal')\n",
    "                                        refTypeList.append('url')\n",
    "                                        refValueTypeList.append('string')\n",
    "                                    else:\n",
    "                                        refEntityOrLiteral.append('entity')\n",
    "                                        refTypeList.append('wikibase-item')\n",
    "                                        refValueTypeList.append('wikibase-entityid')\n",
    "                                else:\n",
    "                                    refTypeList.append('string')\n",
    "                                    refValueTypeList.append('string')\n",
    "                \n",
    "                # After all of the properties have been found and their data have been added to the lists, \n",
    "                # insert the lists into the reference list as values in a dictionary\n",
    "                referenceList.append({'refHashColumn': refHashColumn, 'refPropList': refPropList, 'refValueColumnList': refValueColumnList, 'refEntityOrLiteral': refEntityOrLiteral, 'refTypeList': refTypeList, 'refValueTypeList': refValueTypeList})\n",
    "        \n",
    "    # After every column has been searched for references associated with the property, return the reference list\n",
    "    #print('References: ', json.dumps(referenceList, indent=2))\n",
    "    return referenceList\n",
    "\n",
    "\n",
    "# Each property can have zero to many qualifiers. This function searches the column headers to find all of\n",
    "# the columns that are qualifiers for a particulary property\n",
    "def findQualifiersForProperty(statementUuidColumn, columns):\n",
    "\n",
    "    # These are the lists that will accumulate data about each qualifier\n",
    "    qualPropList = [] # P ID for the property\n",
    "    qualValueColumnList = [] # column header string for the reference property's value\n",
    "    qualEntityOrLiteral = [] # values: entity or literal, determined by presence of a valueUrl key for the column\n",
    "    qualTypeList = [] # the datatype of the qualifier's value: url, time, or string\n",
    "    qualValueTypeList = [] # the specific type of a string: time or string\n",
    "    # The kind of value in the column (dateTime, string) can be retrieved directly from the column 'datatype' value\n",
    "\n",
    "    for column in columns:\n",
    "        if not('suppressOutput' in column):\n",
    "            # find the column that has the statement UUID in the about\n",
    "            # and the property is a qualifier property\n",
    "            if (statementUuidColumn in column['aboutUrl']) and ('qualifier' in column['propertyUrl']):\n",
    "                # Determine whether the value of the qualifier is a value node (e.g. dates) or a direct value\n",
    "                valueString = column['propertyUrl'].partition('prop/qualifier/')[2]\n",
    "                if \"value\" in valueString: # e.g. value/P580\n",
    "                    # The property IRI namespace for qualifiers with value nodes is http://www.wikidata.org/prop/qualifier/value/\n",
    "                    qualPropList.append(valueString.partition('value/')[2])\n",
    "                    # The column title will be something like employer_startDate_nodeId, \n",
    "                    # so get the root of the string to the left of \"_nodeId\"\n",
    "                    qualValueColumnList.append(column['titles'].partition('_nodeId')[0])\n",
    "\n",
    "                    # Find out what kind of value node it is. Currently supported is date; future: globe coordinate value and quantities\n",
    "                    for testColumn in columns:\n",
    "                        try:\n",
    "                            if column['titles'] in testColumn['aboutUrl']:\n",
    "                                if 'timeValue' in testColumn['propertyUrl']: # value is a date\n",
    "                                    qualEntityOrLiteral.append('value')\n",
    "                                    qualTypeList.append('time')\n",
    "                                    qualValueTypeList.append('time')\n",
    "                                elif 'geoLatitude' in testColumn['propertyUrl']: # value is a globe coordinate value\n",
    "                                    pass\n",
    "                                elif 'quantityAmount' in testColumn['propertyUrl']: # value is a quantity\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    continue\n",
    "                        except:\n",
    "                            pass\n",
    "                else: # e.g. P1545\n",
    "                    # The property IRI namespace for qualifiers with direct values is http://www.wikidata.org/prop/qualifier/\n",
    "                    qualPropList.append(valueString)\n",
    "                    # Just use the whole column title\n",
    "                    qualValueColumnList.append(column['titles'])\n",
    "\n",
    "                    # determine whether the qualifier is an entity/URI or string\n",
    "                    if 'valueUrl' in column:\n",
    "                        # URIs are detected when there is a valueUrl whose value has a first character of \"{\"\n",
    "                        if column['valueUrl'][0] == '{':\n",
    "                            qualEntityOrLiteral.append('literal')\n",
    "                            qualTypeList.append('url')\n",
    "                            qualValueTypeList.append('string')\n",
    "                        else:\n",
    "                            qualEntityOrLiteral.append('entity')\n",
    "                            qualTypeList.append('wikibase-item')\n",
    "                            qualValueTypeList.append('wikibase-entityid')\n",
    "                    else:\n",
    "                        qualTypeList.append('string')\n",
    "                        qualValueTypeList.append('string')\n",
    "\n",
    "    # After all of the qualifier columns are found for the property, create a dictionary to pass back\n",
    "    qualifierDictionary = {'qualPropList': qualPropList, 'qualValueColumnList': qualValueColumnList, \"qualEntityOrLiteral\": qualEntityOrLiteral, 'qualTypeList': qualTypeList, 'qualValueTypeList': qualValueTypeList}\n",
    "    #print('Qualifiers: ', json.dumps(qualifierDictionary, indent=2))\n",
    "    return(qualifierDictionary)\n",
    "\n",
    "# The form of snaks is the same for references and qualifiers, so they can be generated systematically\n",
    "# Although the variable names include \"ref\", they apply the same to the analagous \"qual\" variables.\n",
    "def generateSnaks(snakDictionary, require_references, refValue, refPropNumber, refPropList, refValueColumnList, refValueTypeList, refTypeList, refEntityOrLiteral):\n",
    "    if not(refValue):  # evaluates both empty strings for direct values or empty dict for node-valued values\n",
    "        if require_references: # Do not write the record if it's missing a reference.\n",
    "            print('Reference value missing! Cannot write the record.')\n",
    "            sys.exit()\n",
    "    else:\n",
    "        if refEntityOrLiteral[refPropNumber] == 'value':\n",
    "            # Currently time is the only kind of value node handled\n",
    "            if refTypeList[refPropNumber] == 'time':\n",
    "                snakDictionary[refPropList[refPropNumber]] = [\n",
    "                    {\n",
    "                    'snaktype': 'value',\n",
    "                    'property': refPropList[refPropNumber],\n",
    "                    'datavalue':{\n",
    "                        'value': {\n",
    "                            'time': '+' + refValue['timeValue'],\n",
    "                            'timezone': 0,\n",
    "                            'before': 0,\n",
    "                            'after': 0,\n",
    "                            'precision': refValue['timePrecision'],\n",
    "                            'calendarmodel': \"http://www.wikidata.org/entity/Q1985727\"\n",
    "                            },\n",
    "                        'type': 'time'\n",
    "                        },\n",
    "                    'datatype': 'time'\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "            # In the future handle other types here\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        elif refEntityOrLiteral[refPropNumber] == 'entity':\n",
    "            # case where the value is an entity\n",
    "            snakDictionary[refPropList[refPropNumber]] = [\n",
    "                {\n",
    "                'snaktype': 'value',\n",
    "                'property': refPropList[refPropNumber],\n",
    "                'datavalue': {\n",
    "                    'value': {\n",
    "                        'id': refValue\n",
    "                        },\n",
    "                    'type': 'wikibase-entityid'\n",
    "                    },\n",
    "                'datatype': 'wikibase-item'\n",
    "                }\n",
    "            ]\n",
    "        else:\n",
    "            # case where value is a string of some kind\n",
    "            snakDictionary[refPropList[refPropNumber]] = [\n",
    "                {\n",
    "                'snaktype': 'value',\n",
    "                'property': refPropList[refPropNumber],\n",
    "                'datavalue': {\n",
    "                    'value': refValue,\n",
    "                    'type': refValueTypeList[refPropNumber]\n",
    "                },\n",
    "                'datatype': refTypeList[refPropNumber]\n",
    "                }\n",
    "            ]\n",
    "    return snakDictionary\n",
    "\n",
    "# If there are references for a statement, return a reference list\n",
    "def createReferences(referenceListForProperty, rowData):\n",
    "    referenceListToReturn = []\n",
    "    for referenceDict in referenceListForProperty:\n",
    "        refPropList = referenceDict['refPropList']\n",
    "        refValueColumnList = referenceDict['refValueColumnList']\n",
    "        refValueTypeList = referenceDict['refValueTypeList']\n",
    "        refTypeList = referenceDict['refTypeList']\n",
    "        refEntityOrLiteral = referenceDict['refEntityOrLiteral']\n",
    "\n",
    "        snakDictionary = {}\n",
    "        for refPropNumber in range(0, len(refPropList)):\n",
    "            if refEntityOrLiteral[refPropNumber] == 'value':\n",
    "                # value nodes with no nodeId should be considered to have no value\n",
    "                if rowData[refValueColumnList[refPropNumber] + '_nodeId'] == '':\n",
    "                    refValue = {}\n",
    "                else:\n",
    "                    # currently time is the only supported node-valued type\n",
    "                    if refTypeList[refPropNumber] == 'time':\n",
    "                        refValue = {'timeValue': rowData[refValueColumnList[refPropNumber] + '_val'], 'timePrecision': rowData[refValueColumnList[refPropNumber] + '_prec']}\n",
    "                    # other node-valued types will be handled here\n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                refValue = rowData[refValueColumnList[refPropNumber]]\n",
    "            snakDictionary = generateSnaks(snakDictionary, require_references, refValue, refPropNumber, refPropList, refValueColumnList, refValueTypeList, refTypeList, refEntityOrLiteral)\n",
    "        if snakDictionary != {}: # If any references were added, create the outer dict and add to list\n",
    "            outerSnakDictionary = {\n",
    "                'snaks': snakDictionary\n",
    "            }\n",
    "            referenceListToReturn.append(outerSnakDictionary)\n",
    "    return referenceListToReturn\n",
    "\n",
    "\n",
    "# NOTE: this differs from the createReferences function in that it returns\n",
    "# a dictionary of snaks for a single reference, NOT a list for many references\n",
    "def createReferenceSnak(referenceDict, rowData):\n",
    "    refPropList = referenceDict['refPropList']\n",
    "    refValueColumnList = referenceDict['refValueColumnList']\n",
    "    refValueTypeList = referenceDict['refValueTypeList']\n",
    "    refTypeList = referenceDict['refTypeList']\n",
    "    refEntityOrLiteral = referenceDict['refEntityOrLiteral']\n",
    "    \n",
    "    snakDictionary = {}\n",
    "    for refPropNumber in range(0, len(refPropList)):\n",
    "        if refEntityOrLiteral[refPropNumber] == 'value':\n",
    "            # value nodes with no nodeId should be considered to have no value\n",
    "            if rowData[refValueColumnList[refPropNumber] + '_nodeId'] == '':\n",
    "                refValue = {}\n",
    "            else:\n",
    "                # currently time is the only supported node-valued type\n",
    "                if refTypeList[refPropNumber] == 'time':\n",
    "                    refValue = {'timeValue': rowData[refValueColumnList[refPropNumber] + '_val'], 'timePrecision': rowData[refValueColumnList[refPropNumber] + '_prec']}\n",
    "                # other node-valued types will be handled here\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            refValue = rowData[refValueColumnList[refPropNumber]]\n",
    "        snakDictionary = generateSnaks(snakDictionary, require_references, refValue, refPropNumber, refPropList, refValueColumnList, refValueTypeList, refTypeList, refEntityOrLiteral)\n",
    "    #print(json.dumps(snakDictionary, indent = 2))\n",
    "    return snakDictionary\n",
    "\n",
    "\n",
    "# If there are qualifiers for a statement, return a qualifiers dictionary\n",
    "def createQualifiers(qualifierDictionaryForProperty, rowData):\n",
    "    qualPropList = qualifierDictionaryForProperty['qualPropList']\n",
    "    qualValueColumnList = qualifierDictionaryForProperty['qualValueColumnList']\n",
    "    qualTypeList = qualifierDictionaryForProperty['qualTypeList']\n",
    "    qualValueTypeList = qualifierDictionaryForProperty['qualValueTypeList']\n",
    "    qualEntityOrLiteral = qualifierDictionaryForProperty['qualEntityOrLiteral']\n",
    "    snakDictionary = {}\n",
    "    for qualPropNumber in range(0, len(qualPropList)):\n",
    "        if qualEntityOrLiteral[qualPropNumber] == 'value':\n",
    "            # value nodes with no nodeId should be considered to have no value\n",
    "            if rowData[qualValueColumnList[qualPropNumber] + '_nodeId'] == '':\n",
    "                qualValue = {}\n",
    "            else:\n",
    "                # currently time is the only supported node-valued type\n",
    "                if qualTypeList[qualPropNumber] == 'time':\n",
    "                    qualValue = {'timeValue': rowData[qualValueColumnList[qualPropNumber] + '_val'], 'timePrecision': rowData[qualValueColumnList[qualPropNumber] + '_prec']}\n",
    "                # other node-valued types will be handled here\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            qualValue = rowData[qualValueColumnList[qualPropNumber]]\n",
    "        snakDictionary = generateSnaks(snakDictionary, require_qualifiers, qualValue, qualPropNumber, qualPropList, qualValueColumnList, qualValueTypeList, qualTypeList, qualEntityOrLiteral)\n",
    "    return snakDictionary\n",
    "\n",
    "\n",
    "# This function attempts to post and handles maxlag errors\n",
    "def attemptPost(apiUrl, parameters):\n",
    "    maxRetries = 10\n",
    "    baseDelay = 5 # Wikidata recommends a delay of at least 5 seconds\n",
    "    delayLimit = 300\n",
    "    retry = 0\n",
    "    # maximum number of times to retry lagged server = maxRetries\n",
    "    while retry <= maxRetries:\n",
    "        if retry > 0:\n",
    "            print('retry:', retry)\n",
    "        r = session.post(apiUrl, data = parameters)\n",
    "        data = r.json()\n",
    "        try:\n",
    "            # check if response is a maxlag error\n",
    "            # see https://www.mediawiki.org/wiki/Manual:Maxlag_parameter\n",
    "            if data['error']['code'] == 'maxlag':\n",
    "                print('Lag of ', data['error']['lag'], ' seconds.')\n",
    "                # recommended delay is basically useless\n",
    "                # recommendedDelay = int(r.headers['Retry-After'])\n",
    "                #if recommendedDelay < 5:\n",
    "                    # recommendation is to wait at least 5 seconds if server is lagged\n",
    "                #    recommendedDelay = 5\n",
    "                recommendedDelay = baseDelay*2**retry # double the delay with each retry \n",
    "                if recommendedDelay > delayLimit:\n",
    "                    recommendedDelay = delayLimit\n",
    "                if retry != maxRetries:\n",
    "                    print('Waiting ', recommendedDelay , ' seconds.')\n",
    "                    print()\n",
    "                    sleep(recommendedDelay)\n",
    "                retry += 1\n",
    "\n",
    "                # after this, go out of if and try code blocks\n",
    "            else:\n",
    "                # an error code is returned, but it's not maxlag\n",
    "                return data\n",
    "        except:\n",
    "            # if the response doesn't have an error key, it was successful, so return\n",
    "            return data\n",
    "        # here's where execution goes after the delay\n",
    "    # here's where execution goes after maxRetries tries\n",
    "    print('Failed after ' + str(maxRetries) + ' retries.')\n",
    "    exit() # just abort the script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# authentication\n",
    "\n",
    "# This is the format of the wikibase_credentials.txt file. Username and password\n",
    "# are for a bot that you've created.  Save file in your home directory.\n",
    "# Set your own User-Agent header. Do not use the one listed here\n",
    "# See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "'''\n",
    "endpointUrl=https://test.wikidata.org\n",
    "username=User@bot\n",
    "password=465jli90dslhgoiuhsaoi9s0sj5ki3lo\n",
    "userAgentHeader=YourBot/0.1 (someuser@university.edu)\n",
    "'''\n",
    "\n",
    "# default API resource URL when a Wikibase/Wikidata instance is installed.\n",
    "resourceUrl = '/w/api.php'\n",
    "\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for Win and Mac\n",
    "credentialsFilename = 'wikibase_credentials.txt'\n",
    "credentialsPath = home + '/' + credentialsFilename\n",
    "credentials = retrieveCredentials(credentialsPath)\n",
    "endpointUrl = credentials[0] + resourceUrl\n",
    "user = credentials[1]\n",
    "pwd = credentials[2]\n",
    "userAgentHeader = credentials[3]\n",
    "\n",
    "# Instantiate session outside of any function so that it's globally accessible.\n",
    "session = requests.Session()\n",
    "# Set default User-Agent header so you don't have to send it with every request\n",
    "session.headers.update({'User-Agent': userAgentHeader})\n",
    "\n",
    "\n",
    "loginToken = getLoginToken(endpointUrl)\n",
    "data = logIn(endpointUrl, loginToken, user, pwd)\n",
    "csrfToken = getCsrfToken(endpointUrl)\n",
    "\n",
    "# -------------------------------------------\n",
    "# Beginning of script to process the tables\n",
    "\n",
    "# There are options to require values for every mapped reference column or every mapped qualifier column.\n",
    "# By default, these are turned off, but they can be turned on by changing these flags:\n",
    "require_references = False\n",
    "require_qualifiers = False\n",
    "\n",
    "# Set the value of the maxlag parameter to back off when the server is lagged\n",
    "# see https://www.mediawiki.org/wiki/Manual:Maxlag_parameter\n",
    "# The recommended value is 5 seconds.\n",
    "# To not use maxlang, set the value to 0\n",
    "# To test the maxlag handler code, set maxlag to a very low number like .1\n",
    "maxlag = 5\n",
    "\n",
    "# This is the schema that maps the CSV column to Wikidata properties\n",
    "with open('csv-metadata.json', 'rt', encoding='utf-8') as fileObject:\n",
    "    text = fileObject.read()\n",
    "metadata = json.loads(text)\n",
    "\n",
    "tables = metadata['tables']\n",
    "\n",
    "table = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name:  bluffton.csv\n",
      "Subject column:  wikidataId\n",
      "Label column:  labelEn , language:  en\n",
      "Alternate label column:  alias , language:  en\n",
      "Description column:  description , language:  en\n",
      "Property column:  orcid , Property ID:  P496  Value type: string\n",
      "\n",
      "Property column:  employer , Property ID:  P108  Value type: item\n",
      "\n",
      "Property column:  affiliation , Property ID:  P1416  Value type: item\n",
      "\n",
      "Property column:  instanceOf , Property ID:  P31  Value type: item\n",
      "\n",
      "Property column:  sexOrGenderQId , Property ID:  P21  Value type: item\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tableFileName = table['url']\n",
    "print('File name: ', tableFileName)\n",
    "tableData = readDict(tableFileName)\n",
    "\n",
    "# we are opening the file as a csv.reader object as the easy way to get the header row as a list\n",
    "fileObject = open(tableFileName, 'r', newline='', encoding='utf-8')\n",
    "readerObject = csv.reader(fileObject)\n",
    "for row in readerObject:\n",
    "    fieldnames = row\n",
    "    break # we only nead the header row, so break after the first loop\n",
    "fileObject.close()\n",
    "\n",
    "columns = table['tableSchema']['columns']\n",
    "\n",
    "subjectWikidataIdName = ''\n",
    "# assume each row is primarily about an entity\n",
    "# step through the columns until there is an aboutUrl for an entity\n",
    "for column in columns:\n",
    "    # check only columns that have an aboutUrl key\n",
    "    if 'aboutUrl' in column:\n",
    "        # the value ouf the aboutUrl must be an entity\n",
    "        if 'entity/{' in column['aboutUrl']:\n",
    "            # extract the column name of the subject resource from the URI template\n",
    "            temp = column['aboutUrl'].partition('{')[2]\n",
    "            subjectWikidataIdName = temp.partition('}')[0]\n",
    "            # don't worry about repeatedly replacing subjectWikidataIdName as long as the row is only about one entity            \n",
    "#print(subjectWikidataIdName)\n",
    "\n",
    "# make lists of the columns for each kind of property\n",
    "labelColumnList = []\n",
    "labelLanguageList = []\n",
    "aliasColumnList = []\n",
    "aliasLanguageList = []\n",
    "descriptionColumnList = []\n",
    "descriptionLanguageList = []\n",
    "propertiesColumnList = []\n",
    "propertiesUuidColumnList = []\n",
    "propertiesEntityOrLiteral = [] # determines whether value of property is an \"entity\" (i.e. item) or \"literal\" (which includes strings, dates, and URLs that aren't actually literals)\n",
    "propertiesIdList = []\n",
    "propertiesTypeList = [] # the 'datatype' given to a mainsnak. Currently supported types are: \"wikibase-item\", \"url\", \"time\", or \"string\"\n",
    "propertiesValueTypeList = [] # the 'type' given to values of 'datavalue' in the mainsnak. Can be \"wikibase-entityid\", \"string\" or \"time\" \n",
    "propertiesReferencesList = []\n",
    "propertiesQualifiersList = []\n",
    "\n",
    "# step through all of the columns and sort their headers into the appropriate list\n",
    "\n",
    "# find the column whose name matches the URI template for the aboutUrl (only one)\n",
    "for column in columns:\n",
    "    if column['name'] == subjectWikidataIdName:\n",
    "        subjectWikidataIdColumnHeader = column['titles']\n",
    "        print('Subject column: ', subjectWikidataIdColumnHeader)\n",
    "\n",
    "# create a list of the entities that have Wikidata qIDs\n",
    "qIds = []\n",
    "for entity in tableData:\n",
    "    if entity[subjectWikidataIdColumnHeader] != '':\n",
    "        qIds.append(entity[subjectWikidataIdColumnHeader])\n",
    "\n",
    "existingLabels = [] # a list to hold lists of labels in various languages\n",
    "existingDescriptions = [] # a list to hold lists of descriptions in various languages\n",
    "existingAliases = [] # a list to hold lists of lists of aliases in various languages\n",
    "for column in columns:\n",
    "\n",
    "    # special handling for alias column\n",
    "    # In order to allow for multiple aliases to be listed as a JSON string, the alias column is handled idiosyncratically and\n",
    "    # not as with the labels and description columns. It must me named exactly \"alias\" and have output suppressed.\n",
    "    # This hack allows aliases to be processed by the script, but also to allow a csv2rdf to serialize the CSV data as valid RDF.\n",
    "    # However, it limits aliases to a single language.\n",
    "    if 'suppressOutput' in column:\n",
    "        # find columns that contain aliases and ignor any others with suppressOutput\n",
    "        # GUI calls it \"Also known as\"; RDF as skos:altLabel\n",
    "        if column['name'] == 'alias':\n",
    "            altLabelColumnHeader = column['titles']\n",
    "            altLabelLanguage = column['lang']\n",
    "            print('Alternate label column: ', altLabelColumnHeader, ', language: ', altLabelLanguage)\n",
    "            aliasColumnList.append(altLabelColumnHeader)\n",
    "            aliasLanguageList.append(altLabelLanguage)\n",
    "\n",
    "            # retrieve the aliases in that language that already exist in Wikidata and match them with table rows\n",
    "            languageAliases = []\n",
    "            aliasesAtWikidata = searchLabelsDescriptionsAtWikidata(qIds, 'alias', altLabelLanguage)\n",
    "            for entityIndex in range(0, len(tableData)):\n",
    "                personAliasList = []\n",
    "                if tableData[entityIndex][subjectWikidataIdColumnHeader] != '':  # don't look for the label at Wikidata if the item doesn't yet exist\n",
    "                    for wikiLabel in aliasesAtWikidata:\n",
    "                        if tableData[entityIndex][subjectWikidataIdColumnHeader] == wikiLabel['qId']:\n",
    "                            personAliasList.append(wikiLabel['string'])\n",
    "                # if not found, the personAliasList list will remain empty\n",
    "                languageAliases.append(personAliasList)\n",
    "\n",
    "            # add all of the found aliases for that language to the list of aliases in various languages\n",
    "            existingAliases.append(languageAliases)\n",
    "    # handle all other non-suppressed columns.\n",
    "    else:\n",
    "\n",
    "        # find the columns (if any) that provide labels\n",
    "        if column['propertyUrl'] == 'rdfs:label':\n",
    "            labelColumnHeader = column['titles']\n",
    "            labelLanguage = column['lang']\n",
    "            print('Label column: ', labelColumnHeader, ', language: ', labelLanguage)\n",
    "            labelColumnList.append(labelColumnHeader)\n",
    "            labelLanguageList.append(labelLanguage)\n",
    "\n",
    "            # retrieve the labels in that language that already exist in Wikidata and match them with table rows\n",
    "            tempLabels = []\n",
    "            labelsAtWikidata = searchLabelsDescriptionsAtWikidata(qIds, 'label', labelLanguage)\n",
    "            for entityIndex in range(0, len(tableData)):\n",
    "                found = False\n",
    "                if tableData[entityIndex][subjectWikidataIdColumnHeader] != '':  # don't look for the label at Wikidata if the item doesn't yet exist\n",
    "                    for wikiLabel in labelsAtWikidata:\n",
    "                        if tableData[entityIndex][subjectWikidataIdColumnHeader] == wikiLabel['qId']:\n",
    "                            found = True\n",
    "                            tempLabels.append(wikiLabel['string'])\n",
    "                            break # stop looking if there is a match\n",
    "                if not found:\n",
    "                    tempLabels.append('')\n",
    "\n",
    "            # add all of the found labels for that language to the list of labels in various languages\n",
    "            existingLabels.append(tempLabels)\n",
    "\n",
    "        # find columns that contain descriptions\n",
    "        # Note: if descriptions exist for a language, they will be overwritten\n",
    "        elif column['propertyUrl'] == 'schema:description':\n",
    "            descriptionColumnHeader = column['titles']\n",
    "            descriptionLanguage = column['lang']\n",
    "            print('Description column: ', descriptionColumnHeader, ', language: ', descriptionLanguage)\n",
    "            descriptionColumnList.append(descriptionColumnHeader)\n",
    "            descriptionLanguageList.append(descriptionLanguage)\n",
    "\n",
    "            # retrieve the descriptions in that language that already exist in Wikidata and match them with table rows\n",
    "            tempLabels = []\n",
    "            descriptionsAtWikidata = searchLabelsDescriptionsAtWikidata(qIds, 'description', descriptionLanguage)\n",
    "            for entityIndex in range(0, len(tableData)):\n",
    "                found = False\n",
    "                if tableData[entityIndex][subjectWikidataIdColumnHeader] != '':  # don't look for the label at Wikidata if the item doesn't yet exist\n",
    "                    for wikiDescription in descriptionsAtWikidata:\n",
    "                        if tableData[entityIndex][subjectWikidataIdColumnHeader] == wikiDescription['qId']:\n",
    "                            found = True\n",
    "                            tempLabels.append(wikiDescription['string'])\n",
    "                            break # stop looking if there is a match\n",
    "                if not found:\n",
    "                    tempLabels.append('')\n",
    "\n",
    "            # add all of the found labels for that language to the list of labels in various languages\n",
    "            existingDescriptions.append(tempLabels)\n",
    "\n",
    "        # find columns that contain properties with entity values, literal values that are URLs, or value node values\n",
    "        elif 'valueUrl' in column:\n",
    "            # only add columns that have \"statement\" properties\n",
    "            if 'prop/statement/' in column['propertyUrl']:\n",
    "                if 'prop/statement/value/' in column['propertyUrl']: # value is a value node (e.g. date or geo coordinates)\n",
    "                    found = True\n",
    "                    propColumnHeader = column['titles'].partition('_nodeId')[0] # save only the root of the column name for value nodes\n",
    "                    propertyId = column['propertyUrl'].partition('prop/statement/value/')[2]\n",
    "                    propertiesColumnList.append(propColumnHeader)\n",
    "                    propertiesIdList.append(propertyId)\n",
    "                    propertiesEntityOrLiteral.append('value')\n",
    "                    # Find out what kind of value node it is. Currently supported is date; future: globe coordinate value and quantities\n",
    "                    for testColumn in columns:\n",
    "                        try:\n",
    "                            if column['titles'] in testColumn['aboutUrl']:\n",
    "                                if 'timeValue' in testColumn['propertyUrl']: # value is a date\n",
    "                                    propKind = 'time'\n",
    "                                    propertiesTypeList.append('time')\n",
    "                                    propertiesValueTypeList.append('time')\n",
    "                                elif 'geoLatitude' in testColumn['propertyUrl']: # value is a globe coordinate value\n",
    "                                    propKind = 'geocoordinates'\n",
    "                                elif 'quantityAmount' in testColumn['propertyUrl']: # value is a quantity\n",
    "                                    propKind = 'quantity'\n",
    "                                else:\n",
    "                                    continue\n",
    "                                print('Property column: ', propColumnHeader, ', Property ID: ', propertyId, ' Value type: ', propKind)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                else:\n",
    "                    propColumnHeader = column['titles']\n",
    "                    propertyId = column['propertyUrl'].partition('prop/statement/')[2]\n",
    "                    propertiesColumnList.append(propColumnHeader)\n",
    "                    propertiesIdList.append(propertyId)\n",
    "\n",
    "                    # URLs are detected when there is a valueUrl whose value has a first character of \"{\"\n",
    "                    if column['valueUrl'][0] == '{':\n",
    "                        propertiesEntityOrLiteral.append('literal')\n",
    "                        propertiesTypeList.append('url')\n",
    "                        propertiesValueTypeList.append('string')\n",
    "                        print('Property column: ', propColumnHeader, ', Property ID: ', propertyId, ' Value type: url')\n",
    "                    # Otherwise having a valueUrl indicates that it's an item\n",
    "                    else:\n",
    "                        propertiesEntityOrLiteral.append('entity')\n",
    "                        propertiesTypeList.append('wikibase-item')\n",
    "                        propertiesValueTypeList.append('wikibase-entityid')\n",
    "                        print('Property column: ', propColumnHeader, ', Property ID: ', propertyId, ' Value type: item')\n",
    "\n",
    "                propertyUuidColumn = findPropertyUuid(propertyId, columns)\n",
    "                propertiesUuidColumnList.append(propertyUuidColumn)\n",
    "                propertiesReferencesList.append(findReferencesForProperty(propertyUuidColumn, columns))\n",
    "                propertiesQualifiersList.append(findQualifiersForProperty(propertyUuidColumn, columns))\n",
    "                print()\n",
    "\n",
    "        # remaining columns should have properties with literal values\n",
    "        else:\n",
    "            # only add columns that have \"statement\" properties\n",
    "            if 'prop/statement/' in column['propertyUrl']:\n",
    "                propColumnHeader = column['titles']\n",
    "                propertyId = column['propertyUrl'].partition('prop/statement/')[2]\n",
    "                print('Property column: ', propColumnHeader, ', Property ID: ', propertyId, ' Value type: string')\n",
    "                propertiesColumnList.append(propColumnHeader)\n",
    "                propertiesIdList.append(propertyId)\n",
    "\n",
    "                propertiesEntityOrLiteral.append('literal')\n",
    "                propertiesTypeList.append('string')\n",
    "                propertiesValueTypeList.append('string')\n",
    "\n",
    "                propertyUuidColumn = findPropertyUuid(propertyId, columns)\n",
    "                propertiesUuidColumnList.append(propertyUuidColumn)\n",
    "                propertiesReferencesList.append(findReferencesForProperty(propertyUuidColumn, columns))\n",
    "                propertiesQualifiersList.append(findQualifiersForProperty(propertyUuidColumn, columns))\n",
    "                print()\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting dates\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If there are dates in the table that are not in the format Wikibase requires, they will be converted here\n",
    "print('converting dates')\n",
    "\n",
    "# Figure out the column name roots for column sets that are dates\n",
    "dateColumnNameList = []\n",
    "if len(propertiesColumnList) > 0:\n",
    "    for propertyNumber in range(0, len(propertiesColumnList)):\n",
    "        if propertiesTypeList[propertyNumber] == 'time':\n",
    "            #print('property with date:', propertiesColumnList[propertyNumber])\n",
    "            dateColumnNameList.append(propertiesColumnList[propertyNumber])\n",
    "\n",
    "        if len(propertiesReferencesList[propertyNumber]) != 0:\n",
    "            for qualPropNumber in range(0, len(propertiesQualifiersList[propertyNumber]['qualPropList'])):\n",
    "                if propertiesQualifiersList[propertyNumber]['qualTypeList'][qualPropNumber] == 'time':\n",
    "                    #print('qualifier property with date:', propertiesQualifiersList[propertyNumber]['qualValueColumnList'][qualPropNumber])\n",
    "                    dateColumnNameList.append(propertiesQualifiersList[propertyNumber]['qualValueColumnList'][qualPropNumber])\n",
    "\n",
    "        if len(propertiesReferencesList[propertyNumber]) != 0:\n",
    "            for referenceNumber in range(0, len(propertiesReferencesList[propertyNumber])):\n",
    "                for refPropNumber in range(0, len(propertiesReferencesList[propertyNumber][referenceNumber]['refPropList'])):\n",
    "                    if propertiesReferencesList[propertyNumber][referenceNumber]['refTypeList'][refPropNumber] == 'time':\n",
    "                        #print('reference property with date:', propertiesReferencesList[propertyNumber][referenceNumber]['refValueColumnList'][refPropNumber])\n",
    "                        dateColumnNameList.append(propertiesReferencesList[propertyNumber][referenceNumber]['refValueColumnList'][refPropNumber])\n",
    "#print(dateColumnNameList)\n",
    "\n",
    "errorFlag = False\n",
    "for rowNumber in range(0, len(tableData)):\n",
    "    #print('row: ' + str(rowNumber))\n",
    "    #print(tableData[rowNumber])\n",
    "    for dateColumnName in dateColumnNameList:\n",
    "        tableData[rowNumber], error = convertDates(tableData[rowNumber], dateColumnName)\n",
    "        if error:\n",
    "            errorFlag = True\n",
    "    #print()        \n",
    "    #print(tableData[rowNumber])\n",
    "    #print()\n",
    "\n",
    "# Write the file with the converted dates in case the script crashes\n",
    "writeToFile(tableFileName, fieldnames, tableData)\n",
    "\n",
    "# If any of the date formats in the table were bad, don't try to write to the API\n",
    "if errorFlag:\n",
    "    sys.exit('Fix incorrectly formatted dates in file and restart')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing items\n",
      "--------------------------\n",
      "\n",
      "processing row: 0  Label: Stanley R. Clemens  new record\n",
      "\n",
      "{\n",
      "  \"labels\": {\n",
      "    \"en\": {\n",
      "      \"language\": \"en\",\n",
      "      \"value\": \"Stanley R. Clemens\"\n",
      "    }\n",
      "  },\n",
      "  \"aliases\": {\n",
      "    \"en\": [\n",
      "      {\n",
      "        \"language\": \"en\",\n",
      "        \"value\": \"Stan Clemens\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"descriptions\": {\n",
      "    \"en\": {\n",
      "      \"language\": \"en\",\n",
      "      \"value\": \"mathematician and educator\"\n",
      "    }\n",
      "  },\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"mainsnak\": {\n",
      "        \"snaktype\": \"value\",\n",
      "        \"property\": \"P108\",\n",
      "        \"datatype\": \"wikibase-item\",\n",
      "        \"datavalue\": {\n",
      "          \"value\": {\n",
      "            \"id\": \"Q886141\"\n",
      "          },\n",
      "          \"type\": \"wikibase-entityid\"\n",
      "        }\n",
      "      },\n",
      "      \"type\": \"statement\",\n",
      "      \"rank\": \"normal\",\n",
      "      \"references\": [\n",
      "        {\n",
      "          \"snaks\": {\n",
      "            \"P854\": [\n",
      "              {\n",
      "                \"snaktype\": \"value\",\n",
      "                \"property\": \"P854\",\n",
      "                \"datavalue\": {\n",
      "                  \"value\": \"https://www.bluffton.edu/catalog/officers/faculty.aspx\",\n",
      "                  \"type\": \"string\"\n",
      "                },\n",
      "                \"datatype\": \"url\"\n",
      "              }\n",
      "            ],\n",
      "            \"P813\": [\n",
      "              {\n",
      "                \"snaktype\": \"value\",\n",
      "                \"property\": \"P813\",\n",
      "                \"datavalue\": {\n",
      "                  \"value\": {\n",
      "                    \"time\": \"+2020-11-06T00:00:00Z\",\n",
      "                    \"timezone\": 0,\n",
      "                    \"before\": 0,\n",
      "                    \"after\": 0,\n",
      "                    \"precision\": 11,\n",
      "                    \"calendarmodel\": \"http://www.wikidata.org/entity/Q1985727\"\n",
      "                  },\n",
      "                  \"type\": \"time\"\n",
      "                },\n",
      "                \"datatype\": \"time\"\n",
      "              }\n",
      "            ]\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"mainsnak\": {\n",
      "        \"snaktype\": \"value\",\n",
      "        \"property\": \"P31\",\n",
      "        \"datatype\": \"wikibase-item\",\n",
      "        \"datavalue\": {\n",
      "          \"value\": {\n",
      "            \"id\": \"Q5\"\n",
      "          },\n",
      "          \"type\": \"wikibase-entityid\"\n",
      "        }\n",
      "      },\n",
      "      \"type\": \"statement\",\n",
      "      \"rank\": \"normal\"\n",
      "    },\n",
      "    {\n",
      "      \"mainsnak\": {\n",
      "        \"snaktype\": \"value\",\n",
      "        \"property\": \"P21\",\n",
      "        \"datatype\": \"wikibase-item\",\n",
      "        \"datavalue\": {\n",
      "          \"value\": {\n",
      "            \"id\": \"Q6581097\"\n",
      "          },\n",
      "          \"type\": \"wikibase-entityid\"\n",
      "        }\n",
      "      },\n",
      "      \"type\": \"statement\",\n",
      "      \"rank\": \"normal\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# process each row of the table for item writing\n",
    "print('Writing items')\n",
    "print('--------------------------')\n",
    "print()\n",
    "\n",
    "rowNumber = 0\n",
    "\n",
    "status_message = 'processing row: ' + str(rowNumber)\n",
    "if len(labelColumnList) > 0: # skip printing a label if there aren't any\n",
    "    status_message += '  Label: ' + tableData[rowNumber][labelColumnList[0]] # include the first label available\n",
    "if tableData[rowNumber][subjectWikidataIdColumnHeader] != '': # only list existing record IDs\n",
    "    status_message += '  qID: ' + tableData[rowNumber][subjectWikidataIdColumnHeader]\n",
    "else:\n",
    "    status_message += '  new record'\n",
    "print(status_message)\n",
    "\n",
    "# build the parameter string to be posted to the API\n",
    "parameterDictionary = {\n",
    "    'action': 'wbeditentity',\n",
    "    'format':'json',\n",
    "    'token': csrfToken\n",
    "    }\n",
    "\n",
    "if tableData[rowNumber][subjectWikidataIdColumnHeader] == '':\n",
    "    newItem = True\n",
    "    parameterDictionary['new'] = 'item'\n",
    "else:\n",
    "    newItem = False\n",
    "    parameterDictionary['id'] = tableData[rowNumber][subjectWikidataIdColumnHeader]\n",
    "\n",
    "# begin constructing the string for the \"data\" value by creating a data structure to be turned into JSON\n",
    "# the examples are from https://www.wikidata.org/w/api.php?action=help&modules=wbeditentity\n",
    "dataStructure = {}\n",
    "\n",
    "if len(labelColumnList) > 0:\n",
    "    # here's what we need to construct for labels:\n",
    "    # data={\"labels\":{\"de\":{\"language\":\"de\",\"value\":\"de-value\"},\"en\":{\"language\":\"en\",\"value\":\"en-value\"}}}\n",
    "    labelDict = {}\n",
    "    for languageNumber in range(0, len(labelColumnList)):\n",
    "        valueString = tableData[rowNumber][labelColumnList[languageNumber]]\n",
    "        # if there is a new record with no Q ID...\n",
    "        if newItem:\n",
    "            # add the label in the table for that language to the label dictionary\n",
    "            labelDict[labelLanguageList[languageNumber]] = {\n",
    "                'language': labelLanguageList[languageNumber],\n",
    "                'value': valueString\n",
    "                }\n",
    "        else:\n",
    "            # not a new record, check if the value in the table is different from what's currently in Wikidata\n",
    "            if valueString != existingLabels[languageNumber][rowNumber]:\n",
    "                # if they are different check to make sure the table value isn't empty\n",
    "                if valueString != '':\n",
    "                    print('Changing label ', existingLabels[languageNumber][rowNumber], ' to ', valueString)\n",
    "                    # add the label in the table for that language to the label dictionary\n",
    "                    labelDict[labelLanguageList[languageNumber]] = {\n",
    "                        'language': labelLanguageList[languageNumber],\n",
    "                        'value': valueString\n",
    "                        }\n",
    "    if labelDict != {}:\n",
    "        dataStructure['labels'] = labelDict\n",
    "\n",
    "# the alias column contains a list. If the table has more aliases than currently in Wikidata, then update\n",
    "if len(aliasColumnList) > 0:\n",
    "    # no example, but follow the same pattern as labels\n",
    "    aliasDict = {}\n",
    "    # step through each language that has aliases\n",
    "    for aliasColumnNumber in range(0, len(aliasColumnList)):\n",
    "        valueList = json.loads(tableData[rowNumber][aliasColumnList[aliasColumnNumber]])\n",
    "        # don't do anything if there are no alias values for that person\n",
    "        if valueList != []:\n",
    "            # perform an unordered comparison between the aliases currently in Wikidata and\n",
    "            # the aliases in the CSV for that person. Don't do anything if they are the same.\n",
    "            # NOTE: this is actually redundant with the > test that follows, but I'm leaving it here to remember\n",
    "            # how to do an unordered comparison.  The > test might be replaced with something more sophisticated later\n",
    "            if set(valueList) != set(existingAliases[languageNumber][rowNumber]):\n",
    "                # only make a change if there are more aliases in the spreadsheet than currently in Wikidata\n",
    "                if len(valueList) > len(existingAliases[languageNumber][rowNumber]):\n",
    "                    print('')\n",
    "                    # see https://www.mediawiki.org/wiki/Wikibase/DataModel/JSON#Labels,_Descriptions_and_Aliases\n",
    "                    # for structure of aliases in JSON\n",
    "                    aliasLangList = []\n",
    "                    for aliasValue in valueList:\n",
    "                        temp = {\n",
    "                        'language': aliasLanguageList[aliasColumnNumber],\n",
    "                        'value': aliasValue\n",
    "                        }\n",
    "                        aliasLangList.append(temp)\n",
    "                    aliasDict[aliasLanguageList[aliasColumnNumber]] = aliasLangList\n",
    "    if aliasDict != {}:\n",
    "        dataStructure['aliases'] = aliasDict\n",
    "\n",
    "if len(descriptionColumnList) > 0:\n",
    "    # here's what we need to construct for descriptions:\n",
    "    # data={\"descriptions\":{\"nb\":{\"language\":\"nb\",\"value\":\"nb-Description-Here\"}}}\n",
    "    descriptionDict = {}\n",
    "    for languageNumber in range(0, len(descriptionColumnList)):\n",
    "        valueString = tableData[rowNumber][descriptionColumnList[languageNumber]]\n",
    "        # if there is a new record with no Q ID...\n",
    "        if newItem:\n",
    "            # add the description in the table for that language to the description dictionary\n",
    "            descriptionDict[descriptionLanguageList[languageNumber]] = {\n",
    "                'language': descriptionLanguageList[languageNumber],\n",
    "                'value': valueString\n",
    "                }\n",
    "        else:\n",
    "            # not a new record, check if the value in the table is different from what's currently in Wikidata\n",
    "            if valueString != existingDescriptions[languageNumber][rowNumber]:\n",
    "                # if they are different check to make sure the table value isn't empty\n",
    "                if valueString != '':\n",
    "                    print('Changing description ', existingDescriptions[languageNumber][rowNumber], ' to ', valueString)\n",
    "                    # add the description in the table for that language to the description dictionary\n",
    "                    descriptionDict[descriptionLanguageList[languageNumber]] = {\n",
    "                        'language': descriptionLanguageList[languageNumber],\n",
    "                        'value': valueString\n",
    "                        }\n",
    "    if descriptionDict != {}:\n",
    "        dataStructure['descriptions'] = descriptionDict\n",
    "\n",
    "# handle claims\n",
    "if len(propertiesColumnList) > 0:\n",
    "    claimsList = []\n",
    "\n",
    "    # here's what we need to construct for literal valued properties:\n",
    "    # data={\"claims\":[{\"mainsnak\":{\"snaktype\":\"value\",\"property\":\"P56\",\"datavalue\":{\"value\":\"ExampleString\",\"type\":\"string\"}},\"type\":\"statement\",\"rank\":\"normal\"}]}\n",
    "    for propertyNumber in range(0, len(propertiesColumnList)):\n",
    "        propertyId = propertiesIdList[propertyNumber]\n",
    "        statementUuidColumn = propertiesUuidColumnList[propertyNumber]\n",
    "        # If there is already a UUID, then don't write that property to the API\n",
    "        if tableData[rowNumber][statementUuidColumn] != '':\n",
    "            continue  # skip the rest of this iteration and go onto the next property\n",
    "\n",
    "        # The columns whose properties have value node contain only the column name root, so must be handled differently\n",
    "        if propertiesEntityOrLiteral[propertyNumber] == 'value':\n",
    "            valueString = tableData[rowNumber][propertiesColumnList[propertyNumber] + '_val']\n",
    "            if valueString == '':\n",
    "                continue  # skip the rest of this iteration and go onto the next property\n",
    "            # Currently time is the only kind of value node supported\n",
    "            if propertiesTypeList[propertyNumber] == 'time':\n",
    "                snakDict = {\n",
    "                    'mainsnak': {\n",
    "                        'snaktype': 'value',\n",
    "                        'property': propertiesIdList[propertyNumber],\n",
    "                        'datavalue':{\n",
    "                            'value': {\n",
    "                                'time': '+' + valueString,\n",
    "                                'timezone': 0,\n",
    "                                'before': 0,\n",
    "                                'after': 0,\n",
    "                                'precision': tableData[rowNumber][propertiesColumnList[propertyNumber] + '_prec'],\n",
    "                                'calendarmodel': \"http://www.wikidata.org/entity/Q1985727\"\n",
    "                                },\n",
    "                            'type': 'time'\n",
    "                            },\n",
    "                        'datatype': 'time'\n",
    "                        },\n",
    "                    'type': 'statement',\n",
    "                    'rank': 'normal'\n",
    "                    }\n",
    "            # If globe coordinate value or quantities become supported, they will be handled here.\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        # For other property columns, the column name is stored directly in the propertiesColumnList\n",
    "        else:\n",
    "            valueString = tableData[rowNumber][propertiesColumnList[propertyNumber]]\n",
    "            if valueString == '':\n",
    "                continue  # skip the rest of this iteration and go onto the next property\n",
    "            if propertiesEntityOrLiteral[propertyNumber] == 'literal':\n",
    "                    snakDict = {\n",
    "                    'mainsnak': {\n",
    "                        'snaktype': 'value',\n",
    "                        'property': propertiesIdList[propertyNumber],\n",
    "                        'datavalue':{\n",
    "                            'value': valueString,\n",
    "                            'type': propertiesValueTypeList[propertyNumber]\n",
    "                            },\n",
    "                        'datatype': propertiesTypeList[propertyNumber]\n",
    "                        },\n",
    "                    'type': 'statement',\n",
    "                    'rank': 'normal'\n",
    "                    }\n",
    "\n",
    "            elif propertiesEntityOrLiteral[propertyNumber] == 'entity':\n",
    "                snakDict = {\n",
    "                    'mainsnak': {\n",
    "                        'snaktype': 'value',\n",
    "                        'property': propertiesIdList[propertyNumber],\n",
    "                        'datatype': 'wikibase-item',\n",
    "                        'datavalue': {\n",
    "                            'value': {\n",
    "                                'id': valueString\n",
    "                                },\n",
    "                            'type': 'wikibase-entityid'\n",
    "                            }\n",
    "                        },\n",
    "                    'type': 'statement',\n",
    "                    'rank': 'normal'\n",
    "                    }\n",
    "            else:\n",
    "                print('This should not happen')\n",
    "\n",
    "        # Look for references and qualifiers for all properties whose values are being written\n",
    "        if len(propertiesReferencesList[propertyNumber]) != 0:  # skip references if there aren't any\n",
    "            references = createReferences(propertiesReferencesList[propertyNumber], tableData[rowNumber])\n",
    "            if references != []: # check to avoid setting references for an empty reference list\n",
    "                snakDict['references'] = references\n",
    "        if len(propertiesQualifiersList[propertyNumber]['qualPropList']) != 0:\n",
    "            qualifiers = createQualifiers(propertiesQualifiersList[propertyNumber], tableData[rowNumber])\n",
    "            if qualifiers != {}: # check for situation where no qualifier statements were made for that record\n",
    "                snakDict['qualifiers'] = qualifiers\n",
    "\n",
    "        claimsList.append(snakDict)\n",
    "\n",
    "    if claimsList != []:\n",
    "        dataStructure['claims'] = claimsList\n",
    "\n",
    "# The data value has to be turned into a JSON string\n",
    "parameterDictionary['data'] = json.dumps(dataStructure)\n",
    "print(json.dumps(dataStructure, indent = 2))\n",
    "#print(parameterDictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "  \"entity\": {\n",
      "    \"labels\": {\n",
      "      \"en\": {\n",
      "        \"language\": \"en\",\n",
      "        \"value\": \"Stanley R. Clemens\"\n",
      "      }\n",
      "    },\n",
      "    \"descriptions\": {\n",
      "      \"en\": {\n",
      "        \"language\": \"en\",\n",
      "        \"value\": \"mathematician and educator\"\n",
      "      }\n",
      "    },\n",
      "    \"aliases\": {\n",
      "      \"en\": [\n",
      "        {\n",
      "          \"language\": \"en\",\n",
      "          \"value\": \"Stan Clemens\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"sitelinks\": {},\n",
      "    \"claims\": {\n",
      "      \"P108\": [\n",
      "        {\n",
      "          \"mainsnak\": {\n",
      "            \"snaktype\": \"value\",\n",
      "            \"property\": \"P108\",\n",
      "            \"hash\": \"de571f9cebebaf7805f4c6836e9eb027b8d3f57f\",\n",
      "            \"datavalue\": {\n",
      "              \"value\": {\n",
      "                \"entity-type\": \"item\",\n",
      "                \"numeric-id\": 886141,\n",
      "                \"id\": \"Q886141\"\n",
      "              },\n",
      "              \"type\": \"wikibase-entityid\"\n",
      "            },\n",
      "            \"datatype\": \"wikibase-item\"\n",
      "          },\n",
      "          \"type\": \"statement\",\n",
      "          \"id\": \"Q101242960$B9EEE4CC-791F-4E53-9542-9671811179CE\",\n",
      "          \"rank\": \"normal\",\n",
      "          \"references\": [\n",
      "            {\n",
      "              \"hash\": \"2c1963b96bde00545c55c48774c2aa8d09c47a97\",\n",
      "              \"snaks\": {\n",
      "                \"P854\": [\n",
      "                  {\n",
      "                    \"snaktype\": \"value\",\n",
      "                    \"property\": \"P854\",\n",
      "                    \"hash\": \"d15eebebef5e272314bb43d3e5d6a894ef1dc135\",\n",
      "                    \"datavalue\": {\n",
      "                      \"value\": \"https://www.bluffton.edu/catalog/officers/faculty.aspx\",\n",
      "                      \"type\": \"string\"\n",
      "                    },\n",
      "                    \"datatype\": \"url\"\n",
      "                  }\n",
      "                ],\n",
      "                \"P813\": [\n",
      "                  {\n",
      "                    \"snaktype\": \"value\",\n",
      "                    \"property\": \"P813\",\n",
      "                    \"hash\": \"bc92f3b0b3a4ac82c78818993bf9fe814aa6699b\",\n",
      "                    \"datavalue\": {\n",
      "                      \"value\": {\n",
      "                        \"time\": \"+2020-11-06T00:00:00Z\",\n",
      "                        \"timezone\": 0,\n",
      "                        \"before\": 0,\n",
      "                        \"after\": 0,\n",
      "                        \"precision\": 11,\n",
      "                        \"calendarmodel\": \"http://www.wikidata.org/entity/Q1985727\"\n",
      "                      },\n",
      "                      \"type\": \"time\"\n",
      "                    },\n",
      "                    \"datatype\": \"time\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              \"snaks-order\": [\n",
      "                \"P854\",\n",
      "                \"P813\"\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"P31\": [\n",
      "        {\n",
      "          \"mainsnak\": {\n",
      "            \"snaktype\": \"value\",\n",
      "            \"property\": \"P31\",\n",
      "            \"hash\": \"ad7d38a03cdd40cdc373de0dc4e7b7fcbccb31d9\",\n",
      "            \"datavalue\": {\n",
      "              \"value\": {\n",
      "                \"entity-type\": \"item\",\n",
      "                \"numeric-id\": 5,\n",
      "                \"id\": \"Q5\"\n",
      "              },\n",
      "              \"type\": \"wikibase-entityid\"\n",
      "            },\n",
      "            \"datatype\": \"wikibase-item\"\n",
      "          },\n",
      "          \"type\": \"statement\",\n",
      "          \"id\": \"Q101242960$F1DD1696-B213-4AA4-A267-541C905EF8B4\",\n",
      "          \"rank\": \"normal\"\n",
      "        }\n",
      "      ],\n",
      "      \"P21\": [\n",
      "        {\n",
      "          \"mainsnak\": {\n",
      "            \"snaktype\": \"value\",\n",
      "            \"property\": \"P21\",\n",
      "            \"hash\": \"85ad4b1c7348f7a5aac521135040d74e91fb5939\",\n",
      "            \"datavalue\": {\n",
      "              \"value\": {\n",
      "                \"entity-type\": \"item\",\n",
      "                \"numeric-id\": 6581097,\n",
      "                \"id\": \"Q6581097\"\n",
      "              },\n",
      "              \"type\": \"wikibase-entityid\"\n",
      "            },\n",
      "            \"datatype\": \"wikibase-item\"\n",
      "          },\n",
      "          \"type\": \"statement\",\n",
      "          \"id\": \"Q101242960$F234AC6C-A729-419E-8EA8-49BA61341749\",\n",
      "          \"rank\": \"normal\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"id\": \"Q101242960\",\n",
      "    \"type\": \"item\",\n",
      "    \"lastrevid\": 1303475809\n",
      "  },\n",
      "  \"success\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if maxlag > 0:\n",
    "    parameterDictionary['maxlag'] = maxlag\n",
    "responseData = attemptPost(endpointUrl, parameterDictionary)\n",
    "responseDataSafe = responseData\n",
    "#print('Write confirmation: ', responseData)\n",
    "print()\n",
    "print(json.dumps(responseData, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "responseData = responseDataSafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "responseDataJson = '''{\n",
    "  \"entity\": {\n",
    "    \"labels\": {\n",
    "      \"en\": {\n",
    "        \"language\": \"en\",\n",
    "        \"value\": \"Michael David Edmiston\"\n",
    "      }\n",
    "    },\n",
    "    \"descriptions\": {\n",
    "      \"en\": {\n",
    "        \"language\": \"en\",\n",
    "        \"value\": \"physicist and educator\"\n",
    "      }\n",
    "    },\n",
    "    \"aliases\": {\n",
    "      \"en\": [\n",
    "        {\n",
    "          \"language\": \"en\",\n",
    "          \"value\": \"Mike Edmiston\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"sitelinks\": {},\n",
    "    \"claims\": {\n",
    "      \"P108\": [\n",
    "        {\n",
    "          \"mainsnak\": {\n",
    "            \"snaktype\": \"value\",\n",
    "            \"property\": \"P108\",\n",
    "            \"hash\": \"de571f9cebebaf7805f4c6836e9eb027b8d3f57f\",\n",
    "            \"datavalue\": {\n",
    "              \"value\": {\n",
    "                \"entity-type\": \"item\",\n",
    "                \"numeric-id\": 886141,\n",
    "                \"id\": \"Q886141\"\n",
    "              },\n",
    "              \"type\": \"wikibase-entityid\"\n",
    "            },\n",
    "            \"datatype\": \"wikibase-item\"\n",
    "          },\n",
    "          \"type\": \"statement\",\n",
    "          \"id\": \"Q101242220$86C233F9-7E48-406D-9950-18C13D35FB7E\",\n",
    "          \"rank\": \"normal\",\n",
    "          \"references\": [\n",
    "            {\n",
    "              \"hash\": \"2c1963b96bde00545c55c48774c2aa8d09c47a97\",\n",
    "              \"snaks\": {\n",
    "                \"P854\": [\n",
    "                  {\n",
    "                    \"snaktype\": \"value\",\n",
    "                    \"property\": \"P854\",\n",
    "                    \"hash\": \"d15eebebef5e272314bb43d3e5d6a894ef1dc135\",\n",
    "                    \"datavalue\": {\n",
    "                      \"value\": \"https://www.bluffton.edu/catalog/officers/faculty.aspx\",\n",
    "                      \"type\": \"string\"\n",
    "                    },\n",
    "                    \"datatype\": \"url\"\n",
    "                  }\n",
    "                ],\n",
    "                \"P813\": [\n",
    "                  {\n",
    "                    \"snaktype\": \"value\",\n",
    "                    \"property\": \"P813\",\n",
    "                    \"hash\": \"bc92f3b0b3a4ac82c78818993bf9fe814aa6699b\",\n",
    "                    \"datavalue\": {\n",
    "                      \"value\": {\n",
    "                        \"time\": \"+2020-11-06T00:00:00Z\",\n",
    "                        \"timezone\": 0,\n",
    "                        \"before\": 0,\n",
    "                        \"after\": 0,\n",
    "                        \"precision\": 11,\n",
    "                        \"calendarmodel\": \"http://www.wikidata.org/entity/Q1985727\"\n",
    "                      },\n",
    "                      \"type\": \"time\"\n",
    "                    },\n",
    "                    \"datatype\": \"time\"\n",
    "                  }\n",
    "                ]\n",
    "              },\n",
    "              \"snaks-order\": [\n",
    "                \"P854\",\n",
    "                \"P813\"\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"P31\": [\n",
    "        {\n",
    "          \"mainsnak\": {\n",
    "            \"snaktype\": \"value\",\n",
    "            \"property\": \"P31\",\n",
    "            \"hash\": \"ad7d38a03cdd40cdc373de0dc4e7b7fcbccb31d9\",\n",
    "            \"datavalue\": {\n",
    "              \"value\": {\n",
    "                \"entity-type\": \"item\",\n",
    "                \"numeric-id\": 5,\n",
    "                \"id\": \"Q5\"\n",
    "              },\n",
    "              \"type\": \"wikibase-entityid\"\n",
    "            },\n",
    "            \"datatype\": \"wikibase-item\"\n",
    "          },\n",
    "          \"type\": \"statement\",\n",
    "          \"id\": \"Q101242220$7448739D-84D9-4241-90C1-53707122452D\",\n",
    "          \"rank\": \"normal\"\n",
    "        }\n",
    "      ],\n",
    "      \"P21\": [\n",
    "        {\n",
    "          \"mainsnak\": {\n",
    "            \"snaktype\": \"value\",\n",
    "            \"property\": \"P21\",\n",
    "            \"hash\": \"85ad4b1c7348f7a5aac521135040d74e91fb5939\",\n",
    "            \"datavalue\": {\n",
    "              \"value\": {\n",
    "                \"entity-type\": \"item\",\n",
    "                \"numeric-id\": 6581097,\n",
    "                \"id\": \"Q6581097\"\n",
    "              },\n",
    "              \"type\": \"wikibase-entityid\"\n",
    "            },\n",
    "            \"datatype\": \"wikibase-item\"\n",
    "          },\n",
    "          \"type\": \"statement\",\n",
    "          \"id\": \"Q101242220$7149DDA2-CCE5-4DF9-AF11-B3EF192004DA\",\n",
    "          \"rank\": \"normal\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \"id\": \"Q101242220\",\n",
    "    \"type\": \"item\",\n",
    "    \"lastrevid\": 1303433739\n",
    "  },\n",
    "  \"success\": 1\n",
    "}'''\n",
    "responseData = json.loads(responseDataJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv statement number:  0\n",
      "csv statement number:  1\n",
      "\n",
      "{'mainsnak': {'snaktype': 'value', 'property': 'P108', 'hash': 'de571f9cebebaf7805f4c6836e9eb027b8d3f57f', 'datavalue': {'value': {'entity-type': 'item', 'numeric-id': 886141, 'id': 'Q886141'}, 'type': 'wikibase-entityid'}, 'datatype': 'wikibase-item'}, 'type': 'statement', 'id': 'Q101242960$B9EEE4CC-791F-4E53-9542-9671811179CE', 'rank': 'normal', 'references': [{'hash': '2c1963b96bde00545c55c48774c2aa8d09c47a97', 'snaks': {'P854': [{'snaktype': 'value', 'property': 'P854', 'hash': 'd15eebebef5e272314bb43d3e5d6a894ef1dc135', 'datavalue': {'value': 'https://www.bluffton.edu/catalog/officers/faculty.aspx', 'type': 'string'}, 'datatype': 'url'}], 'P813': [{'snaktype': 'value', 'property': 'P813', 'hash': 'bc92f3b0b3a4ac82c78818993bf9fe814aa6699b', 'datavalue': {'value': {'time': '+2020-11-06T00:00:00Z', 'timezone': 0, 'before': 0, 'after': 0, 'precision': 11, 'calendarmodel': 'http://www.wikidata.org/entity/Q1985727'}, 'type': 'time'}, 'datatype': 'time'}]}, 'snaks-order': ['P854', 'P813']}]}\n",
      "\n",
      "responseReference {'hash': '2c1963b96bde00545c55c48774c2aa8d09c47a97', 'snaks': {'P854': [{'snaktype': 'value', 'property': 'P854', 'hash': 'd15eebebef5e272314bb43d3e5d6a894ef1dc135', 'datavalue': {'value': 'https://www.bluffton.edu/catalog/officers/faculty.aspx', 'type': 'string'}, 'datatype': 'url'}], 'P813': [{'snaktype': 'value', 'property': 'P813', 'hash': 'bc92f3b0b3a4ac82c78818993bf9fe814aa6699b', 'datavalue': {'value': {'time': '+2020-11-06T00:00:00Z', 'timezone': 0, 'before': 0, 'after': 0, 'precision': 11, 'calendarmodel': 'http://www.wikidata.org/entity/Q1985727'}, 'type': 'time'}, 'datatype': 'time'}]}, 'snaks-order': ['P854', 'P813']}\n",
      "table reference ref prop list: ['P854', 'P813']\n",
      "reference property index: 0\n",
      "reference property index: 1\n",
      "here 1\n",
      "here 2\n",
      "here 4\n",
      "adding the reference hash to the table 2c1963b96bde00545c55c48774c2aa8d09c47a97\n",
      "csv statement number:  2\n",
      "csv statement number:  3\n",
      "\n",
      "{'mainsnak': {'snaktype': 'value', 'property': 'P31', 'hash': 'ad7d38a03cdd40cdc373de0dc4e7b7fcbccb31d9', 'datavalue': {'value': {'entity-type': 'item', 'numeric-id': 5, 'id': 'Q5'}, 'type': 'wikibase-entityid'}, 'datatype': 'wikibase-item'}, 'type': 'statement', 'id': 'Q101242960$F1DD1696-B213-4AA4-A267-541C905EF8B4', 'rank': 'normal'}\n",
      "csv statement number:  4\n",
      "\n",
      "{'mainsnak': {'snaktype': 'value', 'property': 'P21', 'hash': '85ad4b1c7348f7a5aac521135040d74e91fb5939', 'datavalue': {'value': {'entity-type': 'item', 'numeric-id': 6581097, 'id': 'Q6581097'}, 'type': 'wikibase-entityid'}, 'datatype': 'wikibase-item'}, 'type': 'statement', 'id': 'Q101242960$F234AC6C-A729-419E-8EA8-49BA61341749', 'rank': 'normal'}\n"
     ]
    }
   ],
   "source": [
    "if newItem:\n",
    "    # extract the entity Q number from the response JSON\n",
    "    tableData[rowNumber][subjectWikidataIdColumnHeader] = responseData['entity']['id']\n",
    "\n",
    "# fill into the table the values of newly created claims and references\n",
    "for statementIndex in range(0, len(propertiesIdList)):\n",
    "    print(\"csv statement number: \", statementIndex)\n",
    "    referencesForStatement = propertiesReferencesList[statementIndex]\n",
    "    #print(tableData[rowNumber][propertiesColumnList[statementIndex]])\n",
    "\n",
    "    # need to find out if the value is empty. Value-node values must have their nodeId's checked. Otherwise, just check whether the cell is empty.\n",
    "    if propertiesEntityOrLiteral[statementIndex] =='value':\n",
    "        if tableData[rowNumber][propertiesColumnList[statementIndex] + '_nodeId'] == '':\n",
    "            value = False\n",
    "        else:\n",
    "            value = True\n",
    "    else:\n",
    "        if tableData[rowNumber][propertiesColumnList[statementIndex]] == '':\n",
    "            value = False\n",
    "        else:\n",
    "            value = True\n",
    "    # only add the claim if the UUID cell for that row is empty AND there is a value for the property\n",
    "    if tableData[rowNumber][propertiesUuidColumnList[statementIndex]] =='' and value:\n",
    "        count = 0\n",
    "        statementFound = False\n",
    "        # If there are multiple values for a property, this will loop through more than one statement\n",
    "        for statement in responseData['entity']['claims'][propertiesIdList[statementIndex]]:\n",
    "            print()\n",
    "            print(statement)\n",
    "\n",
    "            # does the value in the cell equal the mainsnak value of the claim?\n",
    "            # it's necessary to check this because there could be other previous claims for that property (i.e. multiple values)\n",
    "            if propertiesEntityOrLiteral[statementIndex] == 'literal':\n",
    "                statementFound = tableData[rowNumber][propertiesColumnList[statementIndex]] == statement['mainsnak']['datavalue']['value']\n",
    "            elif propertiesEntityOrLiteral[statementIndex] == 'entity':\n",
    "                statementFound = tableData[rowNumber][propertiesColumnList[statementIndex]] == statement['mainsnak']['datavalue']['value']['id']\n",
    "            elif propertiesEntityOrLiteral[statementIndex] == 'value':\n",
    "                if propertiesTypeList[statementIndex] == 'time':\n",
    "                    # need to handle negative dates (BCE)\n",
    "                    if tableData[rowNumber][propertiesColumnList[statementIndex] + '_val'][0] == '-':\n",
    "                        # make comparison with the leading minus present\n",
    "                        statementFound = tableData[rowNumber][propertiesColumnList[statementIndex] + '_val'] == statement['mainsnak']['datavalue']['value']['time']\n",
    "                    else:\n",
    "                        # must add leading plus (not stored in the table) to match the non-standard plus included by Wikibase\n",
    "                        statementFound = ('+' + tableData[rowNumber][propertiesColumnList[statementIndex] + '_val']) == statement['mainsnak']['datavalue']['value']['time']\n",
    "                else: # in the future, when other node value types are supported, the code here will need to be expanded to cover the other types\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "            if statementFound:\n",
    "                count += 1\n",
    "                if count > 1:\n",
    "                    # I don't think this should actually happen, since if there were already at least one statement with this value,\n",
    "                    # it would have already been downloaded in the processing prior to running this script.\n",
    "                    print('Warning: duplicate statement ', tableData[rowNumber][subjectWikidataIdColumnHeader], ' ', propertiesIdList[statementIndex], ' ', tableData[rowNumber][propertiesColumnList[statementIndex]])\n",
    "                tableData[rowNumber][propertiesUuidColumnList[statementIndex]] = statement['id'].split('$')[1]  # just keep the UUID part after the dollar sign\n",
    "\n",
    "                # Search for each reference type (set of reference properties) that's being tracked for a particular property's statements\n",
    "                for tableReference in referencesForStatement: # loop will not be executed when length of referenceForStatement = 0 (no references tracked for this property)\n",
    "                    # Check for an exact match of reference properties and their values (since we're looking for reference for a statement that was written)\n",
    "                    # Step through each reference that came back for the statement we are interested in\n",
    "                    for responseReference in statement['references']: # \"outer loop\"\n",
    "                        print()\n",
    "                        print(\"responseReference\", responseReference)\n",
    "                        # Perform a screening process on each returned reference by stepping through each property associated with a refernce type\n",
    "                        # and trying to match it. If the path to the value doesn't exist, there will be an exception and that reference \n",
    "                        # can be ignored. Only if the values for all of the reference properties match will the hash be recorded.\n",
    "                        referenceMatch = True\n",
    "                        print('table reference ref prop list:', tableReference['refPropList'])\n",
    "                        for referencePropertyIndex in range(0, len(tableReference['refPropList'])): # \"inner loop\" to check each property in the reference\n",
    "                            print('reference property index:', referencePropertyIndex)\n",
    "                            try:\n",
    "                                # First try to see if the values in the response JSON for the property match\n",
    "                                if tableReference['refEntityOrLiteral'][referencePropertyIndex] == 'value':\n",
    "                                    print(\"here 1\")\n",
    "                                    # The values for times are buried a layer deeper in the JSON than other types.\n",
    "                                    if tableReference['refTypeList'][referencePropertyIndex] == 'time':\n",
    "                                        print(\"here 2\")\n",
    "                                        # need to handle negative dates (BCE)\n",
    "                                        if tableData[rowNumber][tableReference['refValueColumnList'][referencePropertyIndex] + '_val'][0] == '-':\n",
    "                                            print(\"here 3\")\n",
    "                                            # make comparison with the leading minus present\n",
    "                                            if responseReference['snaks'][tableReference['refPropList'][referencePropertyIndex]][0]['datavalue']['value']['time'] != tableData[rowNumber][tableReference['refValueColumnList'][referencePropertyIndex] + '_val']:\n",
    "                                                referenceMatch = False\n",
    "                                                print(\"Kill #1\")\n",
    "                                                break # kill the inner loop because this value doesn't match\n",
    "                                        else:\n",
    "                                            print(\"here 4\")\n",
    "                                            # must add leading plus (not stored in the table) to match the non-standard plus included by Wikibase\n",
    "                                            # Note that this assumes the first value for a particular reference property. It appears to be unusual for there to be more than one.\n",
    "                                            if responseReference['snaks'][tableReference['refPropList'][referencePropertyIndex]][0]['datavalue']['value']['time'] != '+' + tableData[rowNumber][tableReference['refValueColumnList'][referencePropertyIndex] + '_val']:\n",
    "                                                referenceMatch = False\n",
    "                                                print(\"Kill #2\")\n",
    "                                                break # kill the inner loop because this value doesn't match\n",
    "                                    else: # here is where node-valued types other than time will be handled\n",
    "                                        pass\n",
    "                                else: # Values for types other than node-valued have direct literal values of 'value'\n",
    "                                    if responseReference['snaks'][tableReference['refPropList'][referencePropertyIndex]][0]['datavalue']['value'] != tableData[rowNumber][tableReference['refValueColumnList'][referencePropertyIndex]]:\n",
    "                                        referenceMatch = False\n",
    "                                        print(\"Kill #3\")\n",
    "                                        break # kill the inner loop because this value doesn't match\n",
    "                                # So far, so good -- the value for this property matches\n",
    "                            except:\n",
    "                                # An exception occured because the JSON \"path\" to the value didn't match. So this isn't the right property\n",
    "                                referenceMatch = False\n",
    "                                print(\"Kill #4\")\n",
    "                                break # kill the inner loop because the property doesn't match\n",
    "\n",
    "                            # OK, we got all the way through on this property with it and its value matching, so referenceMatch will still be True\n",
    "                            # The inner loop can continue on to the next property to see if it and its value match.\n",
    "\n",
    "                        # If we got to this point, the inner loop completed withoug being killed. referenceMatch should still be True\n",
    "                        # So this is a match to the reference that we wrote and we need to grab the reference hash\n",
    "                        print('adding the reference hash to the table', responseReference['hash'])\n",
    "                        tableData[rowNumber][tableReference['refHashColumn']] = responseReference['hash']\n",
    "                        # It is not necessary to continue on with the next iteration of the outer loop since we found the reference we wanted.\n",
    "                        # So we can kill the outer loop with the value of referenceMatch being True\n",
    "                        break\n",
    "\n",
    "                    # At this point, the outer loop is finished. Either a response reference has matched or all response references have been checked.\n",
    "                    # Since this check only happens for newly written statements, referenceMatch should always be True since the exact reference was written.\n",
    "                    # But better give an error message if for some reason no reference matched.\n",
    "                    if referenceMatch == False:\n",
    "                        print('No reference in the response JSON matched with the reference for statement:', tableData[rowNumber][subjectWikidataIdColumnHeader], ' ', propertiesIdList[statementIndex], ' ', tableData[rowNumber][propertiesColumnList[statementIndex]])\n",
    "                        print('Reference  ', tableReference)\n",
    "\n",
    "                    # The script will now move on to checking the next reference in the table.\n",
    "\n",
    "        # Print this error message only if there is not match to any of the values after looping through all of the matching properties\n",
    "        # This should never happen because this code is only executed when the statement doesn't have a UUID (i.e. not previously written)\n",
    "        if count == 0:\n",
    "            print('did not find', tableData[rowNumber][propertiesColumnList[statementIndex]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-06T00:00:00Z\n",
      "['orcid', 'employer', 'affiliation', 'instanceOf', 'sexOrGenderQId']\n",
      "employer\n",
      "Q\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'employer_val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-ceab97bc5828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpropertiesColumnList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpropertiesColumnList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpropertiesColumnList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'employer_val'"
     ]
    }
   ],
   "source": [
    "print(tableData[rowNumber][tableReference['refValueColumnList'][referencePropertyIndex] + '_val'])\n",
    "print(propertiesColumnList)\n",
    "print(propertiesColumnList[1])\n",
    "print(tableData[0][propertiesColumnList[1]][0])\n",
    "print(tableData[0][propertiesColumnList[1] + '_val'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the table with a new one containing any new IDs\n",
    "# Note: I'm writing after every line so that if the script crashes, no data will be lost\n",
    "writeToFile(tableFileName, fieldnames, tableData)\n",
    "#with open(tableFileName, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#    writer.writeheader()\n",
    "#    for rowNumber in range(0, len(tableData)):\n",
    "#        try:\n",
    "#            writer.writerow(tableData[rowNumber])\n",
    "#        except:\n",
    "#            print('ERROR row:', rowNumber, '  ', tableData[rowNumber])\n",
    "#            print()\n",
    "\n",
    "# The limit for bots without a bot flag seems to be 50 writes per minute. That's 1.2 s between writes.\n",
    "# To be safe and avoid getting blocked, use 1.25 s.\n",
    "sleep(1.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process each row of the table for references of existing claims\n",
    "print('Writing references of existing claims')\n",
    "print('--------------------------')\n",
    "print()\n",
    "for rowNumber in range(0, len(tableData)):\n",
    "    print('processing row ', rowNumber, 'id:', tableData[rowNumber][subjectWikidataIdColumnHeader])\n",
    "\n",
    "    for propertyNumber in range(0, len(propertiesColumnList)):\n",
    "        propertyId = propertiesIdList[propertyNumber]\n",
    "        statementUuidColumn = propertiesUuidColumnList[propertyNumber]     \n",
    "        # We are only interested in writing references for statements that already have UUIDs\n",
    "        if tableData[rowNumber][statementUuidColumn] != '':\n",
    "            if len(propertiesReferencesList[propertyNumber]) != 0:  # skip that claim if it doesn't have references\n",
    "\n",
    "                for reference in propertiesReferencesList[propertyNumber]:\n",
    "                    if tableData[rowNumber][reference['refHashColumn']] == '': # process only new references\n",
    "                        # in this script, the createReferences function returns a snak dictionary, not a list\n",
    "                        referencesDict = createReferenceSnak(reference, tableData[rowNumber])\n",
    "                        if referencesDict == {}: # Check for the case where no references were specified for this record\n",
    "                            #print('no data to write')\n",
    "                            #print()\n",
    "                            pass\n",
    "                        else:\n",
    "                            # print(json.dumps(referencesDict, indent=2))\n",
    "                            # build the parameter string to be posted to the API\n",
    "                            parameterDictionary = {\n",
    "                                'action': 'wbsetreference',\n",
    "                                'statement': tableData[rowNumber][subjectWikidataIdColumnHeader] + \"$\" + tableData[rowNumber][statementUuidColumn],\n",
    "                                'format':'json',\n",
    "                                'token': csrfToken,\n",
    "                                'snaks': json.dumps(referencesDict)\n",
    "                                }\n",
    "                            if maxlag > 0:\n",
    "                                parameterDictionary['maxlag'] = maxlag\n",
    "                            # print(json.dumps(parameterDictionary, indent = 2))\n",
    "\n",
    "                            # print('ref:', reference['refValueColumnList'])\n",
    "                            responseData = attemptPost(endpointUrl, parameterDictionary)\n",
    "                            print('Write confirmation: ', responseData)\n",
    "                            print()\n",
    "\n",
    "                            tableData[rowNumber][reference['refHashColumn']] = responseData['reference']['hash']\n",
    "\n",
    "                            # Replace the table with a new one containing any new IDs\n",
    "                            # Note: I'm writing after every line so that if the script crashes, no data will be lost\n",
    "                            writeToFile(tableFileName, fieldnames, tableData)\n",
    "\n",
    "                            #with open(tableFileName, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                            #    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                            #    writer.writeheader()\n",
    "                            #    for writeRowNumber in range(0, len(tableData)):\n",
    "                            #        writer.writerow(tableData[writeRowNumber])\n",
    "\n",
    "                            # The limit for bots without a bot flag seems to be 50 writes per minute. That's 1.2 s between writes.\n",
    "                            # To be safe and avoid getting blocked, use 1.25 s.\n",
    "                            sleep(1.25)\n",
    "print()\n",
    "print('done')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
