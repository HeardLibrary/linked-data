{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VanderDeleteBot\n",
    "\n",
    "A Python script to delete Wikidata claims. Requires knowing the Q ID of the item and the UUID for the claim. Both of these identifiers are routinely stored after a VanderBot upload.\n",
    "\n",
    "Designed as an add-on to [VanderBot](http://vanderbi.lt/vanderbot) and most of the configuration and functions are copied from the `vanderbot.py` script there, so go there for more explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '0.1'\n",
    "created = '2022-05-05'\n",
    "\n",
    "# (c) 2022 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import sys\n",
    "import uuid\n",
    "import re\n",
    "from datetime import datetime\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "\n",
    "# Set script-wide variable values. \n",
    "\n",
    "claims_to_delete_filename = 'test_delete.csv'\n",
    "claim_uuid_column_name = 'instance_of_uuid' # Note Q ID column is hard coded to \"qid\"\n",
    "log_path = 'delete_log.txt' # path to log file, default to none if empty string\n",
    "\n",
    "error_log = '' # start the error log\n",
    "\n",
    "if log_path != '':\n",
    "    log_object = open(log_path, 'wt', encoding='utf-8') # direct output sent to log_object to log file instead of sys.stdout\n",
    "else:\n",
    "    log_object = sys.stdout # log output defaults to the console screen\n",
    "\n",
    "credentials_path_string = 'home' # value is \"home\", \"working\", \"gdrive\", or a relative or absolute path with trailing \"/\"\n",
    "credentials_filename = 'wikibase_credentials.txt' # name of the API credentials file\n",
    "\n",
    "if credentials_path_string == 'home': # credential file is in home directory\n",
    "    home = str(Path.home()) # gets path to home directory; works for both Win and Mac\n",
    "    credentials_path = home + '/' + credentials_filename\n",
    "elif credentials_path_string == 'working': # credential file is in current working directory\n",
    "    credentials_path = credentials_filename\n",
    "else:  # credential file is in a directory whose path was specified by the credential_path_string\n",
    "    credentials_path = credentials_path_string + credentials_filename\n",
    "\n",
    "# The limit for bots without a bot flag seems to be 50 writes per minute. That's 1.2 s between writes.\n",
    "# To be safe and avoid getting blocked, leave the api_sleep value at its default: 1.25 s.\n",
    "# The option to increase the delay is offered if the user is a \"newbie\", defined as having an\n",
    "# account less than four days old and with fewer than 50 edits. The newbie limit is 8 edits per minute.\n",
    "# Therefore, newbies should set the API sleep value to 8 to avoid getting blocked.\n",
    "api_sleep = 1.25\n",
    "\n",
    "# DO NOT decrease this limit unless you have obtained a bot flag! If you have a bot flag, then you have created your own\n",
    "# User-Agent and are not using VanderBot any more. In that case, you must change the user_agent_header below to reflect\n",
    "# your own information. DO NOT get me in trouble by saying you are using my User-Agent if you are going to violate \n",
    "# Wikimedia guidelines !!!\n",
    "if api_sleep < 1.25:\n",
    "    api_sleep = 1.25\n",
    "\n",
    "# See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "user_agent_header = 'VanderDeleteBot/' + version + ' (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "# If you don't know what you are doing, leave this value alone. In any case, it is rude to use a value greater than 5.\n",
    "maxlag = 5\n",
    "\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "def generate_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/json',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# Generate the request header using the function above\n",
    "request_header = generate_header_dictionary(accept_media_type,user_agent_header)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# function definitions\n",
    "\n",
    "def retrieveCredentials(path):\n",
    "    with open(path, 'rt') as fileObject:\n",
    "        lineList = fileObject.read().split('\\n')\n",
    "    endpointUrl = lineList[0].split('=')[1]\n",
    "    username = lineList[1].split('=')[1]\n",
    "    password = lineList[2].split('=')[1]\n",
    "    #userAgent = lineList[3].split('=')[1]\n",
    "    credentials = [endpointUrl, username, password]\n",
    "    return credentials\n",
    "\n",
    "def getLoginToken(apiUrl):    \n",
    "    parameters = {\n",
    "        'action':'query',\n",
    "        'meta':'tokens',\n",
    "        'type':'login',\n",
    "        'format':'json'\n",
    "    }\n",
    "    r = session.get(url=apiUrl, params=parameters)\n",
    "    data = r.json()\n",
    "    return data['query']['tokens']['logintoken']\n",
    "\n",
    "def logIn(apiUrl, token, username, password):\n",
    "    parameters = {\n",
    "        'action':'login',\n",
    "        'lgname':username,\n",
    "        'lgpassword':password,\n",
    "        'lgtoken':token,\n",
    "        'format':'json'\n",
    "    }\n",
    "    r = session.post(apiUrl, data=parameters)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "def getCsrfToken(apiUrl):\n",
    "    parameters = {\n",
    "        \"action\": \"query\",\n",
    "        \"meta\": \"tokens\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    r = session.get(url=apiUrl, params=parameters)\n",
    "    data = r.json()\n",
    "    return data[\"query\"][\"tokens\"][\"csrftoken\"]\n",
    "\n",
    "# This function attempts to post and handles maxlag errors\n",
    "def attempt_post(apiUrl, parameters):\n",
    "    maxRetries = 10\n",
    "    # Wikidata recommends a retry delay of at least 5 seconds.\n",
    "    # This differs from api_sleep, which is the delay when there is no lag. The baseDelay is a starting point; the\n",
    "    # actual delay is increased with each retry after the server reports being lagged.\n",
    "    baseDelay = 5\n",
    "    delayLimit = 300\n",
    "    retry = 0\n",
    "    # maximum number of times to retry lagged server = maxRetries\n",
    "    while retry <= maxRetries:\n",
    "        if retry > 0:\n",
    "            print('retry:', retry)\n",
    "        r = session.post(apiUrl, data = parameters)\n",
    "        data = r.json()\n",
    "        try:\n",
    "            # check if response is a maxlag error\n",
    "            # see https://www.mediawiki.org/wiki/Manual:Maxlag_parameter\n",
    "            if data['error']['code'] == 'maxlag':\n",
    "                print('Lag of ', data['error']['lag'], ' seconds.')\n",
    "                # recommended delay is basically useless\n",
    "                # recommendedDelay = int(r.headers['Retry-After'])\n",
    "                #if recommendedDelay < 5:\n",
    "                    # recommendation is to wait at least 5 seconds if server is lagged\n",
    "                #    recommendedDelay = 5\n",
    "                recommendedDelay = baseDelay*2**retry # double the delay with each retry \n",
    "                if recommendedDelay > delayLimit:\n",
    "                    recommendedDelay = delayLimit\n",
    "                if retry != maxRetries:\n",
    "                    print('Waiting ', recommendedDelay , ' seconds.')\n",
    "                    print()\n",
    "                    sleep(recommendedDelay)\n",
    "                retry += 1\n",
    "\n",
    "                # after this, go out of if and try code blocks\n",
    "            else:\n",
    "                # an error code is returned, but it's not maxlag\n",
    "                return data\n",
    "        except:\n",
    "            # if the response doesn't have an error key, it was successful, so return\n",
    "            return data\n",
    "        # here's where execution goes after the delay\n",
    "    # here's where execution goes after maxRetries tries\n",
    "    print('Failed after ' + str(maxRetries) + ' retries.')\n",
    "    exit() # just abort the script\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# authentication\n",
    "\n",
    "# default API resource URL when a Wikibase/Wikidata instance is installed.\n",
    "resourceUrl = '/w/api.php'\n",
    "\n",
    "credentials = retrieveCredentials(credentials_path)\n",
    "endpointUrl = credentials[0] + resourceUrl\n",
    "user = credentials[1]\n",
    "pwd = credentials[2]\n",
    "#userAgentHeader = credentials[3]\n",
    "\n",
    "# Instantiate session outside of any function so that it's globally accessible.\n",
    "session = requests.Session()\n",
    "# Set default User-Agent header so you don't have to send it with every request\n",
    "session.headers.update({'User-Agent': user_agent_header})\n",
    "\n",
    "loginToken = getLoginToken(endpointUrl)\n",
    "data = logIn(endpointUrl, loginToken, user, pwd)\n",
    "csrfToken = getCsrfToken(endpointUrl)\n",
    "\n",
    "# -------------------------------------------\n",
    "# Beginning of script to process the table\n",
    "\n",
    "# The input data is in a CSV file (name specified in the configuration section) with only two required columns: \n",
    "#`qid` and the UUID column whose name is specified in the configureation section. Other columns may be present, \n",
    "#but will be ignored. One can create this table by copying and pasting from a VanderBot upload table using the \n",
    "#`property_name_uuid` column associated with `property_name`.\n",
    "\n",
    "# For information about the wbremoveclaims action, see\n",
    "# https://www.wikidata.org/w/api.php?action=help&modules=wbremoveclaims\n",
    "# https://www.wikidata.org/wiki/Special:ApiSandbox#action=wbremoveclaims&claim=Q4115189$D8404CDA-25E4-4334-AF13-A3290BCD9C0N&token=foobar&baserevid=7201010\n",
    "\n",
    "# Here's what the request JSON looks like.\n",
    "'''\n",
    "{\n",
    "\t\"action\": \"wbremoveclaims\",\n",
    "\t\"format\": \"json\",\n",
    "\t\"claim\": \"Q15397819$7C27786A-5FA6-4813-83B8-8ED8A81FB7D3\",\n",
    "\t\"token\": \"5378abbde76544dfb260e49000bf828b6274226d+\\\\\"\n",
    "}\n",
    "'''\n",
    "\n",
    "# Here's what the response JSON looks like.\n",
    "'''\n",
    "{\n",
    "    \"pageinfo\": {\n",
    "        \"lastrevid\": 1632748100\n",
    "    },\n",
    "    \"success\": 1,\n",
    "    \"claims\": [\n",
    "        \"Q15397819$7C27786A-5FA6-4813-83B8-8ED8A81FB7D3\"\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "\n",
    "full_error_log = '' # start the full error log\n",
    "\n",
    "claims_to_delete_frame = pd.read_csv(claims_to_delete_filename, na_filter=False, dtype = str)\n",
    "\n",
    "for index, claim_row in claims_to_delete_frame.iterrows():\n",
    "    qid = claim_row['qid']\n",
    "    uuid = claim_row[claim_uuid_column_name]\n",
    "\n",
    "    print('deleting:', index, qid, uuid)\n",
    "\n",
    "    # build the parameter string to be posted to the API\n",
    "    parameterDictionary = {\n",
    "        'action': 'wbremoveclaims',\n",
    "        'format':'json',\n",
    "        'token': csrfToken\n",
    "        }\n",
    "\n",
    "    # The data value has to be turned into a JSON string\n",
    "    parameterDictionary['claim'] = qid + '$' + uuid\n",
    "    #print(json.dumps(dataStructure, indent = 2))\n",
    "    #print(parameterDictionary)\n",
    "\n",
    "    if maxlag > 0:\n",
    "        parameterDictionary['maxlag'] = maxlag\n",
    "    responseData = attempt_post(endpointUrl, parameterDictionary)\n",
    "    print('Delete confirmation: ', json.dumps(responseData), file=log_object)\n",
    "    print('', file=log_object)\n",
    "\n",
    "    if 'error' in responseData:\n",
    "        error_log += 'Error message from API in row ' + str(index) + ': ' + responseData['error']['info'] + '\\n'\n",
    "        print('Error message from API in row ' + str(index) + ': ' + responseData['error']['info'] + '\\n')\n",
    "        print('failed write due to error from API', file=log_object)\n",
    "        print('', file=log_object)\n",
    "        continue # Do not try to extract data from the response JSON. Go on with the next row and leave CSV unchanged.\n",
    "\n",
    "    # Do not change this value, see top of script for an explanation\n",
    "    sleep(api_sleep)\n",
    "\n",
    "\n",
    "if error_log != '': # If there were errors display them\n",
    "    print(error_log)\n",
    "    if log_path != '': # if there is logging to a file, write the error log to the file\n",
    "        print('\\n\\n' + error_log, file=log_object)\n",
    "else:\n",
    "    print('\\nNo errors occurred.')\n",
    "    if log_path != '': # if there is logging to a file, write the error log to the file\n",
    "        print('\\n\\nNo errors occurred.', file=log_object)\n",
    "\n",
    "if log_path != '': # only close the log_object if it's a file (otherwise it's std.out)\n",
    "    log_object.close()\n",
    "\n",
    "print('done')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
