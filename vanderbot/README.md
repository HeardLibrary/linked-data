# VanderBot

The short link to this page is [vanderbi.lt/vanderbot](http://vanderbi.lt/vanderbot)

This page is about general use of the VanderBot Wikidata API-writing script and other scripts associated with it. To view information about the original VanderBot project to upload Vanderbilt researcher and scholar items, see [this page](../researcher-project.md). That page contains information about the bespoke scripts used in the original project, ways to explore the data, and release notes through v1.6 .

## Summary

VanderBot is a Python script ([vanderbot.py](../vanderbot.py)) used to create or update researcher records in Wikidata from CSV spreadsheet data. The script that writes to the API uses a customizable schema based on the W3C [Generating RDF from Tabular Data on the Web](https://www.w3.org/TR/csv2rdf/) Recommendation, making it possible to write data about any kind of item using the Wikidata API. To learn more about this aspect of the project, see [our paper currently under review](http://www.semantic-web-journal.net/content/managing-linked-open-data-wikidata-using-w3c-generating-rdf-tabular-data-web-recommendation) at the Semantic Web Journal.

Since the project started, the generalized code for writing to the API has been used with modifications of other Python scripts from the original project to carry out other projects at Vanderbilt. They include [creating records for items in the Vanderbilt Fine Arts Gallery](https://www.wikidata.org/wiki/Wikidata:WikiProject_Vanderbilt_Fine_Arts_Gallery), connecting and creating image items with the [Vanderbilt Divinity Library's Art in the Christian Tradition (ACT) database](https://www.wikidata.org/wiki/Wikidata:WikiProject_Art_in_the_Christian_Tradition_(ACT)), and managing journal data as part of the [VandyCite WikiProject](https://www.wikidata.org/wiki/Wikidata:WikiProject_VandyCite). Through these explorations, we are learning how to generalize the process so that it can be used in many areas.


## How it works

For a detailed do-it-yourself walkthrough on using VanderBot, see the series of blog posts starting with [this one](). More general instructions are below.

If you want to use the VanderBot script to upload your own data to Wikidata, you will need to create a spreadsheet with appropriate column headers and a `csv-metadata.json` file to map those headers to the Wikibase graph model according to the [Generating RDF from Tabular Data on the Web](https://www.w3.org/TR/csv2rdf/) Recommendation. To create these files, use [this form](https://heardlibrary.github.io/digital-scholarship/script/wikidata/wikidata-csv2rdf-metadata.html). Copy and paste the generated CSV header generated by the `Create CSV` button into a plain text document having the name that you specified (with `.csv` extension), then open that file with a spreadsheet program like Libre Office and enter your data. Click the `Create JSON` button, then copy and paste the JSON into a file named `csv-metadata.json` in the same directory as the CSV and `vanderbot.py`. 

The code that generates this form includes the files `wikidata-csv2rdf-metadata.html`, `wikidata-csv2rdf-metadata.js`, and `wikidata-csv2rdf-metadata.css` in this directory.

The script [acquire_wikidata_metadata.py](https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/acquire_wikidata_metadata.py), which is still under development, downloads existing data from Wikidata into a CSV file that is compatible with the format required by VanderBot. The configuration JSON used with this script is convertable to the csv2rdf schema using the [convert_json_to_metadata_schema.py](convert_json_to_metadata_schema.py) script. Another utility, [count_entities.py](https://github.com/HeardLibrary/linked-data/blob/master/json_schema/count_entities.py), can be used to count the use of properties in statements made about a defined set of items, or to determine the most common values for particular properties used in statements about those items.

# Script details

Current version: v1.7

Written by Steve Baskauf 2020-21.

Copyright 2021 Vanderbilt University. This program is released under a [GNU General Public License v3.0](http://www.gnu.org/licenses/gpl-3.0).


## Command line options

| long form | short form | values | default |
| --------- | ---------- | ------ | ------- |
| --log | -L | log filename, or path and appended filename | none |
| --json | -J | JSON metadata description filename or path and appended filename | "csv-metadata.json" |
| --credentials | -C | name of the credentials file | "wikibase_credentials.txt" |
| --path | -P | credentials directory: "home", "working", or path with trailing "/" | "home" |
| --update | -U | "allow" or "suppress" automatic updates to labels and descriptions | "suppress" |

**Examples:**

----

```
python vanderbot.py --json project-metadata.json --log ../log.txt
```

Metadata description file is called `project-metadata.json` and is in the current working directory. Progress and error logs saved to the file `log.txt` in the parent directory.

----

```
python vanderbot.py -P working -C wikidata-credentials.txt
```

Credentials file called `wikidata-credentials.txt` is in the current working directory.

----

```
python vanderbot.py --update allow -L update.log
```

Progress and error logs saved to the file `update.log` in the current working directory. Labels and descriptions of existing items in Wikidata are automatically replaced with local values if they differ.

----
Revised 2021-03-01
