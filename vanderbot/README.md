# VanderBot

The short link to this page is [vanderbi.lt/vanderbot](http://vanderbi.lt/vanderbot)

This page is about general use of the VanderBot Wikidata API-writing script and other scripts associated with it. To view information about the original VanderBot project to upload Vanderbilt researcher and scholar items, see [this page](researcher-project.md). That page contains information about the bespoke scripts used in the original project, ways to explore the data, and release notes through v1.6 .

## Summary

VanderBot is a Python script ([vanderbot.py](vanderbot.py)) used to create or update items in Wikidata using data from CSV spreadsheet data. The script uses a customizable schema based on the W3C [Generating RDF from Tabular Data on the Web](https://www.w3.org/TR/csv2rdf/) Recommendation, making it possible to write data about any kind of item using the Wikidata API. To learn more about this aspect of the project, see [our paper currently under review](http://www.semantic-web-journal.net/content/using-w3c-generating-rdf-tabular-data-web-recommendation-manage-small-wikidata-datasets) at the Semantic Web Journal.

Since the project started, the generalized code for writing to the API has been used with modifications of other Python scripts from the original project to carry out several Wikidata projects at Vanderbilt. They include [creating records for items in the Vanderbilt Fine Arts Gallery](https://www.wikidata.org/wiki/Wikidata:WikiProject_Vanderbilt_Fine_Arts_Gallery), connecting and creating image items with the [Vanderbilt Divinity Library's Art in the Christian Tradition (ACT) database](https://www.wikidata.org/wiki/Wikidata:WikiProject_Art_in_the_Christian_Tradition_(ACT)), and managing journal data as part of the [VandyCite WikiProject](https://www.wikidata.org/wiki/Wikidata:WikiProject_VandyCite). Through these explorations, we are learning how to generalize the process so that it can be used in many areas.


## How it works

For a detailed do-it-yourself walkthrough on using VanderBot, see the series of blog posts starting with [this one](http://baskauf.blogspot.com/2021/03/writing-your-own-data-to-wikidata-using.html). A video walk-through for getting started is [here](https://heardlibrary.github.io/digital-scholarship/script/wikidata/vanderbot/). More general instructions are below.

If you want to use the VanderBot script to upload your own data to Wikidata, you will need to create a spreadsheet with appropriate column headers and a `csv-metadata.json` file to map those headers to the Wikibase graph model according to the [Generating RDF from Tabular Data on the Web](https://www.w3.org/TR/csv2rdf/) Recommendation. To create those two files, use [this web tool](https://heardlibrary.github.io/digital-scholarship/script/wikidata/wikidata-csv2rdf-metadata.html). Copy and paste the generated CSV header generated by the `Create CSV` button into a plain text document having the name that you specified (with `.csv` extension), then open that file with a spreadsheet program like Libre Office and enter your data. Click the `Create JSON` button, then copy and paste the JSON into a file named `csv-metadata.json` in the same directory as the CSV and `vanderbot.py`. 

The source code that generates the web tool includes the files `wikidata-csv2rdf-metadata.html`, `wikidata-csv2rdf-metadata.js`, and `wikidata-csv2rdf-metadata.css` in [this directory](./).

Another method for generating a metadata description file is to use a simplified JSON configuration file. The script [convert_json_to_metadata_schema.py](convert_json_to_metadata_schema.py) performs the conversion and generates CSV files with appropriate headers. For more information about that script and the format of the configuration file, visit [this information page](convert-config.md). Using the script is described with much hand-holding and many screenshots in [this blog post](http://baskauf.blogspot.com/2021/03/writing-your-own-data-to-wikidata-using_7.html).

The script [acquire_wikidata_metadata.py](acquire_wikidata_metadata.py), downloads existing data from Wikidata into a CSV file that is compatible with the format required by VanderBot. It requires the same JSON configuration file as the conversion script above -- the two scripts are designed to work together. See [this page](acquire_wikidata.md) for details.

Another utility, [count_entities.py](../json_schema/count_entities.py), can be used to count the use of properties in statements made about a defined set of items, or to determine the most common values for particular properties used in statements about those items.

# Script details

Script location: <https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/vanderbot.py>

Current version: v1.7.1

Written by Steve Baskauf 2020-21.

Copyright 2021 Vanderbilt University. This program is released under a [GNU General Public License v3.0](http://www.gnu.org/licenses/gpl-3.0).

### RFC 2119 key words

The key words “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “MAY”, and “OPTIONAL” in this document are to be interpreted as described in [RFC 2119](https://tools.ietf.org/html/rfc2119).

## Credentials text file format example

The API credentials MUST be stored in a plain text file using the following format:

```
endpointUrl=https://www.wikidata.org
username=User@bot
password=465jli90dslhgoiuhsaoi9s0sj5ki3lo
```

A trailing newline is OPTIONAL.

Username and password are created on the `Bot passwords` page, accessed from `Special pages`. Wikimedia credentials are shared across all platforms (Wikipedia, Wikidata, Commons, etc.). The endpoint URL is the subdomain of a Wikibase instance -- Wikidata in the example above. The credentials file name and location MAY be set using the options below, otherwise the defaults are used.

## Command line options

| long form | short form | values | default |
| --------- | ---------- | ------ | ------- |
| --log | -L | log filename, or path and appended filename. Omit to log to console. | none |
| --json | -J | JSON metadata description filename or path and appended filename | "csv-metadata.json" |
| --credentials | -C | name of the credentials file | "wikibase_credentials.txt" |
| --path | -P | credentials directory: "home", "working", or path with trailing "/" | "home" |
| --update | -U | "allow" or "suppress" automatic updates to labels and descriptions | "suppress" |
| --endpoint | -E | a Wikibase SPARQL endpoint URL | "https://query.wikidata.org/sparql" |
| --version | -V | no values; displays current version information |  |
| --help | -H | no values; displays link to this page |  |

**Examples:**

----

```
python vanderbot.py --json project-metadata.json --log ../log.txt
```

Metadata description file is called `project-metadata.json` and is in the current working directory. Progress and error logs saved to the file `log.txt` in the parent directory.

----

```
python vanderbot.py -P working -C wikidata-credentials.txt
```

Credentials file called `wikidata-credentials.txt` is in the current working directory. Logging will be to the standard output console.

----

```
python vanderbot.py --update allow -L update.log
```

Progress and error logs saved to the file `update.log` in the current working directory. Labels and descriptions of existing items in Wikidata are automatically replaced with local values if they differ.

## Q identifiers

When stored in the CSV, Q identifiers ("Q IDs") for items MUST be written with the leading `Q` but without any namespace prefix. Example: `Q42`.

## Value nodes

Generally, CSV column names are flexible and can be whatever is specified in the metadata description JSON file. However, VanderBot REQUIRES several suffixes for complex values that require more than one column to describe (value nodes). The following table lists the supported value nodes and the REQUIRED suffixes.

| type | component | suffix | example | datatype |
| ---- | --------- | ------ | ------- | -------- |
| time | timestamp | _val | startDate_val | ISO 8601-like dateTime timestamp<sup>*</sup> |
|      | precision | _prec | startDate_prec | integer |
| quantity | amount | _val | height_val | decimal |
|          | unit | _unit | height_unit | Q ID IRI |
| globecoordinate | latitude | _val | location_val | decimal degrees |
|                 | longitude | _long | location_long | decimal degrees |
|                 | precision | _prec | location_prec | decimal degrees |

<sup>*</sup> The value required by the API differs slightly from ISO 8601, particularly in requiring a leading `+`. However, to allow the schemas to be used to generate valied ISO 8601 dateTimes, values in the CSV MUST omit the leading `+`, which is added by the script when values are sent to the API. VanderBot will also convert dates in certain formats to what is required by the API. See below for details.

Each value node also includes a column with a `_nodeId` suffix (e.g. `startDate_nodeId`) that contains an arbitrary unique identifier assigned by the script when the item line is processed.

See the [Wikibase data model](https://www.mediawiki.org/wiki/Wikibase/DataModel#Datatypes_and_their_Values) for more details. Note that VanderBot supports common attributes of these value nodes but assumes defaults for others (such as the Gregorian calendar model for time and earth globe system for globecooordinate). 

## Abbreviated time values

Time values MAY be abbreviated when entered in the CSV. VanderBot will convert times that conform to certain patterns into the format required by the Wikibase model. Here are the acceptable abbreviated formats:

| character pattern | example    | precision | Wikibase precision integer |
| ----------------- | -------    | --------- | -------------------------- |
| YYYY              | 1885       | to year   | 9  |
| YYYY-MM           | 2020-03    | to month  | 10 |
| YYYY-MM-DD        | 2001-09-11 | to day    | 11 |

When these abbreviated values are used in the timestamp (`_val`) column, the precision (`_prec`) column MUST be left empty. The precision column will be filled with the appropriate integer when the date is converted to the required timestamp format.

Time values at lower precisions and BCE dates with negative values MUST be in long form. For example:

```
2020-11-30T00:00:00Z
```
for 30 November 2020

```
-0100-01-01T00:00:00Z
```
for 100 BCE. The dateTime strings MUST end in `T00:00:00Z` regardless of the precision.

----
Revised 2021-06-05
