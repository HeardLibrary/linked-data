{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the last step before completing Phase 2 of the ACT project by uploading with VanderBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2022 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # web-scraping library, use PIP to install beautifulsoup4 (included in Anaconda)\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import re # regex\n",
    "from time import sleep\n",
    "from fuzzywuzzy import fuzz # fuzzy string matching\n",
    "\n",
    "import urllib.parse\n",
    "import urllib.request as urlrequest\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "get_server_sleep = 0.1 # number of seconds to wait before get calls to webserver\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "date_problems_frame = pd.read_csv('issues_with_inception_dates.csv', na_filter=False, dtype = str)\n",
    "date_problems_list = list(date_problems_frame['act'])\n",
    "creator_mismatches_frame = pd.read_csv('creator_name_mismatches.csv', na_filter=False, dtype = str)\n",
    "creator_mismatches_list = list(creator_mismatches_frame['act'])\n",
    "works_frame = pd.read_csv('abstract_artworks_charlotte_edits.csv', na_filter=False, dtype = str)\n",
    "#works_frame = works_frame.head(7).copy()\n",
    "works_frame.head()\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# Read from a CSV file on disk into a list of dictionaries (representing a table)\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        table = []\n",
    "        for row in dict_object:\n",
    "            table.append(row)\n",
    "    return table\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "\n",
    "def convertToRGB(image):\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "commons_prefix = 'http://commons.wikimedia.org/wiki/Special:FilePath/'\n",
    "commons_page_prefix = 'https://commons.wikimedia.org/wiki/File:'\n",
    "def commons_url_to_filename(url):\n",
    "    # form of URL is: http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\n",
    "    string = url.split(commons_prefix)[1] # get local name file part of URL\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_url(filename):\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_prefix + encoded_filename\n",
    "    return url\n",
    "\n",
    "def commons_page_url_to_filename(url):\n",
    "    # form of URL is: https://commons.wikimedia.org/wiki/File:Castle_De_Haar_(1892-1913)_-_360%C2%B0_Panorama_of_Castle_%26_Castle_Grounds.jpg\n",
    "    string = url.split(commons_page_prefix)[1] # get local name file part of URL\n",
    "    string = string.replace('_', ' ')\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_page_url(filename):\n",
    "    filename = filename.replace(' ', '_')\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_page_prefix + encoded_filename\n",
    "    url = url.replace('%28', '(').replace('%29', ')').replace('%2C', ',')\n",
    "    return url\n",
    "\n",
    "\n",
    "def check_commons_page_for_wikidata_image_link(image_filename):\n",
    "    # image_filename = 'Christ sur la mer de Galilée (Delacroix) Walters Art Museum 37.186.jpg'\n",
    "    # image_filename = 'Christ_and_SocratesSAAM_1974.28.341A_B_1.jpg'\n",
    "    page_url = 'https://commons.wikimedia.org/wiki/File:' + image_filename\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    # Create a soup object and find the file info table\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    image_tags = soup.findAll('img', src= 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/20px-Wikidata-logo.svg.png')\n",
    "    if len(image_tags) > 0:\n",
    "        anchor = image_tags[0].parent\n",
    "        link = anchor['href']\n",
    "        qid = extract_localname(link)\n",
    "    else:\n",
    "        qid = ''\n",
    "    sleep(get_server_sleep) # Don't hit the API too fast\n",
    "    return qid\n",
    "\n",
    "def check_commons_page_for_wikidata_link(image_filename):\n",
    "    # image_filename = 'Christ sur la mer de Galilée (Delacroix) Walters Art Museum 37.186.jpg'\n",
    "    # image_filename = 'Christ_and_SocratesSAAM_1974.28.341A_B_1.jpg'\n",
    "    page_url = 'https://commons.wikimedia.org/wiki/File:' + image_filename\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    # Create a soup object and find the file info table\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.findAll('table', class_= re.compile('fileinfotpl'))\n",
    "    if len(tables) > 0:\n",
    "        # Have to check for this span because there are other subtables with a tags besides the one at the top\n",
    "        span = tables[0].findAll('span', id = 'artwork')\n",
    "        if len(span) > 0:\n",
    "            # The link to the Wikidata item will be in an href\n",
    "            # Need to go up to parent, since the anchor is sometimes a sibling tag and not a child tag\n",
    "            anchors = span[0].parent.findAll('a', href = re.compile('https://www.wikidata.org/wiki/'))\n",
    "            if len(anchors) > 0:\n",
    "                try:\n",
    "                    link = anchors[0]['href']\n",
    "                    qid = extract_localname(link)\n",
    "                except:\n",
    "                    qid = ''\n",
    "            else:\n",
    "                qid = ''\n",
    "        else:\n",
    "            qid = ''\n",
    "    else:\n",
    "        qid = ''\n",
    "    sleep(get_server_sleep) # Don't hit the API too fast\n",
    "    return qid\n",
    "\n",
    "# function to get local name from an IRI\n",
    "def extract_localname(iri):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[len(pieces)-1] # return the last piece\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "# NOTE: request_header and endpoint are global variables defined earlier in the script\n",
    "def send_sparql_query(query_string):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    #print('querying SPARQL endpoint to acquire item metadata')\n",
    "    #response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    no_response = True\n",
    "    while no_response:\n",
    "        try:\n",
    "            response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=requestheader)\n",
    "            no_response = False\n",
    "        except:\n",
    "            print('Query service error. Waiting 1 minute.')\n",
    "            sleep(60)\n",
    "        \n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    #print('done retrieving data')\n",
    "    # print(json.dumps(results, indent=2))\n",
    "    \n",
    "    sleep(sparql_sleep) # delay to avoid hitting the Query Service too fast\n",
    "    return results\n",
    "\n",
    "def get_wikidata_item_label(qid):\n",
    "    query_string = '''\n",
    "    select distinct ?label\n",
    "    where {\n",
    "    wd:''' + qid + ''' rdfs:label ?label.\n",
    "    filter(lang(?label)=\"en\")\n",
    "    }'''\n",
    "    #print(query_string)\n",
    "    \n",
    "    results = send_sparql_query(query_string)\n",
    "\n",
    "    #print('done retrieving data')\n",
    "    #print(json.dumps(results, indent=2))\n",
    "    if len(results) > 0:\n",
    "        label = results[0]['label']['value']\n",
    "    else:\n",
    "        label = ''\n",
    "\n",
    "    return label\n",
    "\n",
    "def find_works_by_artists(qid_text_list):\n",
    "    query_string = '''\n",
    "    select distinct ?artist ?work ?workLabel\n",
    "    where {\n",
    "    values ?artist {''' + qid_text_list + '''}\n",
    "    ?work wdt:P170 ?artist.\n",
    "    ?work rdfs:label ?workLabel.\n",
    "    filter(lang(?workLabel)=\"en\")\n",
    "    }'''\n",
    "    #print(query_string)\n",
    "    \n",
    "    results = send_sparql_query(query_string)\n",
    "\n",
    "    #print('done retrieving data')\n",
    "    #print(json.dumps(results, indent=2))\n",
    "    if len(results) > 0:\n",
    "        artworks = []\n",
    "        for result in results:\n",
    "            artist_qid = extract_localname(result['artist']['value'])\n",
    "            qid = extract_localname(result['work']['value'])\n",
    "            label = result['workLabel']['value']\n",
    "            artworks.append({'artist': artist_qid, 'work': qid, 'label': label})\n",
    "    else:\n",
    "        artworks = []\n",
    "\n",
    "    return artworks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out rows with Commons page having link to Wikidata\n",
    "\n",
    "There are a bunch of rows, mostly with details, where the link to Wikidata wasn't found. This checks using the new function that looks for the tiny Wikidata flag image to get the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_link_found_output_list = []\n",
    "work_qid_list = []\n",
    "work_label_list = []\n",
    "remaing_works_list = []\n",
    "for work_index, work_row in works_frame.iterrows():\n",
    "    print(work_row['act'])\n",
    "    \n",
    "    qid = check_commons_page_for_wikidata_link(work_row['image'])\n",
    "    #print(qid)\n",
    "\n",
    "    if qid != '':\n",
    "        wikidata_link_found_output_list.append(work_row)\n",
    "        work_qid_list.append(qid)\n",
    "        label = get_wikidata_item_label(qid)\n",
    "        work_label_list.append(label)\n",
    "        #print(label)\n",
    "    else:\n",
    "        remaing_works_list.append(work_row)\n",
    "    #print()\n",
    "\n",
    "remaing_works_frame = pd.DataFrame(remaing_works_list)\n",
    "\n",
    "wikidata_link_found_output_frame = pd.DataFrame(wikidata_link_found_output_list)\n",
    "\n",
    "# Stick the lists of found work Q IDs and labels onto the end of the DataFrame before saving\n",
    "wikidata_link_found_output_frame['work_qid'] = work_qid_list\n",
    "wikidata_link_found_output_frame['work_label'] = work_label_list\n",
    "\n",
    "wikidata_link_found_output_frame.to_csv('wikidata_link_found.csv')\n",
    "wikidata_link_found_output_frame.head()\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out rows with date problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_problems_output_list = []\n",
    "remaing_works_list = []\n",
    "for work_index, work_row in remaing_works_frame.iterrows():\n",
    "    #print(work_row['act'])\n",
    "    \n",
    "    if work_row['act'] in date_problems_list: # only try to match if it's on the problem list\n",
    "        for date_index, date_row in date_problems_frame.iterrows():\n",
    "            if date_row['act'] == work_row['act']:\n",
    "                date_problems_output_list.append(work_row)\n",
    "    else:\n",
    "        remaing_works_list.append(work_row)\n",
    "\n",
    "remaing_works_frame = pd.DataFrame(remaing_works_list)\n",
    "\n",
    "date_problems_output_frame = pd.DataFrame(date_problems_output_list)\n",
    "date_problems_output_frame.to_csv('date_problems.csv')\n",
    "date_problems_output_frame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out rows with missing creator values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_creators_output_list = []\n",
    "remaing_works_list = []\n",
    "for work_index, work_row in remaing_works_frame.iterrows():\n",
    "    #print(work_row['act'])\n",
    "    \n",
    "    if work_row['creator'] == '':\n",
    "        missing_creators_output_list.append(work_row)\n",
    "    else:\n",
    "        remaing_works_list.append(work_row)\n",
    "\n",
    "remaing_works_frame = pd.DataFrame(remaing_works_list)\n",
    "\n",
    "missing_creators_output_frame = pd.DataFrame(missing_creators_output_list)\n",
    "missing_creators_output_frame.to_csv('missing_creators.csv')\n",
    "missing_creators_output_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out rows with creator name mismatches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator_mismatch_output_list = []\n",
    "remaing_works_list = []\n",
    "for work_index, work_row in remaing_works_frame.iterrows():\n",
    "    #print(work_row['act'])\n",
    "    \n",
    "    if work_row['act'] in creator_mismatches_list: # only try to match if it's on the problem list\n",
    "        for date_index, date_row in creator_mismatches_frame.iterrows():\n",
    "            if date_row['act'] == work_row['act']:\n",
    "                creator_mismatch_output_list.append(work_row)\n",
    "    else:\n",
    "        remaing_works_list.append(work_row)\n",
    "\n",
    "remaing_works_frame = pd.DataFrame(remaing_works_list)\n",
    "\n",
    "creator_mismatch_output_frame = pd.DataFrame(creator_mismatch_output_list)\n",
    "creator_mismatch_output_frame.to_csv('creator_mismatch_problems.csv')\n",
    "creator_mismatch_output_frame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the works that remain after screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaing_works_frame.to_csv('works_to_write.csv')\n",
    "remaing_works_frame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean labels\n",
    "\n",
    "Remove problematic characters in the labels, check that they aren't too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_characters = '\\\\'\n",
    "\n",
    "works_frame = pd.read_csv('abstract_artworks.csv', na_filter=False, dtype = str)\n",
    "#works_frame = pd.read_csv('works_to_write.csv', na_filter=False, dtype = str)\n",
    "#works_frame = works_frame.head(9).copy()\n",
    "for work_index, work_row in works_frame.iterrows():\n",
    "    print(work_row['act'])\n",
    "    \n",
    "    # Note: since work_row is basically a slice of the DataFrame, the change is made to the original DataFrame\n",
    "    # Change tabs to spaces\n",
    "    work_row['label_en'] = work_row['label_en'].replace('\\t', ' ')\n",
    "        \n",
    "    # Remove problematic characters\n",
    "    for problematic_character in problematic_characters:\n",
    "        if problematic_character in work_row['label_en']:\n",
    "            work_row['label_en'] = work_row['label_en'].replace(problematic_character, '')\n",
    "    \n",
    "    # Keep removing double spaces from the label until there aren't any more (works for 3x or more spaces)\n",
    "    while '  ' in work_row['label_en']:\n",
    "        work_row['label_en'] = work_row['label_en'].replace('  ', ' ')\n",
    "\n",
    "    work_row['label_en'] = work_row['label_en'].strip()\n",
    "\n",
    "    # Repeat for descriptions\n",
    "    # Change tabs to spaces\n",
    "    work_row['description_en'] = work_row['description_en'].replace('\\t', ' ')\n",
    "\n",
    "    for problematic_character in problematic_characters:\n",
    "        if problematic_character in work_row['description_en']:\n",
    "            work_row['description_en'] = work_row['description_en'].replace(problematic_character, '')\n",
    "            \n",
    "    while '  ' in work_row['description_en']:\n",
    "        work_row['description_en'] = work_row['description_en'].replace('  ', ' ')\n",
    "        \n",
    "    work_row['description_en'] = work_row['description_en'].strip()\n",
    "        \n",
    "    # Copy the label to the title_en column\n",
    "    # NOTE: a few titles aren't in English, but they can be removed manually\n",
    "    # I copied over the image reference values since every items has them\n",
    "    work_row['title'] = work_row['label_en']\n",
    "    work_row['title_ref1_referenceUrl'] = work_row['image_ref1_referenceUrl']\n",
    "    work_row['title_ref1_retrieved_val'] = work_row['image_ref1_retrieved_val']\n",
    "        \n",
    "    # Replace double quotes in labels with single quotes since VanderBot has problems with them.\n",
    "    if '\"' in work_row['label_en']:\n",
    "        work_row['label_en'] = work_row['label_en'].replace('\"', \"'\")\n",
    "        \n",
    "    # Limit the length of the label to 250 characters (the maximum)\n",
    "    if len(work_row['label_en']) > 250:\n",
    "        work_row['label_en'] = work_row['label_en'][:250]\n",
    "        \n",
    "works_frame.to_csv('works_to_write_out.csv', index = False)\n",
    "works_frame.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy match the titles against works in Wikidata\n",
    "\n",
    "There are too many artworks (especially paintings) by famous artists that are already in Wikidata, but not yet matched.\n",
    "\n",
    "This first cell retrieves from the WD QS all works for all artists in ACT whose works will potentially be written. (There are about 35 000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_frame = pd.read_csv('works_to_write.csv', na_filter=False, dtype = str)\n",
    "artists_list = list(works_frame['creator'])\n",
    "#artists_list.remove('anon') # get rid of \"anon\" values\n",
    "artists_list = list(set(artists_list)) # get rid of redundant values\n",
    "artists_string = 'wd:' + '\\nwd:'.join(artists_list) # prepend \"wd:\" and concatenate with each Q ID on its own line\n",
    "works = find_works_by_artists(artists_string)\n",
    "write_dicts_to_csv(works, 'test_titles.csv', ['artist', 'work', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform fuzzy matching of our title strings against the labels for that artist in Wikidata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_frame = pd.read_csv('works_to_write.csv', na_filter=False, dtype = str)\n",
    "#works_frame = works_frame.head(5).copy()\n",
    "works = read_dicts_from_csv('test_titles.csv')\n",
    "\n",
    "possible_match_list = []\n",
    "for work_index, work_row in works_frame.iterrows():\n",
    "    #print(work_row['label_en'])\n",
    "    artist_qid = work_row['creator']\n",
    "    if artist_qid != 'anon':\n",
    "        work_name = work_row['label_en']\n",
    "        for work in works:\n",
    "            if artist_qid == work['artist']:\n",
    "                test_name = work['label']\n",
    "                ratio = fuzz.ratio(test_name, work_name)\n",
    "                partial_ratio = fuzz.partial_ratio(test_name, work_name)\n",
    "                sort_ratio = fuzz.token_sort_ratio(test_name, work_name)\n",
    "                set_ratio = fuzz.token_set_ratio(test_name, work_name)\n",
    "                w_ratio = fuzz.WRatio(test_name, work_name)\n",
    "                if ratio > 60:\n",
    "                    print('artist:', artist_qid)\n",
    "                    print('act work:', work_name)\n",
    "                    print(work['work'], test_name)\n",
    "                    print('name similarity ratio', ratio)\n",
    "                    print('partial ratio', partial_ratio)\n",
    "                    print('sort_ratio', sort_ratio)\n",
    "                    print('set_ratio', set_ratio)\n",
    "                    print('w_ratio', w_ratio)\n",
    "                    print()\n",
    "                    possible_match_list.append({'score': ratio, 'artist': artist_qid, 'act': work_row['act'], 'act_title': work_name, 'match_qid': work['work'], 'match_label': test_name})\n",
    "write_dicts_to_csv(possible_match_list, 'possible_matches.csv', ['score', 'artist', 'act', 'act_title', 'match_qid', 'match_label'])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove possible matches and \"details\" from the list of works to write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_frame = pd.read_csv('works_to_write.csv', na_filter=False, dtype = str)\n",
    "\n",
    "possible_matches_frame = pd.read_csv('possible_matches.csv', na_filter=False, dtype = str)\n",
    "possible_matches_list = list(set(possible_matches_frame['act'])) # Create non-redundant list from act column\n",
    "\n",
    "# Create new dataframe for rows where the act value is NOT in the possible matches list\n",
    "remaining_works = works_frame[~works_frame['act'].isin(possible_matches_list)]\n",
    "\n",
    "# Screen out works that contain \"Detail\" or \"detail\" in their labels (case insensitive)\n",
    "details_frame = remaining_works[remaining_works['label_en'].str.contains('detail', case=False)]\n",
    "details_frame.to_csv('works_that_are_details.csv', index = False)\n",
    "\n",
    "out_frame = remaining_works[~remaining_works['label_en'].str.contains('detail', case=False)] # tilde for NOT\n",
    "\n",
    "out_frame.to_csv('test_works_to_write.csv', index = False)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script to screen missing artist works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_artists_work_frame = pd.read_csv('missing_creators.csv', na_filter=False, dtype = str)\n",
    "#missing_artists_work_frame = missing_artists_work_frame.head(6).copy()\n",
    "\n",
    "for work_index, work_row in missing_artists_work_frame.iterrows():\n",
    "    print(work_row['image'])\n",
    "    \n",
    "    page_url = filename_to_commons_page_url(work_row['image'])\n",
    "    commons_url = filename_to_commons_url(work_row['image'])\n",
    "    bookmark = page_url + '#mw-imagepage-content'\n",
    "    \n",
    "    #get image by url and load into cv2\n",
    "    resp = urlrequest.urlopen(commons_url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "    print(bookmark)\n",
    "    description = work_row['description_en']\n",
    "    print(description)\n",
    "    pieces = description.split(' by ')\n",
    "    \n",
    "    choice = input('')\n",
    "    # Leave the way it is\n",
    "    if choice == '':\n",
    "        pass\n",
    "    # Give only the genre\n",
    "    elif choice == '0':\n",
    "        work_row['description_en'] = pieces[0]\n",
    "    # Leave the description, but add anonymous as creator\n",
    "    elif choice == '1':\n",
    "        work_row['creator'] = '_:'\n",
    "        work_row['creator_object_has_role'] = 'Q4233718'\n",
    "    # Try to insert artist unknown and leave photo by, add anonymous as creator\n",
    "    elif choice == '2':\n",
    "        if ',' in pieces[0]:\n",
    "            prefix = pieces[0].split(',')[0]\n",
    "        elif ';' in pieces[0]:\n",
    "            prefix = pieces[0].split(';')[0]\n",
    "        else:\n",
    "            prefix = pieces[0]\n",
    "        work_row['description_en'] = prefix + ' by artist unknown, photo by ' + pieces[1]\n",
    "        work_row['creator'] = '_:'\n",
    "        work_row['creator_object_has_role'] = 'Q4233718'\n",
    "    # Insert that it's the photographer\n",
    "    elif choice == '3':\n",
    "        work_row['description_en'] = pieces[0] + ', photo by ' + pieces[1]\n",
    "    # Insert that it's the photographer with artist unknown, add anonymous as creator\n",
    "    elif choice == '4':\n",
    "        work_row['description_en'] = pieces[0] + ' by artist unknown, photo by ' + pieces[1]\n",
    "        work_row['creator'] = '_:'\n",
    "        work_row['creator_object_has_role'] = 'Q4233718'\n",
    "    # Give only the genre, but also add artist unknown and anonymous creator\n",
    "    elif choice == '5':\n",
    "        work_row['description_en'] = pieces[0] + ' by artist unknown'\n",
    "        work_row['creator'] = '_:'\n",
    "        work_row['creator_object_has_role'] = 'Q4233718'\n",
    "    # If something else was typed in, use it as the description\n",
    "    else:\n",
    "        work_row['description_en'] = choice\n",
    "    print()\n",
    "    \n",
    "    missing_artists_work_frame.to_csv('test.csv', index = False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
