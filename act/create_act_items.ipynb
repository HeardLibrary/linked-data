{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to create Wikidata items for ACT artwork\n",
    "\n",
    "Initially, this will be for works that are already in Commons, but for which the links to Commons are from non-artwork Wikidata items. Eventually, we can modify this for any work that's in Commons and doesn't have a Wikidata item.\n",
    "\n",
    "## Properties whose values need to be cleaned/generated\n",
    "\n",
    "Taken from [here](https://github.com/HeardLibrary/vandycite/blob/master/act/processed_lists/candidate_properties_to_write.csv).\n",
    "\n",
    "P571 (inception): get from ACT \"DateCreation\" and Commons \"date\" fields\n",
    "\n",
    "## Configuration section\n",
    "\n",
    "Run once at the start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2022 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# Import modules\n",
    "import json\n",
    "import csv\n",
    "import math\n",
    "import datetime\n",
    "from time import sleep\n",
    "import requests\n",
    "import re # regex\n",
    "# Pandas for data frame management\n",
    "import pandas as pd\n",
    "# Fuzzy string matching\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "\n",
    "accept_media_type = 'application/json'\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "user_agent_header = 'act_disambiguation/0.1 (mailto:steve.baskauf@vanderbilt.edu)'\n",
    "sparql_sleep = 0.1\n",
    "default_language = 'en'\n",
    "output_directory = 'create_missing_artwork_items/'\n",
    "\n",
    "# Load data from the two sources (ACT database dump and Commons Mediawiki table scrape)\n",
    "act_data = pd.read_csv('act_data_fix.csv', na_filter=False, dtype = str)\n",
    "commons_data = pd.read_csv('commons_data_fix.csv', na_filter=False, dtype = str)\n",
    "ids = pd.read_csv('clean_ids.csv', na_filter=False, dtype = str)\n",
    "country_mappings = pd.read_csv('country_mappings.csv', na_filter=False, dtype = str)\n",
    "collections_mappings = pd.read_csv('collections.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# For testing purposes, just use the first few rows\n",
    "test_rows = 10\n",
    "#act_data = act_data.head(test_rows).copy()\n",
    "#commons_data = commons_data.head(test_rows).copy()\n",
    "\n",
    "# --------------------\n",
    "# Low-level functions\n",
    "# --------------------\n",
    "\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "today = generate_utc_date()\n",
    "\n",
    "# Load JSON file data from local drive into a Python data structure\n",
    "def load_json_into_data_struct(path):\n",
    "    with open(path, 'rt', encoding='utf-8') as file_object:\n",
    "        file_text = file_object.read()\n",
    "    structure = json.loads(file_text)\n",
    "    # uncomment the following line to view the data\n",
    "    # print(json.loads(structure, indent = 2))\n",
    "    return(structure)\n",
    "\n",
    "# screens.json is a configuration file that defines the kinds of screens to be performed on potential Q ID matches from Wikidata\n",
    "screens = load_json_into_data_struct('screens.json')\n",
    "\n",
    "# Read from a CSV file on disk into a list of dictionaries (representing a table)\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        table = []\n",
    "        for row in dict_object:\n",
    "            table.append(row)\n",
    "    return table\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def pad_zeros_left(date_string):\n",
    "    length = len(date_string)\n",
    "    pad = 4-length\n",
    "    return '0' * pad + date_string\n",
    "\n",
    "def generate_date_string(date, bce):\n",
    "    if bce:\n",
    "        date_string = '-'\n",
    "    else:\n",
    "        date_string = ''\n",
    "    date_string += pad_zeros_left(str(date)) + '-01-01T00:00:00Z'\n",
    "    return date_string\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# NOTE: there are still some issues that have not been worked out with quotation marks in query strings.\n",
    "# Still working on this; see also the send_sparql_query() below.\n",
    "def generate_sparql_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "#        'Content-Type': 'application/sparql-query',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "sparql_request_header = generate_sparql_header_dictionary(accept_media_type, user_agent_header)\n",
    "\n",
    "# ----------------------------\n",
    "# Intermediate-level functions\n",
    "# ----------------------------\n",
    "\n",
    "# Parse the ACT date string into structured components\n",
    "def process_act_date(act_date):\n",
    "    act_circa = False\n",
    "    act_range = False\n",
    "    act_century = False\n",
    "    non_numeric = False\n",
    "    date = 0\n",
    "    start_date = 0\n",
    "    end_date = 0\n",
    "    \n",
    "    # If there is no date from ACT, kill the function and return False\n",
    "    if act_date == '':\n",
    "        return False, date, act_range, start_date, end_date, act_century, act_circa\n",
    "    \n",
    "    # Determine circa status of ACT date\n",
    "    if 'ca.' in act_date:\n",
    "        act_circa = True\n",
    "        # Remove the \"ca.\" from the beginning and clean whitespace\n",
    "        act_date = act_date.split('ca.')[1].strip()\n",
    "    \n",
    "    # Test whether the ACT date is a number\n",
    "    try:\n",
    "        date = int(act_date)\n",
    "        #print('numeric date:', date)\n",
    "    except:\n",
    "        non_numeric = True\n",
    "        #print('non-numeric string:', act_date)\n",
    "        \n",
    "    if non_numeric:\n",
    "        # Determine century status of ACT date\n",
    "        if 'century' in act_date: # single century date\n",
    "            act_century = True\n",
    "            # Remove the \"century\" and \"th\", \"rd\", \"st\", etc. from the end\n",
    "            act_date = act_date[:-10]\n",
    "            non_numeric = False\n",
    "            try:\n",
    "                date = int(act_date) * 100 - 50 # set the date at mid-century\n",
    "            except:\n",
    "                print('numeric conversion error on', act_date)\n",
    "        elif 'centuries' in act_date:\n",
    "            act_century = True\n",
    "            act_range = True\n",
    "            # Remove the \"centuries\" and \"th\", \"rd\", \"st\", etc. from the end\n",
    "            act_date = act_date[:-10].strip()\n",
    "            try:\n",
    "                pieces = act_date.split('-')\n",
    "                start_date = int(pieces[0][:-2]) * 100 - 50 # set the date at mid-century\n",
    "                end_date = int(pieces[1][:-2]) * 100 - 50 # set the date at mid-century\n",
    "            except:\n",
    "                print('error in processing century range')\n",
    "    # Process date ranges (non-numeric because they include \"-\")\n",
    "    if non_numeric and not act_century:\n",
    "        #print(act_date)\n",
    "        try:\n",
    "            pieces = act_date.split('-')\n",
    "            start_date = int(pieces[0])\n",
    "            end_date = int(pieces[1])\n",
    "            act_range = True\n",
    "        except:\n",
    "            print('error in processing date range')\n",
    "\n",
    "        \n",
    "    # if there is a range of dates, set the single date as the midpoint\n",
    "    if start_date != 0 or end_date != 0:\n",
    "        date = math.floor((start_date + end_date)/2)\n",
    "            \n",
    "    return True, date, act_range, start_date, end_date, act_century, act_circa\n",
    "    \n",
    "# Disassemble Wikibase-style dateTime strings into year, precision, and BCE components\n",
    "def extract_from_iso_date_string(string):\n",
    "    pieces = string.split('/')\n",
    "    # precision comes after the slash in the Wikibase format\n",
    "    precision = pieces[1]\n",
    "    # check for negative sign for BCE dates\n",
    "    if pieces[0][0] == '-':\n",
    "        bce = True\n",
    "    else:\n",
    "        bce = False\n",
    "    no_sign_dateTime = pieces[0][1:] # skip sign\n",
    "    pieces = no_sign_dateTime.split('-')\n",
    "    year = pieces[0]\n",
    "    return int(year), precision, bce\n",
    "\n",
    "# Parse any structured date data that was scraped from the Commons Mediawiki table\n",
    "def process_commons_date(commons_date_string):\n",
    "    # Set all values to defaults to return something even if they aren't determined from the data\n",
    "    commons_circa = False\n",
    "    commons_range = False\n",
    "    date = 0\n",
    "    precision = '9'\n",
    "    bce = False\n",
    "    start_date = 0\n",
    "    start_precision = '9'\n",
    "    start_bce = False\n",
    "    end_date = 0\n",
    "    end_precision = '9'\n",
    "    end_bce = False\n",
    "\n",
    "    commons_date_list = json.loads(commons_date_string)\n",
    "    found = False\n",
    "    for string in commons_date_list:\n",
    "        # Find the part of the extracted metadata that includes the structured data\n",
    "        if 'date QS' in string:\n",
    "            found = True\n",
    "            pieces = string.split(',') # split into fields by comma\n",
    "            pieces = pieces[1:] # get rid of initial \"inception field\"\n",
    "            date, precision, bce = extract_from_iso_date_string(pieces[0])\n",
    "            # Remove the initial date from the list\n",
    "            pieces = pieces[1:]\n",
    "            \n",
    "            # Check for circa\n",
    "            if len(pieces) >= 2:\n",
    "                # Check if last piece is \"circa\"\n",
    "                if pieces[len(pieces)-1] == 'Q5727902':\n",
    "                    commons_circa = True\n",
    "                    # Remove the last two items from the list\n",
    "                    pieces = pieces[:-2]\n",
    "            #print(commons_circa, pieces)\n",
    "            \n",
    "            # Extract start date (if any)\n",
    "            if len(pieces) > 0 and (pieces[0] == 'P1319' or pieces[0] == 'P580'): # check for earliest date or start time\n",
    "                commons_range = True\n",
    "                start_date, start_precision, start_bce = extract_from_iso_date_string(pieces[1]) # start date follows the P ID\n",
    "                # Remove the first two pieces\n",
    "                if len(pieces) > 0:\n",
    "                    pieces = pieces[2:]\n",
    "            \n",
    "            if len(pieces) > 0 and (pieces[0] == 'P1326' or pieces[0] == 'P582'): # check for latest date or end time\n",
    "                commons_range = True\n",
    "                end_date, end_precision, end_bce = extract_from_iso_date_string(pieces[1]) # start date follows the P ID\n",
    "    return found, date, precision, bce, commons_range, start_date, start_precision, start_bce, end_date, end_precision, end_bce, commons_circa\n",
    "    \n",
    "def retrieve_date_wikidata(qid, pid):\n",
    "    query_string = '''\n",
    "select distinct ?object where {\n",
    "    wd:'''+ qid + ''' wdt:''' + pid + ''' ?object.\n",
    "    }'''\n",
    "    #print(query_string)\n",
    "    r = requests.post(endpoint, data=dict(query=query_string), headers=sparql_request_header)\n",
    "    no_response = True\n",
    "    while no_response:\n",
    "        try:\n",
    "            r = requests.post(endpoint, data=dict(query=query_string), headers=sparql_request_header)\n",
    "            no_response = False\n",
    "        except:\n",
    "            print('Query service error. Waiting 1 minute.')\n",
    "            sleep(60)\n",
    "\n",
    "    \n",
    "    results_list = []\n",
    "    try:\n",
    "        data = r.json()\n",
    "        #print(data)\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                result_value = statement['object']['value']\n",
    "                results_list.append(result_value)\n",
    "    except:\n",
    "        results_list = [r.text]\n",
    "\n",
    "    # delay by some amount to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results_list\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "# NOTE: request_header and endpoint are global variables defined earlier in the script\n",
    "def send_sparql_query(query_string):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    #print('querying SPARQL endpoint to acquire item metadata')\n",
    "    #response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    no_response = True\n",
    "    while no_response:\n",
    "        try:\n",
    "            response = requests.post(endpoint, data=dict(query=query_string), headers=sparql_request_header)\n",
    "            no_response = False\n",
    "        except:\n",
    "            print('Query service error. Waiting 1 minute.')\n",
    "            sleep(60)\n",
    "        \n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    #print('done retrieving data')\n",
    "    # print(json.dumps(results, indent=2))\n",
    "    \n",
    "    sleep(sparql_sleep) # delay to avoid hitting the Query Service too fast\n",
    "    return results\n",
    "\n",
    "def generate_name_alternatives(name_list):\n",
    "    alternatives = []\n",
    "\n",
    "    # This is a hack of the previous script to allow for checking more than one name alternative\n",
    "    for name in name_list:\n",
    "        # treat commas as if they were spaces\n",
    "        name = name.replace(',', ' ')\n",
    "        # get rid of periods, sometimes periods are close up with no spaces\n",
    "        name = name.replace('.', ' ')\n",
    "\n",
    "        pieces = name.split(' ')\n",
    "        while '' in pieces:\n",
    "            pieces.remove('')\n",
    "\n",
    "        # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "        if pieces[len(pieces)-1] == 'Jr':\n",
    "            pieces = pieces[0:len(pieces)-1]\n",
    "            suffix = ', Jr.'\n",
    "        elif pieces[len(pieces)-1] == 'II':\n",
    "            pieces = pieces[0:len(pieces)-1]\n",
    "            suffix = ' II'\n",
    "        elif pieces[len(pieces)-1] == 'III':\n",
    "            pieces = pieces[0:len(pieces)-1]\n",
    "            suffix = ' III'\n",
    "        elif pieces[len(pieces)-1] == 'IV':\n",
    "            pieces = pieces[0:len(pieces)-1]\n",
    "            suffix = ' IV'\n",
    "        elif pieces[len(pieces)-1] == 'V':\n",
    "            pieces = pieces[0:len(pieces)-1]\n",
    "            suffix = ' V'\n",
    "        elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "            pieces = pieces[0:len(pieces)-2]\n",
    "            suffix = ' the elder'\n",
    "        else:\n",
    "            suffix = ''\n",
    "\n",
    "        # generate initials for all names\n",
    "        initials = []\n",
    "        for piece in pieces:\n",
    "            # make sure first character is alphabetic\n",
    "            # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "            # typical cases are like (Kit) or \"Kit\"\n",
    "            if not piece[0:1].isalpha():\n",
    "                piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "            if len(piece) > 0:\n",
    "                initials.append(piece[0:1])\n",
    "\n",
    "        # full name\n",
    "        name_version = ''\n",
    "        for piece_number in range(0, len(pieces)-1):\n",
    "            name_version += pieces[piece_number] + ' '\n",
    "        name_version += pieces[len(pieces)-1]\n",
    "        alternatives.append(name_version)\n",
    "\n",
    "        # full name with suffix\n",
    "        if suffix != '':\n",
    "            name_version = ''\n",
    "            for piece_number in range(0, len(pieces)-1):\n",
    "                name_version += pieces[piece_number] + ' '\n",
    "            name_version += pieces[len(pieces)-1] + suffix\n",
    "            alternatives.append(name_version)\n",
    "\n",
    "        # first and last name with initials\n",
    "        name_version = pieces[0] + ' '\n",
    "        for piece_number in range(1, len(initials)-1):\n",
    "            name_version += initials[piece_number] + ' '\n",
    "        name_version += pieces[len(pieces)-1]\n",
    "        alternatives.append(name_version)\n",
    "\n",
    "        # first and last name with initials and periods\n",
    "        name_version = pieces[0] + ' '\n",
    "        for piece_number in range(1, len(initials)-1):\n",
    "            name_version += initials[piece_number] + '. '\n",
    "        name_version += pieces[len(pieces)-1]\n",
    "        alternatives.append(name_version)\n",
    "\n",
    "        # first and last name only\n",
    "        name_version = pieces[0] + ' '\n",
    "        name_version += pieces[len(pieces)-1]\n",
    "        alternatives.append(name_version)\n",
    "\n",
    "        # first initial and last name only\n",
    "        name_version = initials[0] + ' '\n",
    "        name_version += pieces[len(pieces)-1]\n",
    "        alternatives.append(name_version)\n",
    "\n",
    "        # first initial with period and last name only\n",
    "        name_version = initials[0] + '. '\n",
    "        name_version += pieces[len(pieces)-1]\n",
    "        alternatives.append(name_version)\n",
    "\n",
    "        # all name initials with last name\n",
    "        name_version = initials[0] + ' '\n",
    "        for piece_number in range(1, len(initials)-1):\n",
    "            name_version += initials[piece_number] + ' '\n",
    "        name_version += pieces[len(pieces)-1]\n",
    "        alternatives.append(name_version)\n",
    "\n",
    "        # all name initials with periods with last name\n",
    "        name_version = ''\n",
    "        for piece_number in range(0, len(initials)-1):\n",
    "            name_version += initials[piece_number] + '. '\n",
    "        name_version += pieces[len(pieces)-1]\n",
    "        alternatives.append(name_version)\n",
    "\n",
    "        # all name initials concatenated with last name\n",
    "        name_version = ''\n",
    "        for piece_number in range(0, len(initials)-1):\n",
    "            name_version += initials[piece_number]\n",
    "        name_version += ' ' + pieces[len(pieces)-1]\n",
    "        alternatives.append(name_version)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def search_name_at_wikidata(name_list):\n",
    "    # carry out search for most languages that use Latin characters, plus some other commonly used languages\n",
    "    # See https://doi.org/10.1145/3233391.3233965\n",
    "    language_codes = [\n",
    "        'en',\n",
    "        'es',\n",
    "        'pt',\n",
    "        'fr',\n",
    "        'it',\n",
    "        'nl',\n",
    "        'de',\n",
    "        'da',\n",
    "        'et',\n",
    "        'hu',\n",
    "        'ga',\n",
    "        'ro',\n",
    "        'sk',\n",
    "        'sl',\n",
    "        'zu',\n",
    "        'tr',\n",
    "        'sv',\n",
    "        'zh',\n",
    "        'ru',\n",
    "        'ja',\n",
    "        'ar',\n",
    "        'pl',\n",
    "        'uk',\n",
    "        'ca',\n",
    "        'cs',\n",
    "        'la',\n",
    "        'nb',\n",
    "        'hu',\n",
    "        'he',\n",
    "        'eo',\n",
    "        'fi',\n",
    "        'ko'\n",
    "      ]\n",
    "    name_list = generate_name_alternatives(name_list)\n",
    "    alternatives = ''\n",
    "    for language_code in language_codes:\n",
    "        for alternative in name_list:\n",
    "            # get rid of quotes, which will break the query\n",
    "            alternative = alternative.replace('\"', '')\n",
    "            alternative = alternative.replace(\"'\", '')\n",
    "            alternatives += '\"' + alternative + '\"@' + language_code + '\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "#    r = requests.post(endpoint, data=query.encode('utf-8'), headers=sparql_request_header)\n",
    "    no_response = True\n",
    "    while no_response:\n",
    "        try:\n",
    "            r = requests.post(endpoint, data=dict(query=query), headers=sparql_request_header)\n",
    "            no_response = False\n",
    "        except:\n",
    "            print('Query service error. Waiting 1 minute.')\n",
    "            sleep(60)\n",
    "\n",
    "    try:\n",
    "        data = r.json()\n",
    "        #print(data)\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidata_iri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qnumber = extract_local_name(wikidata_iri)\n",
    "            results.append({'qid': qnumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def screen_qids(qids, screens):\n",
    "    qid_values =''\n",
    "    for qid in qids:\n",
    "        qid_values += 'wd:' + qid + '\\n'\n",
    "\n",
    "    graph_pattern = ''\n",
    "    first_screen = True\n",
    "    for screen in screens:\n",
    "        # Each requirement in a screen has an AND relationship (all must be satisfied)\n",
    "        subgraph_pattern = ''\n",
    "        for requirement in screen:\n",
    "\n",
    "            # Set the value if required or use a dummy variable if any value is allowed\n",
    "            if requirement['entity'] == '':\n",
    "                value = '?var' + requirement['property'] # add the property string to the variable to guarantee uniqueness\n",
    "            elif re.match(r'Q\\d+', requirement['entity']): # regex to match Q IDs\n",
    "                value = 'wd:' + requirement['entity']\n",
    "            else: # if not nothing or a Q ID, assume it's a string literal\n",
    "                if requirement['lang'] == '':\n",
    "                    value = '\"' + requirement['entity'] + '\"'\n",
    "                else:\n",
    "                    value = '\"' + requirement['entity'] + '\"@' + requirement['lang']\n",
    "\n",
    "            # Set the property (label, description, or P value)\n",
    "            if requirement['property'] == 'label':\n",
    "                property = 'rdfs:label'\n",
    "            elif requirement['property'] == 'description':\n",
    "                property = 'schema:description'\n",
    "            else:\n",
    "                property = 'wdt:' + requirement['property']\n",
    "\n",
    "            # Place the value in either the subject or object position in the triple\n",
    "            if requirement['position'] == 'object':\n",
    "                triple_pattern = '?qid ' + property + ' ' + value + '.'\n",
    "            else:\n",
    "                triple_pattern = value + ' ' + property + ' ?qid.'\n",
    "\n",
    "            # Add filters if needed\n",
    "            if requirement['filter_type'] == '<' or requirement['filter_type'] == '>': \n",
    "                # note: string comparison only e.g. for datetimes, needs modification for actual numbers\n",
    "                triple_pattern += '\\nFILTER (STR(?var' + requirement['property'] + ') ' + requirement['filter_type'] + ' \"' + requirement['filter_string'] + '\")'\n",
    "\n",
    "            if requirement['filter_type'] == 'in': \n",
    "                # note: string comparison only\n",
    "                triple_pattern += '\\nFILTER (CONTAINS(?var' + requirement['property'] + ', \"' + requirement['filter_string'] + '\"))'\n",
    "\n",
    "            # Use MINUS if you want to exclude items that fit the pattern.\n",
    "            if requirement['require'] == 'exclude':\n",
    "                triple_pattern = 'minus {' + triple_pattern + '}'\n",
    "\n",
    "            triple_pattern += '\\n'\n",
    "            #print(triple_pattern)\n",
    "            subgraph_pattern += triple_pattern\n",
    "\n",
    "        # Now attach the subgraph pattern to any previous subgraph patterns using UNION to great an OR relationship\n",
    "        subgraph_pattern = '{\\n' + subgraph_pattern + '}\\n' # create a subgraph pattern so that several can be UNIONed\n",
    "        if first_screen: # The first subgraph pattern doesn't need the UNION inserted\n",
    "            first_screen = False\n",
    "        else:\n",
    "            graph_pattern = graph_pattern + 'UNION\\n'\n",
    "        graph_pattern += subgraph_pattern \n",
    "\n",
    "    query_string = '''\n",
    "    select distinct ?qid ?label ?description where {\n",
    "      VALUES ?qid\n",
    "      {\n",
    "      ''' + qid_values + '''}\n",
    "    ''' + graph_pattern + '''\n",
    "    \n",
    "    ?qid rdfs:label ?label.\n",
    "    FILTER(lang(?label)=\"'''+ default_language + '''\")\n",
    "    \n",
    "    OPTIONAL {\n",
    "    ?qid schema:description ?description.\n",
    "    FILTER(lang(?description)=\"'''+ default_language + '''\")\n",
    "    }\n",
    "      }\n",
    "    '''\n",
    "    #print(query_string)\n",
    "\n",
    "    results = send_sparql_query(query_string)\n",
    "    #print(json.dumps(results, indent=2))\n",
    "\n",
    "    return_list = []\n",
    "    for result in results:\n",
    "        out_dict = {\n",
    "            'qid': extract_local_name(result['qid']['value']),\n",
    "            'label': result['label']['value']\n",
    "            }\n",
    "        if 'description' in result:\n",
    "            out_dict['description'] = result['description']['value']\n",
    "        else:\n",
    "            out_dict['description'] = ''           \n",
    "        return_list.append(out_dict)\n",
    "    return return_list\n",
    "\n",
    "# -------------------\n",
    "# Top level functions\n",
    "# -------------------\n",
    "    \n",
    "def process_dates(act_id, output_dict, issue_log, act_url, commons_url, act_date_string, commons_date_string):\n",
    "    # Hard code ACT BCE to False, at least until it's determined whether any ACT dates\n",
    "    # are designated as BCE or have negative signs.\n",
    "    act_bce = False\n",
    "    \n",
    "    act_found, act_date, act_range, act_start_date, act_end_date, act_century, act_circa  = process_act_date(act_date_string)\n",
    "    #print(act_date, act_start_date, act_end_date)\n",
    "    \n",
    "    commons_found, commons_date, commons_precision, commons_bce, commons_range, commons_start_date, commons_start_precision, commons_start_bce, commons_end_date, commons_end_precision, commons_end_bce, commons_circa = process_commons_date(commons_date_string)\n",
    "    #print(commons_date, commons_start_date, commons_end_date)\n",
    "    \n",
    "    # Perform quality control and determine output values\n",
    "    if not act_found and not commons_found:\n",
    "        issue_log += act_id + ' | ' + filename + ' | no dates retrieved. Commons data: ' + commons_date_string + '\\n'\n",
    "        output_dict['inception_ref1_referenceUrl'] = ''\n",
    "        output_dict['inception_ref1_retrieved_val'] = ''\n",
    "        output_dict['inception_val'] = ''\n",
    "        output_dict['inception_sourcing_circumstances'] = ''\n",
    "        output_dict['inception_prec'] = ''\n",
    "        output_dict['inception_earliest_date_val'] = ''\n",
    "        output_dict['inception_earliest_date_prec'] = ''\n",
    "        output_dict['inception_latest_date_val'] = ''\n",
    "        output_dict['inception_latest_date_prec'] = ''\n",
    "        \n",
    "    if act_found and not commons_found:\n",
    "        output_dict['inception_ref1_referenceUrl'] = act_url\n",
    "        output_dict['inception_ref1_retrieved_val'] = today\n",
    "        output_dict['inception_val'] = generate_date_string(act_date, act_bce)\n",
    "        if act_circa:\n",
    "            output_dict['inception_sourcing_circumstances'] = 'Q5727902'\n",
    "        else:\n",
    "            output_dict['inception_sourcing_circumstances'] = ''\n",
    "        if act_century:\n",
    "            output_dict['inception_prec'] = '7'\n",
    "        else:\n",
    "            output_dict['inception_prec'] = '9'\n",
    "        if act_range:\n",
    "            output_dict['inception_earliest_date_val'] = generate_date_string(act_start_date, act_bce)\n",
    "            if act_century:\n",
    "                output_dict['inception_earliest_date_prec'] = '7'\n",
    "            else:\n",
    "                output_dict['inception_earliest_date_prec'] = '9'\n",
    "            output_dict['inception_latest_date_val'] = generate_date_string(act_end_date, act_bce)\n",
    "            if act_century:\n",
    "                output_dict['inception_latest_date_prec'] = '7'\n",
    "            else:\n",
    "                output_dict['inception_latest_date_prec'] = '9'\n",
    "        else:\n",
    "            output_dict['inception_earliest_date_val'] = ''\n",
    "            output_dict['inception_earliest_date_prec'] = ''\n",
    "            output_dict['inception_latest_date_val'] = ''\n",
    "            output_dict['inception_latest_date_prec'] = ''\n",
    "            \n",
    "    if not act_found and commons_found:\n",
    "        output_dict['inception_ref1_referenceUrl'] = commons_url\n",
    "        output_dict['inception_ref1_retrieved_val'] = today\n",
    "        output_dict['inception_val'] = generate_date_string(commons_date, commons_bce)\n",
    "        if commons_circa:\n",
    "            output_dict['inception_sourcing_circumstances'] = 'Q5727902'\n",
    "        else:\n",
    "            output_dict['inception_sourcing_circumstances'] = ''\n",
    "        output_dict['inception_prec'] = commons_precision\n",
    "        if act_range:\n",
    "            output_dict['inception_earliest_date_val'] = generate_date_string(commons_start_date, commons_start_bce)\n",
    "            output_dict['inception_earliest_date_prec'] = commons_start_precision\n",
    "            output_dict['inception_latest_date_val'] = generate_date_string(commons_end_date, act_bce)\n",
    "            output_dict['inception_latest_date_prec'] = commons_end_precision\n",
    "        else:\n",
    "            output_dict['inception_earliest_date_val'] = ''\n",
    "            output_dict['inception_earliest_date_prec'] = ''\n",
    "            output_dict['inception_latest_date_val'] = ''\n",
    "            output_dict['inception_latest_date_prec'] = ''\n",
    "        \n",
    "    # In the event that dates are available from both sources, use the ACT date data, but flag\n",
    "    # as a potential error if any of the dates disagree.\n",
    "    if act_found and commons_found:\n",
    "        output_dict['inception_ref1_referenceUrl'] = act_url\n",
    "        output_dict['inception_ref1_retrieved_val'] = today\n",
    "        # Inconsistency check for CE/BCE\n",
    "        if act_bce != commons_bce:\n",
    "            issue_log += act_id + ' | ' + filename + ' | Disagreement between ACT and Commons on CE/BCE.\\n'\n",
    "        \n",
    "        # Check for mismatch between the primary inception date of ACT and Commons\n",
    "        if act_date != commons_date:\n",
    "            issue_log += act_id + ' | ' + filename + ' | ACT inception: ' + str(act_date) + ', Commons inception: ' + str(commons_date) + '\\n'\n",
    "        output_dict['inception_val'] = generate_date_string(act_date, act_bce)\n",
    "        \n",
    "        # Inconsistency check for circa\n",
    "        if act_circa != commons_circa:\n",
    "            issue_log += act_id + ' | ' + filename + ' | Disagreement between ACT and Commons on circa.\\n'\n",
    "        if act_circa:\n",
    "            output_dict['inception_sourcing_circumstances'] = 'Q5727902'\n",
    "        else:\n",
    "            output_dict['inception_sourcing_circumstances'] = ''\n",
    "            \n",
    "        if act_century:\n",
    "            act_precision = '7'\n",
    "        else:\n",
    "            act_precision = '9'\n",
    "            \n",
    "        # Perform a precision consistency check. \n",
    "        if act_precision != commons_precision:\n",
    "            issue_log += act_id + ' | ' + filename + ' | ACT precision: ' + act_precision + ', Commons precision: ' + commons_precision + '\\n'\n",
    "        output_dict['inception_prec'] = act_precision\n",
    "        \n",
    "        if act_range:\n",
    "            # Perform date range consistency check\n",
    "            if not(commons_start_date == 0 and commons_end_date == 0): # Skip check if no Commons range\n",
    "                if not(act_start_date == commons_start_date and act_end_date == commons_end_date):\n",
    "                    issue_log += act_id + ' | ' + filename + ' | ACT date range: ' + str(act_start_date) + '-' + str(act_end_date) + ', Commons date range: ' + str(commons_start_date) + '-' + str(commons_end_date) + '\\n'\n",
    "            output_dict['inception_earliest_date_val'] = generate_date_string(act_start_date, act_bce)\n",
    "            output_dict['inception_earliest_date_prec'] = act_precision\n",
    "            output_dict['inception_latest_date_val'] = generate_date_string(act_end_date, act_bce)\n",
    "            output_dict['inception_latest_date_prec'] = act_precision\n",
    "        else:\n",
    "            output_dict['inception_earliest_date_val'] = ''\n",
    "            output_dict['inception_earliest_date_prec'] = ''\n",
    "            output_dict['inception_latest_date_val'] = ''\n",
    "            output_dict['inception_latest_date_prec'] = ''\n",
    "            \n",
    "\n",
    "    # Check dates for reasonableness\n",
    "    if output_dict['inception_val'] > today:\n",
    "        issue_log += act_id + ' | ' + filename + ' | Inception date occurs in the future. Date: ' + output_dict['inception_val'] + '\\n'\n",
    "    if output_dict['inception_earliest_date_val'] > output_dict['inception_latest_date_val']:\n",
    "        issue_log += act_id + ' | ' + filename + ' | Final date in range before initial date: ' + output_dict['inception_earliest_date_val'] + ', ' + output_dict['inception_latest_date_val'] + '\\n'\n",
    "\n",
    "    return output_dict, issue_log\n",
    "\n",
    "def process_labels(filename, commons_data_type, commons_data):\n",
    "    languages = [\n",
    "        {'string':'English', 'code': 'en'},\n",
    "        {'string':'Русский', 'code': 'ru'},\n",
    "        {'string':'Deutsch', 'code': 'de'},\n",
    "        {'string':'Français', 'code': 'fr'},\n",
    "        {'string':'Ελληνικά', 'code': 'el'},\n",
    "        {'string':'Norsk bokmål', 'code': 'nb'},\n",
    "        {'string':'Português', 'code': 'pt'},\n",
    "        {'string':'Italiano', 'code': 'it'},\n",
    "        {'string':'Español', 'code': 'es'},\n",
    "        {'string':'עברית', 'code': 'he'},\n",
    "        {'string':'Català', 'code': 'ca'},\n",
    "        {'string':'Slovenščina', 'code': 'sl'},\n",
    "        {'string':'Hrvatski', 'code': 'hr'},\n",
    "        {'string':'Svenska', 'code': 'sv'},\n",
    "        {'string':'Српски / srpski', 'code': 'sr'},\n",
    "        {'string': '中文', 'code': 'zh-Hans'}\n",
    "         ]\n",
    "\n",
    "    # Look up the title in the Commons data. For information template, it's usually description; for artwork: title\n",
    "    commons_title_list = []\n",
    "    if commons_data_type != 'none':\n",
    "        if commons_data_type == 'artwork':\n",
    "            title_field = 'title'\n",
    "        else:\n",
    "            title_field = 'description'\n",
    "        commons_title_list = json.loads(commons_data.loc[commons_data.filename == filename, title_field].values[0])\n",
    "    #print(act_title, commons_title_list)\n",
    "    \n",
    "    work_language_strings = []\n",
    "    build_string = ''\n",
    "    language_code = 'en' # assume starting with English if no explicit tag\n",
    "    # Examine all of the blobs of the Commons language list\n",
    "    for blob in commons_title_list:\n",
    "        found = False\n",
    "        for language in languages:\n",
    "            if language['string'] in blob:\n",
    "                found = True\n",
    "                found_code = language['code']\n",
    "                break\n",
    "        if found:\n",
    "            # close off previous language string\n",
    "            work_language_strings.append({'lang': language_code, 'title': build_string.strip()})\n",
    "            # start new build string\n",
    "            language_code = found_code\n",
    "            build_string = ''\n",
    "        else:\n",
    "            if language_code == 'zh-Hans': # don't put spaces between Chinese characters\n",
    "                build_string += blob\n",
    "            else:\n",
    "                if blob[0] == ',': # don't put a space before a comma\n",
    "                    build_string += blob\n",
    "                else:\n",
    "                    build_string += ' ' + blob # otherwise, put spaces between the concatenated blobs\n",
    "    # When through the entire list, finish off the last string\n",
    "    work_language_strings.append({'lang': language_code, 'title': build_string.strip()})\n",
    "    # If the very first blob was a language tag and nothing was added to the initial string, then delete it\n",
    "    if work_language_strings[0]['title'] == '':\n",
    "        work_language_strings = work_language_strings[1:]\n",
    "            \n",
    "    return work_language_strings\n",
    "\n",
    "def process_type(act_id, act_type_string, commons_type_string, issue_log):\n",
    "    types = [\n",
    "        {'string': 'drawing', 'qid': 'Q93184'},\n",
    "        {'string': 'painting', 'qid': 'Q3305213'},\n",
    "        {'string': 'manuscript', 'qid': 'Q87167'},\n",
    "        {'string': 'sculpture', 'qid': 'Q860861'},\n",
    "        {'string': 'carving', 'qid': 'Q97570030'},\n",
    "        {'string': 'textile', 'qid': 'Q28823'},\n",
    "        {'string': 'garment', 'qid': 'Q11460'},\n",
    "        {'string': 'photograph', 'qid': 'Q125191'},\n",
    "        {'string': 'architecture', 'qid': 'Q811979'},\n",
    "        {'string': 'mosaic', 'qid': 'Q133067'},\n",
    "        {'string': 'fresco', 'qid': 'Q22669139'},\n",
    "        {'string': 'print', 'qid': 'Q11060274'},\n",
    "        {'string': 'mural', 'qid': 'Q219423'},\n",
    "        {'string': 'watercolor', 'qid': 'Q3305213'}\n",
    "         ]\n",
    "\n",
    "    #print(act_type_string, commons_type_string)\n",
    "    commons_type_list = json.loads(commons_type_string)\n",
    "    qid = ''\n",
    "    type_string = ''\n",
    "    data_source = ''\n",
    "    # Look for type in ACT data\n",
    "    found = False\n",
    "    for kind in types:\n",
    "        if kind['string'] in act_type_string.lower():\n",
    "            found = True\n",
    "            type_string = kind['string']\n",
    "            qid = kind['qid']\n",
    "            data_source = 'act'\n",
    "            break\n",
    "    if not found:\n",
    "        found = False\n",
    "        for kind in types:\n",
    "            for blob in commons_type_list:\n",
    "                if kind['string'] in blob.lower():\n",
    "                    found = True\n",
    "                    type_string = kind['string']\n",
    "                    qid = kind['qid']\n",
    "                    data_source = 'commons'\n",
    "                    break\n",
    "    if not found:\n",
    "        issue_log += act_id + ' | Could not identify type (instance of)\\n'\n",
    "        \n",
    "    if type_string == 'architecture':\n",
    "        type_string = 'architectural structure'\n",
    "    \n",
    "    return type_string, qid, issue_log, data_source\n",
    "\n",
    "# *** TODO: need to output information that was discovered that might be used to create artist items\n",
    "# Also, make use of any information found about photographers of 3D objects\n",
    "def process_artists(act_id, act_artist_string, filename, commons_data_type, commons_data, issue_log):\n",
    "    # Look up the artist in the Commons data. For information template, it's author; for artwork: artist\n",
    "    commons_artist_list = []\n",
    "    if commons_data_type != 'none':\n",
    "        if commons_data_type == 'artwork':\n",
    "            creator_field = 'artist'\n",
    "        else:\n",
    "            creator_field = 'author'\n",
    "        commons_artist_list = json.loads(commons_data.loc[commons_data.filename == filename, creator_field].values[0])\n",
    "    #print(act_artist_string, commons_artist_list)\n",
    "    #print()\n",
    "    \n",
    "    # Dissect the ACT artist string\n",
    "    # Note: currently not really making use of given_name and family_name\n",
    "    unidentified = False\n",
    "    approximate = False\n",
    "    if act_artist_string == '':\n",
    "            given_name = ''\n",
    "            family_name = ''\n",
    "            name = ''\n",
    "            dates_string = ''\n",
    "            birth_date = ''\n",
    "            death_date = ''\n",
    "    else:\n",
    "        pieces = act_artist_string.split(',')\n",
    "        #print(pieces)\n",
    "        if len(pieces) == 1:\n",
    "            given_name = ''\n",
    "            family_name = ''\n",
    "            name = act_artist_string\n",
    "            dates_string = ''\n",
    "        elif len(pieces) == 2:\n",
    "            given_name = pieces[1].strip()\n",
    "            family_name = pieces[0].strip()\n",
    "            name = given_name + ' ' + family_name\n",
    "            dates_string = ''\n",
    "        elif len(pieces) == 3:\n",
    "            given_name = pieces[1].strip()\n",
    "            family_name = pieces[0].strip()\n",
    "            name = given_name + ' ' + family_name\n",
    "            dates_string = pieces[2].strip()\n",
    "        else:\n",
    "            given_name = pieces[1].strip()\n",
    "            family_name = pieces[0].strip()\n",
    "            name = given_name + ' ' + family_name\n",
    "            dates_string = pieces[2].strip()\n",
    "            issue_log += act_id + ' | Check ACT artist name (more than 4 parts)\\n'\n",
    "            \n",
    "        if 'unidentified' in name.lower():\n",
    "            unidentified = True\n",
    "            name = ''\n",
    "        \n",
    "        # Process artist dates\n",
    "        if dates_string == '':\n",
    "            birth_date = ''\n",
    "            death_date = ''\n",
    "        else:\n",
    "            if 'approximately' in dates_string:\n",
    "                dates_string = dates_string.split('approximately')[1].strip()\n",
    "                approximate = True\n",
    "                \n",
    "            if '-' in dates_string:\n",
    "                pieces = dates_string.split('-')\n",
    "                birth_date = pieces[0]\n",
    "                death_date = pieces[1]\n",
    "            else:\n",
    "                try:\n",
    "                    if 'd' in dates_string:\n",
    "                        death_date = dates_string.split(' ')[1]\n",
    "                        birth_date = ''\n",
    "                    elif 'b' in dates_string:\n",
    "                        birth_date = dates_string.split(' ')[1]\n",
    "                        death_date = ''\n",
    "                    else:\n",
    "                        birth_date = ''\n",
    "                        death_date = ''\n",
    "                except:\n",
    "                    birth_date = ''\n",
    "                    death_date = ''\n",
    "\n",
    "    #print('given:', given_name, '/ family:', family_name, '/ name:', name, '/ unidentified:', unidentified)\n",
    "    #print('born:', birth_date, '/ died:', death_date, '/ approx:', approximate)\n",
    "    #print()\n",
    "        \n",
    "    # Process Commons creator data\n",
    "    unknown = False\n",
    "    if len(commons_artist_list) == 0:\n",
    "        commons_name = ''\n",
    "        commons_photographer = ''\n",
    "    else:\n",
    "        first_blob = commons_artist_list[0].strip().lower()\n",
    "        if 'anonymous' in first_blob or 'unknown' in first_blob:\n",
    "            unknown = True\n",
    "            commons_name = ''\n",
    "            commons_photographer = ''\n",
    "        else:\n",
    "            if len(commons_artist_list) == 1:\n",
    "                commons_name = commons_artist_list[0].strip()\n",
    "                commons_photographer = ''\n",
    "            else:\n",
    "                if 'photograph' in first_blob:\n",
    "                    commons_name = ''\n",
    "                    commons_photographer = commons_artist_list[1].strip()\n",
    "                else:\n",
    "                    commons_name = commons_artist_list[0].strip()\n",
    "                    commons_photographer = ''\n",
    "    \n",
    "        # If the Commons name has a comma, then it needs to be reversed\n",
    "        #if ',' in commons_name:\n",
    "        #    pieces = commons_name.split(',')\n",
    "        #    commons_name = pieces[1].strip() + ' ' + pieces[0].strip()\n",
    "    \n",
    "    #print('commons name:', commons_name, '/ photographer:', commons_photographer, '/ unknown:', unknown)\n",
    "    \n",
    "    #print('act name:', name)\n",
    "    #print('commons name:', commons_name)\n",
    "\n",
    "    # Reconcile names, attempt to match with Wikidata item, and prepare data to return from function\n",
    "    anonymous = False\n",
    "    source = ''\n",
    "    name_string = ''\n",
    "    \n",
    "    if name != '' and commons_name != '': # case where both names are known\n",
    "        source = 'act'\n",
    "        name_string = name\n",
    "        #ratio = fuzz.ratio(name, commons_name)\n",
    "        #partial_ratio = fuzz.partial_ratio(name, commons_name)\n",
    "        #sort_ratio = fuzz.token_sort_ratio(name, commons_name)\n",
    "        set_ratio = fuzz.token_set_ratio(name, commons_name)\n",
    "        #w_ratio = fuzz.WRatio(name, commons_name)\n",
    "        #print('name similarity ratio', ratio)\n",
    "        #print('partial ratio', partial_ratio)\n",
    "        #print('sort_ratio', sort_ratio)\n",
    "        #print('set_ratio', set_ratio)\n",
    "        #print('w_ratio', w_ratio)\n",
    "        \n",
    "        if set_ratio < 95:\n",
    "            issue_log += act_id + ' | Low similarity ratio: ' + str(set_ratio) + ' between ACT name: ' + name + ' and Commons name: ' + commons_name + '\\n'\n",
    "            search_names = [name] # if disagreement, go with the ACT name\n",
    "        else:\n",
    "            # If the names are similar, use both in search\n",
    "            search_names = [name, commons_name]\n",
    "    elif name == '' and commons_name == '': # case where neither name is known\n",
    "        search_names = []\n",
    "        if unknown or unidentified: # Only set as anonymous if asserted by either ACT or Commons\n",
    "            anonymous = True\n",
    "            name_string = 'artist unknown'\n",
    "            if unknown:\n",
    "                source = 'commons'\n",
    "            else:\n",
    "                source = 'act'\n",
    "    elif commons_name == '':\n",
    "        search_names = [name]\n",
    "        name_string = name\n",
    "        source = 'act'\n",
    "        if unknown: # Commons says doesn't know name but there is an ACT name\n",
    "            issue_log += act_id + ' | Commons says name unknown, but ACT gives name: ' + name\n",
    "    else:\n",
    "        search_names = [commons_name]\n",
    "        name_string = commons_name\n",
    "        source = 'commons'\n",
    "        if unidentified: # ACT says doesn't know name but there is an Commons name\n",
    "            issue_log += act_id + ' | ACT says author unidentified, but Commons gives name: ' + commons_name\n",
    "\n",
    "    artist_qid = ''\n",
    "    unmatched = {}\n",
    "    if search_names != []:\n",
    "        # Search Wikidata for variants of the name\n",
    "        hits = search_name_at_wikidata(search_names) # Searching a list of names is a hack of the original function\n",
    "\n",
    "        # Perform screens to eliminate items that aren't human, etc.\n",
    "        qids = []\n",
    "        for hit in hits:\n",
    "            qids.append(hit['qid'])\n",
    "        return_list = screen_qids(qids, screens)\n",
    "        #print(return_list)\n",
    "\n",
    "        # Try to match birth or death dates with what's in Wikidata for the candidate matches\n",
    "        matched = False\n",
    "        for artist in return_list:\n",
    "            #print(artist['qid'])\n",
    "            birth_date_list = retrieve_date_wikidata(artist['qid'],'P569')\n",
    "            if len(birth_date_list) >= 1:\n",
    "                wd_birth_date = birth_date_list[0][0:4]\n",
    "            else:\n",
    "                wd_birth_date = ''\n",
    "\n",
    "            death_date_list = retrieve_date_wikidata(artist['qid'],'P570')\n",
    "            if len(death_date_list) >= 1:\n",
    "                wd_death_date = death_date_list[0][0:4]\n",
    "            else:\n",
    "                wd_death_date = ''\n",
    "\n",
    "            if birth_date != '' and birth_date == wd_birth_date:\n",
    "                matched = True\n",
    "            if death_date !='' and death_date == wd_death_date:\n",
    "                matched = True\n",
    "\n",
    "            if matched:\n",
    "                artist_qid = artist['qid']\n",
    "                break\n",
    "        \n",
    "        if len(return_list) > 0 and not matched:\n",
    "            print('No match with possible names:', return_list)\n",
    "            unmatched = {\n",
    "                'search_names': search_names,\n",
    "                'possible_matches': return_list\n",
    "            }\n",
    "        \n",
    "    return artist_qid, name_string, source, anonymous, issue_log, death_date, unmatched\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract/clean inception date\n",
    "\n",
    "The dates given in ACT and Commons can have several characteristics:\n",
    "\n",
    "- beginning and ending ranges\n",
    "- circa. Designated very consistently as \"ca.\" in ACT. \n",
    "- century designation (essentially setting the precision to the century level)\n",
    "\n",
    "The Commons data also sometimes is structured using Wikidata date Q IDs, qualifiers, and standard xsd:dateTime format:\n",
    "- Starts with `QS:P571` (inception)\n",
    "- Sometimes has `P1319` (earliest date) and `P1326` (latest date)\n",
    "- Sometimes had `P580` (start time) and `P582` (end time)\n",
    "- Sometimes has qualifier `P1480` (sourcing circumstances) with value `Q5727902` (circa)\n",
    "- Date precisions can be 7 (century), 8 (decade), or 9 (year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_issue_log = ''\n",
    "unidentified_artists = []\n",
    "\n",
    "# If there are any existing data in VanderBot format, load it (or just the file headers)\n",
    "output_list = read_dicts_from_csv('abstract_artworks.csv')\n",
    "fieldnames = [\n",
    "    'commons_template',\n",
    "    'label_en',\n",
    "    'label_commons',\n",
    "    'description_en',\n",
    "    'act_uuid',\n",
    "    'act',\n",
    "    'act_ref1_hash',\n",
    "    'act_ref1_retrieved_nodeId',\n",
    "    'act_ref1_retrieved_val',\n",
    "    'act_ref1_retrieved_prec',\n",
    "    'inventory_number_uuid',\n",
    "    'inventory_number',\n",
    "    'inventory_number_collection',\n",
    "    'inventory_number_ref1_hash',\n",
    "    'inventory_number_ref1_referenceUrl',\n",
    "    'inventory_number_ref1_retrieved_nodeId',\n",
    "    'inventory_number_ref1_retrieved_val',\n",
    "    'inventory_number_ref1_retrieved_prec',\n",
    "    'title_uuid',\n",
    "    'title',\n",
    "    'title_ref1_hash',\n",
    "    'title_ref1_referenceUrl',\n",
    "    'title_ref1_retrieved_nodeId',\n",
    "    'title_ref1_retrieved_val',\n",
    "    'title_ref1_retrieved_prec',\n",
    "    'creator_uuid',\n",
    "    'creator',\n",
    "    'creator_object_has_role,'\n",
    "    'creator_ref1_hash',\n",
    "    'creator_ref1_referenceUrl',\n",
    "    'creator_ref1_retrieved_nodeId',\n",
    "    'creator_ref1_retrieved_val',\n",
    "    'creator_ref1_retrieved_prec',\n",
    "    'instance_of_uuid',\n",
    "    'instance_of',\n",
    "    'instance_of_ref1_hash',\n",
    "    'instance_of_ref1_referenceUrl',\n",
    "    'instance_of_ref1_retrieved_nodeId',\n",
    "    'instance_of_ref1_retrieved_val',\n",
    "    'instance_of_ref1_retrieved_prec',\n",
    "    'inception_uuid',\n",
    "    'inception_nodeId',\n",
    "    'inception_val',\n",
    "    'inception_prec',\n",
    "    'inception_earliest_date_nodeId',\n",
    "    'inception_earliest_date_val',\n",
    "    'inception_earliest_date_prec',\n",
    "    'inception_latest_date_nodeId',\n",
    "    'inception_latest_date_val',\n",
    "    'inception_latest_date_prec',\n",
    "    'inception_sourcing_circumstances',\n",
    "    'inception_ref1_hash',\n",
    "    'inception_ref1_referenceUrl',\n",
    "    'inception_ref1_retrieved_nodeId',\n",
    "    'inception_ref1_retrieved_val',\n",
    "    'inception_ref1_retrieved_prec',\n",
    "    'country_of_origin_uuid',\n",
    "    'country_of_origin',\n",
    "    'country_of_origin_ref1_hash',\n",
    "    'country_of_origin_ref1_referenceUrl',\n",
    "    'country_of_origin_ref1_retrieved_nodeId',\n",
    "    'country_of_origin_ref1_retrieved_val',\n",
    "    'country_of_origin_ref1_retrieved_prec',\n",
    "    'copyright_status_uuid',\n",
    "    'copyright_status',\n",
    "    'copyright_status_applies_to_jurisdiction',\n",
    "    'copyright_status_determination_method',\n",
    "    'copyright_status_ref1_hash',\n",
    "    'copyright_status_ref1_referenceUrl',\n",
    "    'copyright_status_ref1_retrieved_nodeId',\n",
    "    'copyright_status_ref1_retrieved_val',\n",
    "    'copyright_status_ref1_retrieved_prec',\n",
    "    'image_uuid',\n",
    "    'image',\n",
    "    'image_ref1_hash',\n",
    "    'image_ref1_referenceUrl',\n",
    "    'image_ref1_retrieved_nodeId',\n",
    "    'image_ref1_retrieved_val',\n",
    "    'image_ref1_retrieved_prec',\n",
    "    'collection_uuid',\n",
    "    'collection',\n",
    "    'collection_ref1_hash',\n",
    "    'collection_ref1_referenceUrl',\n",
    "    'collection_ref1_retrieved_nodeId',\n",
    "    'collection_ref1_retrieved_val',\n",
    "    'collection_ref1_retrieved_prec'\n",
    "]\n",
    "\n",
    "labels_data = []\n",
    "\n",
    "for index, work in act_data.iterrows():\n",
    "    issue_log = ''\n",
    "    output_dict = {}\n",
    "    act_id = work['RecordNumber']\n",
    "    act_url = 'http://diglib.library.vanderbilt.edu/act-imagelink.pl?RC=' + act_id\n",
    "    \n",
    "    # Look up the Commons page URL\n",
    "    commons_url = ids.loc[ids.RecordNumber == act_id, 'commons_page_url'].values[0]\n",
    "    \n",
    "    # Look up the Commons filename in the IDs file using the ACT ID\n",
    "    # This seems inefficient to have to look up the row twice, but I suppose Pandas is\n",
    "    # efficient enough with a small dataset like this that it doesn't matter.\n",
    "    filename = ids.loc[ids.RecordNumber == act_id, 'filename'].values[0]\n",
    "\n",
    "    # Determine whether the information or artwork template was used, or if Commons data not available\n",
    "    commons_date_type_series = commons_data.loc[commons_data.filename == filename, 'template_type']    \n",
    "    if len(commons_date_type_series) == 1:\n",
    "        commons_data_type = commons_date_type_series.values[0]\n",
    "    else:\n",
    "        commons_data_type = 'none'\n",
    "    output_dict['commons_template'] = commons_data_type\n",
    "\n",
    "    print('commons data type:', commons_data_type)\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # *** Process title information to generate English label ***\n",
    "    \n",
    "    act_title = work['Title'].strip()\n",
    "    \n",
    "    print(act_title)\n",
    "    \n",
    "    # Extract information about titles/labels from Commons data\n",
    "    work_language_strings = process_labels(filename, commons_data_type, commons_data)\n",
    "    \n",
    "    # Attach the extracted language data to the list for later saving\n",
    "    labels_data.append({'act_id': act_id, 'act_title': act_title, 'commons_data_type': commons_data_type, 'commons_language_strings': work_language_strings})\n",
    "    \n",
    "    # Add discovered label data to the output\n",
    "    output_dict['label_en'] = act_title # Use the ACT title as the default\n",
    "    \n",
    "    commons_label = ''\n",
    "    for work_language in work_language_strings:\n",
    "        if work_language['lang'] == 'en':\n",
    "            commons_label = work_language['title'].strip()\n",
    "    output_dict['label_commons'] = commons_label\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # *** Other fields ***\n",
    "    \n",
    "    # Create ACT ID output\n",
    "    output_dict['act'] = act_id\n",
    "    output_dict['act_ref1_retrieved_val'] = today\n",
    "    \n",
    "    # Create Commons image output\n",
    "    # VanderBot will convert the raw, unencoded file name into the appropriate IRI for Wikidata\n",
    "    output_dict['image'] = filename\n",
    "    output_dict['image_ref1_referenceUrl'] = act_url\n",
    "    output_dict['image_ref1_retrieved_val'] = today\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # *** Process inception dates ***\n",
    "    \n",
    "    # Get the ACT date string value\n",
    "    act_date_string = work['DateCreation'].strip()\n",
    "\n",
    "    # Look up the date value in the Commons data\n",
    "    if commons_data_type != 'none':\n",
    "        commons_date_string = commons_data.loc[commons_data.filename == filename, 'date'].values[0]\n",
    "    else:\n",
    "        commons_date_string = '[]' # case where no commons data exists for this work\n",
    "\n",
    "    #print(act_date_string, commons_date_string)\n",
    "    \n",
    "    output_dict, issue_log = process_dates(act_id, output_dict, issue_log, act_url, commons_url, act_date_string, commons_date_string)\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # *** Determine type (instance of) ***\n",
    "    \n",
    "    act_type_string = work['ObjectFunction'].strip()\n",
    "    \n",
    "    # Look up the object type value in the Commons data\n",
    "    if commons_data_type != 'none':\n",
    "        commons_type_string = commons_data.loc[commons_data.filename == filename, 'object type'].values[0]\n",
    "    else:\n",
    "        commons_type_string = '[]' # case where no commons data exists for this work\n",
    "    type_string, instance_of_qid, issue_log, data_source = process_type(act_id, act_type_string, commons_type_string, issue_log)\n",
    "    #print(type_string, instance_of_qid)\n",
    "    \n",
    "    output_dict['instance_of'] = instance_of_qid\n",
    "    if data_source == 'act':\n",
    "        output_dict['instance_of_ref1_referenceUrl'] = act_url\n",
    "    elif data_source == 'commons':\n",
    "        output_dict['instance_of_ref1_referenceUrl'] = commons_url\n",
    "    else:\n",
    "        output_dict['instance_of_ref1_referenceUrl'] = ''\n",
    "    if data_source != '':\n",
    "        output_dict['instance_of_ref1_retrieved_val'] = today\n",
    "    else:\n",
    "        output_dict['instance_of_ref1_retrieved_val'] = ''\n",
    "\n",
    "    print('work type:', type_string)\n",
    "\n",
    "    # -----------------------------------\n",
    "    # *** Determine artist ***\n",
    "    \n",
    "    act_artist_string = work['CreatorArtist'].strip()\n",
    "    artist_qid, name_string, data_source, anonymous, issue_log, death_date, unmatched = process_artists(act_id, act_artist_string, filename, commons_data_type, commons_data, issue_log)\n",
    "    \n",
    "    output_dict['creator'] = artist_qid\n",
    "    if anonymous:\n",
    "        output_dict['creator'] = 'anon'\n",
    "    if output_dict['creator'] != '':\n",
    "        if data_source == 'act':\n",
    "            output_dict['creator_ref1_referenceUrl'] = act_url\n",
    "        elif data_source == 'commons':\n",
    "            output_dict['creator_ref1_referenceUrl'] = commons_url\n",
    "        else:\n",
    "            output_dict['creator_ref1_referenceUrl'] = ''\n",
    "        if data_source != '':\n",
    "            output_dict['creator_ref1_retrieved_val'] = today\n",
    "        else:\n",
    "            output_dict['creator_ref1_retrieved_val'] = ''\n",
    "            \n",
    "    # If the artist was unidentified and there were possible matches, add them to the list\n",
    "    if unmatched != {}:\n",
    "        unidentified_artists.append(unmatched)\n",
    "        \n",
    "    # -----------------------------------\n",
    "    # *** Generate description string ***\n",
    "    \n",
    "    description = ''\n",
    "    if type_string != '':\n",
    "        description += type_string\n",
    "    if name_string != '':\n",
    "        description += ' by ' + name_string\n",
    "    output_dict['description_en'] = description\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # *** Find out if the work is Public Domain ***\n",
    "    \n",
    "    act_copyright_string = work['CopyrightStatus'].strip()\n",
    "    \n",
    "    # Look up the date value in the Commons data\n",
    "    if commons_data_type != 'none':\n",
    "        commons_permission_string = commons_data.loc[commons_data.filename == filename, 'permission'].values[0]\n",
    "    else:\n",
    "        commons_permission_string = '[]' # case where no commons data exists for this work\n",
    "     \n",
    "    # Start off with no values and overwrite as discovered\n",
    "    output_dict['copyright_status'] = ''\n",
    "    output_dict['copyright_status_applies_to_jurisdiction'] = ''\n",
    "    output_dict['copyright_status_determination_method'] = ''\n",
    "    output_dict['copyright_status_ref1_referenceUrl'] = ''\n",
    "    output_dict['copyright_status_ref1_retrieved_val'] = ''\n",
    "\n",
    "    # Determine years since death of artist, if known\n",
    "    if death_date != '':\n",
    "        #print(today, death_date)\n",
    "        try:\n",
    "            years_since_death = int(today[:4]) - int(death_date)\n",
    "        except:\n",
    "            years_since_death = 0\n",
    "        if years_since_death > 100:\n",
    "            output_dict['copyright_status'] = 'Q19652' # Public Domain\n",
    "            output_dict['copyright_status_applies_to_jurisdiction'] = 'Q60332278' # countries with 100 years pma or shorter\n",
    "            output_dict['copyright_status_determination_method'] = 'Q29940705' # 100 years or more after author's death\n",
    "        \n",
    "    if 'public domain' in act_copyright_string.lower() or 'public domain' in commons_permission_string.lower():\n",
    "        if 'public domain' in act_copyright_string.lower():\n",
    "            copyright_ref = act_url\n",
    "        else:\n",
    "            copyright_ref = commons_url\n",
    "        # Determination method will be left blank since we don't know how the sources decided this\n",
    "        output_dict['copyright_status'] = 'Q19652' # OK to write over value if already determined from dates\n",
    "        output_dict['copyright_status_ref1_referenceUrl'] = copyright_ref\n",
    "        output_dict['copyright_status_ref1_retrieved_val'] = today\n",
    "        \n",
    "    # -----------------------------------\n",
    "    # *** Determine country of origin ***\n",
    "    \n",
    "    # Default to no values\n",
    "    output_dict['country_of_origin'] = ''\n",
    "    output_dict['country_of_origin_ref1_referenceUrl'] = ''\n",
    "    output_dict['country_of_origin_ref1_retrieved_val'] = ''\n",
    "\n",
    "    country_string = work['LocationCountry'].strip()\n",
    "    if country_string != '' and work['OriginalLocation'] == '': # override if there is an original location (rare)\n",
    "        country_qid_series = country_mappings.loc[country_mappings.string == country_string, 'qid']\n",
    "        if len(country_qid_series) == 1: # must be at least one match\n",
    "            output_dict['country_of_origin'] = country_qid_series.values[0]\n",
    "            output_dict['country_of_origin_ref1_referenceUrl'] = act_url\n",
    "            output_dict['country_of_origin_ref1_retrieved_val'] = today\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # Look up the collection from Commons data if it exists\n",
    "    \n",
    "    if commons_data_type != 'none':\n",
    "        commons_collection_string = commons_data.loc[commons_data.filename == filename, 'collection'].values[0]\n",
    "    else:\n",
    "        commons_collection_string = '[]' # case where no commons data exists for this work\n",
    "    commons_collection_list = json.loads(commons_collection_string)\n",
    "    #print(commons_collection_list)\n",
    "\n",
    "    # Default values to empty string\n",
    "    output_dict['collection'] = ''\n",
    "    output_dict['collection_ref1_referenceUrl'] = ''\n",
    "    output_dict['collection_ref1_retrieved_val'] = ''\n",
    "    collection_qid = ''\n",
    "    \n",
    "    if len(commons_collection_list) > 0:\n",
    "        if ':' in commons_collection_list[0]: # for cases where the language is the first item, e.g. \"Deutsch:\"\n",
    "            try: # need to error trap because in rare cases there is only a collection name and it includes a colon\n",
    "                collection_name = commons_collection_list[1]\n",
    "            except:\n",
    "                collection_name = commons_collection_list[0]\n",
    "        else: # otherwise, the name is listed first\n",
    "            collection_name = commons_collection_list[0]\n",
    "\n",
    "        # Look up the collection Q ID\n",
    "        collection_series = collections_mappings.loc[collections_mappings.name == collection_name, 'qid']\n",
    "        if len(collection_series) == 1: # a single match is found\n",
    "            collection_qid = collection_series.values[0]\n",
    "            output_dict['collection'] = collection_qid\n",
    "            output_dict['collection_ref1_referenceUrl'] = commons_url\n",
    "            output_dict['collection_ref1_retrieved_val'] = today\n",
    "        else:\n",
    "            pass # No match is found (or perhaps multiple, but avoid that in the collections.csv file)\n",
    "    \n",
    "    # -----------------------------------\n",
    "    # Look up the accession number from Commons data if it exists\n",
    "\n",
    "    if commons_data_type != 'none':\n",
    "        commons_accession_string = commons_data.loc[commons_data.filename == filename, 'accession number'].values[0]\n",
    "    else:\n",
    "        commons_accession_string = '[]' # case where no commons data exists for this work\n",
    "    commons_accession_list = json.loads(commons_accession_string)\n",
    "\n",
    "    # Default values to empty string\n",
    "    output_dict['inventory_number'] = ''\n",
    "    output_dict['inventory_number_collection'] = ''\n",
    "    output_dict['inventory_number_ref1_referenceUrl'] = ''\n",
    "    output_dict['inventory_number_ref1_retrieved_val'] = ''\n",
    "    \n",
    "    # Only supply the inventory number if it can be associated with a collection\n",
    "    if collection_qid != '':\n",
    "        # If there is a clean accession number, it's generally the first item on the list\n",
    "        if len(commons_accession_list) > 0:\n",
    "            output_dict['inventory_number'] = commons_accession_list[0]\n",
    "            output_dict['inventory_number_collection'] = collection_qid\n",
    "            output_dict['inventory_number_ref1_referenceUrl'] = commons_url\n",
    "            output_dict['inventory_number_ref1_retrieved_val'] = today\n",
    "    \n",
    "    # -----------------------------------\n",
    "    if issue_log != '': # include extra blank line only if some issues were added for this work\n",
    "        issue_log += '\\n'\n",
    "    entire_issue_log += issue_log\n",
    "    print()\n",
    "    output_list.append(output_dict)\n",
    "    \n",
    "#print(json.dumps(labels_data, indent =2, ensure_ascii=False))\n",
    "\n",
    "write_dicts_to_csv(output_list, output_directory + 'abstract_artworks_out.csv', fieldnames)\n",
    "\n",
    "# Save the data for multilingual labels for future use\n",
    "with open(output_directory + 'language_labels.json', 'wt', encoding='utf8') as file_object:\n",
    "    json.dump(labels_data, file_object, indent = 2, ensure_ascii=False)\n",
    "    \n",
    "# Save the issue log in a file\n",
    "with open(output_directory + 'issue_log.txt', 'wt', encoding='utf-8') as file_object:\n",
    "    file_object.write(entire_issue_log)\n",
    "    \n",
    "# Save the potential artist matches in a file\n",
    "with open(output_directory + 'unidentified_artists.json', 'wt', encoding='utf-8') as file_object:\n",
    "    file_object.write(json.dumps(unidentified_artists, indent=2))\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entire_issue_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
