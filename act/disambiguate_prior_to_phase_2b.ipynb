{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality control script prior to phase 2b\n",
    "\n",
    "2022-02-01\n",
    "\n",
    "After finishing the creation of artwork items for works where ACT IDs were mis-assigned to non-artwork items, this is a quality control script to check for duplicates and make sure that the list of items to be added is actually correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2022 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# Import modules\n",
    "import json\n",
    "import csv\n",
    "import math\n",
    "import datetime\n",
    "import urllib\n",
    "from time import sleep\n",
    "import requests\n",
    "import re # regex\n",
    "# Pandas for data frame management\n",
    "import pandas as pd\n",
    "# Fuzzy string matching\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "# Web scraping library\n",
    "from bs4 import BeautifulSoup # web-scraping library, use PIP to install beautifulsoup4 (included in Anaconda)\n",
    "\n",
    "accept_media_type = 'application/json'\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "user_agent_header = 'act_disambiguation/0.1 (mailto:steve.baskauf@vanderbilt.edu)'\n",
    "sparql_sleep = 0.1\n",
    "request_sleep = 0.1 # wait 0.1 seconds between each HTTP request\n",
    "default_language = 'en'\n",
    "\n",
    "# Load data\n",
    "act_data = pd.read_csv('../processed_lists/act_all_202109241353_repaired.csv', na_filter=False, dtype = str)\n",
    "ids = pd.read_csv('clean_ids.csv', na_filter=False, dtype = str)\n",
    "duplicates = pd.read_csv('../processed_lists/duplicates_of_existing_commons_ids.csv', na_filter=False, dtype = str)\n",
    "\n",
    "country_mappings = pd.read_csv('country_mappings.csv', na_filter=False, dtype = str)\n",
    "collections_mappings = pd.read_csv('collections.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# For testing purposes, just use the first few rows\n",
    "#test_rows = 10\n",
    "#ids = ids.head(test_rows).copy()\n",
    "\n",
    "# --------------------\n",
    "# Low-level functions\n",
    "# --------------------\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# NOTE: there are still some issues that have not been worked out with quotation marks in query strings.\n",
    "# Still working on this; see also the send_sparql_query() below.\n",
    "def generate_sparql_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "#        'Content-Type': 'application/sparql-query',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "sparql_request_header = generate_sparql_header_dictionary(accept_media_type, user_agent_header)\n",
    "\n",
    "\n",
    "# Functions to interconvert various forms of Commons identifiers\n",
    "commons_prefix = 'http://commons.wikimedia.org/wiki/Special:FilePath/'\n",
    "commons_page_prefix = 'https://commons.wikimedia.org/wiki/File:'\n",
    "\n",
    "def commons_url_to_filename(url):\n",
    "    # form of URL is: http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\n",
    "    string = url.split(commons_prefix)[1] # get local name file part of URL\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_url(filename):\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_prefix + encoded_filename\n",
    "    return url\n",
    "\n",
    "def commons_page_url_to_filename(url):\n",
    "    # form of URL is: https://commons.wikimedia.org/wiki/File:Castle_De_Haar_(1892-1913)_-_360%C2%B0_Panorama_of_Castle_%26_Castle_Grounds.jpg\n",
    "    string = url.split(commons_page_prefix)[1] # get local name file part of URL\n",
    "    string = string.replace('_', ' ')\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_page_url(filename):\n",
    "    filename = filename.replace(' ', '_')\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_page_prefix + encoded_filename\n",
    "    url = url.replace('%28', '(').replace('%29', ')').replace('%2C', ',')\n",
    "    return url\n",
    "\n",
    "# ----------------------------\n",
    "# Intermediate-level functions\n",
    "# ----------------------------\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "# NOTE: request_header and endpoint are global variables defined earlier in the script\n",
    "def send_sparql_query(query_string):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    #print('querying SPARQL endpoint to acquire item metadata')\n",
    "    #response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    response = requests.post(endpoint, data=dict(query=query_string), headers=sparql_request_header)\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    print('done retrieving data')\n",
    "    # print(json.dumps(results, indent=2))\n",
    "    \n",
    "    sleep(sparql_sleep) # delay to avoid hitting the Query Service too fast\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicates in the clean_ids.csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate ACT IDs (none found)\n",
    "duplicate_act_ids = ids[ids.duplicated(['RecordNumber'])]\n",
    "duplicate_act_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate filenames\n",
    "\n",
    "# Find duplicated rows\n",
    "duplicate_filename_rows = ids[ids.duplicated(['filename'])].copy()\n",
    "# Get the filename column\n",
    "duplicated_filenames_series = duplicate_filename_rows.filename\n",
    "# deduplicate, sort, and convert to list\n",
    "filenames = list(duplicated_filenames_series.drop_duplicates().sort_values())\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the discovered duplicates are in the hand-curated duplicates listing\n",
    "# Any undiscovered duplicates should added to the duplicates listing and removed from \n",
    "# the cleaned_output.csv file.\n",
    "\n",
    "for filename in filenames:\n",
    "    print(filename)\n",
    "    # look up the filename in the ids file to get all of the ACT IDs for those files\n",
    "    act_ids = ids.loc[ids.filename == filename, 'RecordNumber'].values\n",
    "    for act_id in act_ids:\n",
    "        if act_id in duplicates.actId_of_original.values:\n",
    "            print(act_id, 'is original')\n",
    "        elif act_id in duplicates.RecordNumber.values:\n",
    "            print(act_id, 'is copy')\n",
    "        else:\n",
    "            print('https://diglib.library.vanderbilt.edu//act-imagelink.pl?RC=' + act_id)\n",
    "            description = act_data.loc[act_data.RecordNumber == act_id, 'Title'].values[0]\n",
    "            print(description)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that Commons URLs still dereference\n",
    "\n",
    "The following cell is copied from the original `act.ipynb` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On 2022-02-01 there were three works that didn't dereference. See https://github.com/HeardLibrary/vandycite/issues/53#issuecomment-1027858980\n",
    "\n",
    "file_path = '../processed_lists/cleaned_output.csv'\n",
    "\n",
    "output_list = []\n",
    "data = read_dict(file_path)\n",
    "for record_number in range(len(data)):\n",
    "#for record_number in range(1530, 1535):\n",
    "    if record_number%10 == 0: # print the row number every 10 requests\n",
    "        print(record_number)\n",
    "    response = requests.get(data[record_number]['commons_page_url'])\n",
    "    #print(data[record_number]['commons_page_url'])\n",
    "    #print(response.url)\n",
    "    #print(response.status_code)\n",
    "    #print()\n",
    "    output_list.append({'status': response.status_code, 'requested_url': data[record_number]['commons_page_url'], 'response_url': response.url})\n",
    "    sleep(request_sleep)\n",
    "\n",
    "fieldnames = ['status', 'requested_url', 'response_url']\n",
    "write_dicts_to_csv(output_list, 'dereference_test.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data about items with ACT ID statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = '''\n",
    "select distinct ?qid ?act_id ?label ?description ?image_iri where {\n",
    "?qid wdt:P9092 ?act_id.\n",
    "?qid wdt:P18 ?image_iri.\n",
    "OPTIONAL {\n",
    "?qid rdfs:label ?label.\n",
    "FILTER(lang(?label)=\"'''+ default_language + '''\")\n",
    "  }\n",
    "OPTIONAL {\n",
    "?qid schema:description ?description.\n",
    "FILTER(lang(?description)=\"'''+ default_language + '''\")\n",
    "  }\n",
    "}\n",
    "'''\n",
    "#print(query_string)\n",
    "results = send_sparql_query(query_string)\n",
    "\n",
    "output_list = []\n",
    "for result in results:\n",
    "    qid = extract_local_name(result['qid']['value'])\n",
    "    act_id = result['act_id']['value']\n",
    "    label = result['label']['value']\n",
    "    try:\n",
    "        description = result['description']['value']\n",
    "    except:\n",
    "        description = ''\n",
    "    image_filename = commons_url_to_filename(result['image_iri']['value'])\n",
    "    output_list.append({'qid': qid, 'act_id': act_id, 'label': label, 'description': description, 'image_filename': image_filename})\n",
    "\n",
    "print('There are', len(output_list), 'works already in Wikidata.')\n",
    "\n",
    "print(json.dumps(output_list[:5], indent=2))\n",
    "\n",
    "fieldnames = ['qid', 'act_id', 'label', 'description', 'image_filename']\n",
    "write_dicts_to_csv(output_list, '../processed_lists/works_already_in_wikidata.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove works already in Wikidata from the cleaned output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_in = pd.read_csv('../processed_lists/works_already_in_wikidata.csv', na_filter=False, dtype = str)\n",
    "\n",
    "file_path = '../processed_lists/cleaned_output.csv'\n",
    "\n",
    "output_list = []\n",
    "data = read_dict(file_path)\n",
    "for record_number in range(len(data)):\n",
    "    if record_number%10 == 0: # print the row number every 10 requests\n",
    "        #print(record_number)\n",
    "        pass\n",
    "    # Add record to output list if not one of the ACT IDs that's already in Wikidata\n",
    "    if not data[record_number]['RecordNumber'] in already_in.act_id.values:\n",
    "        output_list.append(data[record_number])\n",
    "\n",
    "fieldnames = ['RecordNumber', 'actId', 'qid', 'filename', 'commons_uri', 'commons_page_url']\n",
    "write_dicts_to_csv(output_list, '../processed_lists/add_to_wikidata.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove works found during search of artwork labels\n",
    "\n",
    "This is basically a hack of the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_in = pd.read_csv('artwork_matches.csv', na_filter=False, dtype = str)\n",
    "\n",
    "file_path = 'add_to_wikidata.csv'\n",
    "\n",
    "output_list = []\n",
    "data = read_dict(file_path)\n",
    "for record_number in range(len(data)):\n",
    "    # Add record to output list if not one of the ACT IDs that's already in Wikidata\n",
    "    # Note: the add_to_wikidata.csv file in this directory had the column header \"act_id\"\n",
    "    if not data[record_number]['act_id'] in already_in.act_id.values:\n",
    "        output_list.append(data[record_number])\n",
    "\n",
    "fieldnames = ['act_id', 'filename', 'commons_uri', 'commons_page_url']\n",
    "write_dicts_to_csv(output_list, 'add_to_wikidata_new.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run the check for the little Wikidata flag on the works we think we need to write\n",
    "\n",
    "Best to do this before the manual work part, since it will save us from processing those that just need to be linked.\n",
    "\n",
    "Code cell hacked from https://github.com/HeardLibrary/linked-data/blob/77052c1dd0e761c58f8ba7e4395134f6255e1cfb/commonsbot/commons_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Look for a Wikidata link on the image page\n",
    "# ---------------------------\n",
    "\n",
    "# This script finds the link for the tiny little Wikidata logo found on many pages that use the artwork template. \n",
    "# It's significant because this links to the abstract artwork even if the file represented on the Commons page\n",
    "# isn't the one used as the value of the image (P18) property in Wikidata.\n",
    "\n",
    "# For references on art in Wikidata, see https://www.wikidata.org/wiki/Wikidata:WikiProject_sum_of_all_paintings\n",
    "# https://www.wikidata.org/wiki/Wikidata:WikiProject_Visual_arts/Item_structure\n",
    "\n",
    "file_path = 'add_to_wikidata.csv'\n",
    "file_data = read_dict(file_path)\n",
    "\n",
    "output_list = []\n",
    "#if True:\n",
    "for record in file_data:\n",
    "    print(record['filename'])\n",
    "\n",
    "    # Retrieve the page HTML\n",
    "    image_filename = record['filename']\n",
    "    # image_filename = 'Christ sur la mer de Galilée (Delacroix) Walters Art Museum 37.186.jpg'\n",
    "    # image_filename = 'Ingobertus_001.jpg' # should not work as of 2022-01-13\n",
    "    # image_filename = 'Fra_Filippo_Lippi_-_Madonna_and_Child_with_two_Angels_-_Uffizi.jpg'\n",
    "    # image_filename = 'Drawing of Abbie Sweetwine treating injured.jpg' # should produce nothing\n",
    "    # image_filename = 'A_Walk_along_a_Path_at_Sunset_by_Hermann_Herzog.jpg' # should be screened out\n",
    "    # image_filename = 'Andrea_Mantegna_015.jpg'\n",
    "    page_url = 'https://commons.wikimedia.org/wiki/File:' + image_filename\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    # Create a soup object and find the file info table\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Restrict only to links found in the mediawiki image page content section\n",
    "    tables = soup.findAll('table', class_ = re.compile('fileinfotpl-type-artwork'))\n",
    "    if len(tables) > 0:\n",
    "        # Find the rows in the image page content table\n",
    "        rows = tables[0].findAll('tr')\n",
    "        if len(rows) > 0:\n",
    "            # The header row of the table actually contains a nested th element that isn't inside a td element\n",
    "            # When the header row is missing, the first row contains two td elements and the second one has a th\n",
    "            # So the th must be directly inside the tr, not inside a td inside the tr.\n",
    "            for tag in rows[0].children:\n",
    "                if tag.name == 'th':\n",
    "                    # The link to the Wikidata item will be in an href in the th element.\n",
    "                    # Sometimes the artist link is to a Wikidata item, so can't screen on subdomain.\n",
    "                    anchors = tag.findAll('a', title = re.compile('wikidata:'))\n",
    "                    if len(anchors) > 0:\n",
    "                        link = anchors[0]['href']\n",
    "                        qid = extract_local_name(link)\n",
    "                        print(qid)\n",
    "                        retrieved_data = {'act_id': record['act_id'], 'qid': qid, 'filename': record['filename']}\n",
    "                        output_list.append(retrieved_data)\n",
    "    sleep(request_sleep) # Don't hit the server too fast\n",
    "    \n",
    "fieldnames = ['act_id', 'qid', 'filename']\n",
    "write_dicts_to_csv(output_list, 'wikidata_found.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
