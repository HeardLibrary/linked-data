{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare data dumped from ACT database to what's in Commons\n",
    "\n",
    "This is initially to create artwork items for ACT items attached to a non-artwork item in Wikidata.\n",
    "\n",
    "## Function section\n",
    "\n",
    "This needs to be run before any of the other cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2022 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import urllib\n",
    "import csv\n",
    "import os\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from copy import deepcopy\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "import datetime\n",
    "\n",
    "import re # regex\n",
    "from bs4 import BeautifulSoup # web-scraping library, use PIP to install beautifulsoup4 (included in Anaconda)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "get_server_sleep = 0.1 # number of seconds to wait before get calls to webserver\n",
    "dots_sleep = 1 # number of seconds to wait between calls to ParallelDots API\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# Calculate the reference date retrieved value for all statements\n",
    "whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "dateZ = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "ref_retrieved = dateZ + 'T00:00:00Z' # form 2019-12-05T00:00:00Z as provided by Wikidata, without leading +\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def load_credential(filename, directory):\n",
    "    cred = ''\n",
    "    # to change the script to look for the credential in the working directory, change the value of home to empty string\n",
    "    if directory == 'home':\n",
    "        home = str(Path.home()) #gets path to home directory; works for both Win and Mac\n",
    "        credential_path = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credential_path = filename\n",
    "    try:\n",
    "        with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "            cred = file_object.read()\n",
    "    except:\n",
    "        print(filename + ' file not found - is it in your ' + directory + ' directory?')\n",
    "        exit()\n",
    "    return(cred)\n",
    "\n",
    "# find non-redundant values for a column or simple list\n",
    "def non_redundant(table, column_key):\n",
    "    non_redundant_list = []\n",
    "    for row in table:\n",
    "        found = False\n",
    "        for test_item in non_redundant_list:\n",
    "            if column_key == '':\n",
    "                if row == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            else:\n",
    "                if row[column_key] == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            if column_key == '':\n",
    "                non_redundant_list.append(row)\n",
    "            else:\n",
    "                non_redundant_list.append(row[column_key])\n",
    "    return non_redundant_list\n",
    "\n",
    "# function to use in sort of simple list\n",
    "def sort_funct(row):\n",
    "    return row\n",
    "\n",
    "# function to use in sort last_first names\n",
    "def sort_last_first(row):\n",
    "    return row['last_first']\n",
    "\n",
    "# function to use in sort by match score\n",
    "def sort_score(row):\n",
    "    return row['score']\n",
    "\n",
    "# function to get local name from an IRI\n",
    "def extract_localname(iri):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[len(pieces)-1] # return the last piece\n",
    "\n",
    "# search label and alias\n",
    "# For whatever reason, if I use the graph pattern\n",
    "\n",
    "# wd:Q21 wdt:P31 ?class.\n",
    "\n",
    "# England is not Q6256 (country)\n",
    "# But if I use the graph pattern\n",
    "\n",
    "#   wd:Q21 p:P31 ?statement.\n",
    "#  ?statement ps:P31 ?class.\n",
    "\n",
    "# it is ??!!\n",
    "def searchLabelsAtWikidata(string, class_list):\n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id '\n",
    "    query += '''where {\n",
    "  {?id rdfs:label \"''' + string + '''\"@en.}\n",
    "  union\n",
    "  {?id skos:altLabel \"''' + string + '''\"@en.}\n",
    "  '''\n",
    "    for class_index in range(len(class_list)):\n",
    "        if class_index == 0:\n",
    "            query += '''{?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "        else:\n",
    "            query += '''union\n",
    "  {?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "    query += '''}'''\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    # r = requests.get(endpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_localname(result['id']['value'])\n",
    "        return_value.append(qid)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def retrieve_gallery_classes():\n",
    "    # create a string for the query\n",
    "    # use Metropolitan Museum of Art because there are too many collections to not specify the collection.\n",
    "    query = '''select distinct ?class ?label where \n",
    "      {\n",
    "      ?item wdt:P195 wd:Q160236.\n",
    "      ?item wdt:P31 ?class.\n",
    "      ?class rdfs:label ?label.\n",
    "      filter(lang(?label) = 'en')\n",
    "      }\n",
    "      order by ?label'''\n",
    "\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    print('sending query')\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    print('results returned')\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_localname(result['class']['value'])\n",
    "        label = result['label']['value']\n",
    "        return_value.append({'label': label, 'qid': qid})\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        nameVersion = ''\n",
    "        for pieceNumber in range(0, len(pieces)-1):\n",
    "            nameVersion += pieces[pieceNumber] + ' '\n",
    "        nameVersion += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        # get rid of quotes, which will break the query\n",
    "        alternative = alternative.replace('\"', '')\n",
    "        alternative = alternative.replace(\"'\", '')\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    # r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = extract_localname(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def name_variant_testing(name, variant):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.','')\n",
    "    variant = variant.replace('.','')\n",
    "    \n",
    "    # create first names\n",
    "    name_pieces = name.split(' ')\n",
    "    variant_pieces = variant.split(' ')\n",
    "    last_name = name_pieces[len(name_pieces)-1]\n",
    "    last_variant = variant_pieces[len(variant_pieces)-1]\n",
    "    if len(name_pieces) > 1:\n",
    "        first_names = name[0:-(len(last_name)+1)]\n",
    "    else:\n",
    "        first_names = name     \n",
    "    if len(variant_pieces) > 1:\n",
    "        first_variants = variant[0:-(len(last_variant)+1)]\n",
    "    else:\n",
    "        first_variants = variant      \n",
    "    #print(first_names)\n",
    "    #print(first_variants)\n",
    "    \n",
    "    # compare first names\n",
    "    # I experimented with the different ratios and I think fuzz might be best.\n",
    "    ratio = fuzz.ratio(first_names, first_variants)\n",
    "    #partial_ratio = fuzz.partial_ratio(first_names, first_variants)\n",
    "    #sort_ratio = fuzz.token_sort_ratio(first_names, first_variants)\n",
    "    #set_ratio = fuzz.token_set_ratio(first_names, first_variants)\n",
    "    # print('name similarity ratio', ratio)\n",
    "    #print('partial ratio', partial_ratio)\n",
    "    #print('sort_ratio', sort_ratio)\n",
    "    #print('set_ratio', set_ratio)\n",
    "\n",
    "    return(ratio)\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', '')\n",
    "    name = name.replace(',', '')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def remove_parens(string):\n",
    "    name_string = string.split('(')[0]\n",
    "    return name_string.strip()\n",
    "\n",
    "def remove_description(string):\n",
    "    try:\n",
    "        right_string = string.split('(')[1]\n",
    "        left_string = right_string.split(')')[0]\n",
    "        result = left_string.strip()\n",
    "    except:\n",
    "        result = ''\n",
    "    return result\n",
    "\n",
    "def reverse_names(string):\n",
    "    pieces = string.split(',')\n",
    "    return pieces[1].strip() + ' ' + pieces[0].strip()\n",
    "\n",
    "# Screens for Wikidata items that are potential matches\n",
    "\n",
    "import vb_common_code as vbc\n",
    "retrieve_class_list_query = vbc.Query(pid='P31', uselabel=False, sleep=sparql_sleep)\n",
    "retrieve_birth_date_query = vbc.Query(isitem=False, pid='P569', sleep=sparql_sleep)\n",
    "retrieve_death_date_query = vbc.Query(isitem=False, pid='P570', sleep=sparql_sleep)\n",
    "\n",
    "def human(qId):\n",
    "    screen = True\n",
    "    wdClassList = retrieve_class_list_query.single_property_values_for_item(qId)\n",
    "    # if there is a class property, check if it's a human\n",
    "    if len(wdClassList) != 0:\n",
    "        # if it's not a human\n",
    "        if wdClassList[0] != 'Q5':\n",
    "            #print('*** This item is not a human!')\n",
    "            screen = False\n",
    "    return screen\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return resultsDict\n",
    "\n",
    "def determine_era(string):\n",
    "    # dates with no CE or BCE, including empty string\n",
    "    if 'CE' not in string:\n",
    "        value = string\n",
    "        era = 'unknown'      \n",
    "    else:\n",
    "        if 'BCE' in string:\n",
    "            value = string[0:len(string)-3].strip()\n",
    "            era = 'BCE'\n",
    "        else: # string ends with CE\n",
    "            value = string[0:len(string)-2].strip()\n",
    "            era = 'CE'\n",
    "    return value, era\n",
    "\n",
    "def determine_zeros(date):\n",
    "    zero_count = 0\n",
    "    for char_number in range(len(date), 0, -1):\n",
    "        if date[char_number-1] == '0':\n",
    "            zero_count += 1\n",
    "        else:\n",
    "            return zero_count\n",
    "\n",
    "def pad_zeros_left(date_string):\n",
    "    length = len(date_string)\n",
    "    pad = 4-length\n",
    "    return '0' * pad + date_string\n",
    "\n",
    "def sign(era):\n",
    "    if era == 'BCE':\n",
    "        return '-'\n",
    "    elif era == 'CE':\n",
    "        return ''\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def detect_language(string):\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    return lang, confidence\n",
    "\n",
    "commons_prefix = 'http://commons.wikimedia.org/wiki/Special:FilePath/'\n",
    "commons_page_prefix = 'https://commons.wikimedia.org/wiki/File:'\n",
    "def commons_url_to_filename(url):\n",
    "    # form of URL is: http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\n",
    "    string = url.split(commons_prefix)[1] # get local name file part of URL\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_url(filename):\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_prefix + encoded_filename\n",
    "    return url\n",
    "\n",
    "def commons_page_url_to_filename(url):\n",
    "    # form of URL is: https://commons.wikimedia.org/wiki/File:Castle_De_Haar_(1892-1913)_-_360%C2%B0_Panorama_of_Castle_%26_Castle_Grounds.jpg\n",
    "    string = url.split(commons_page_prefix)[1] # get local name file part of URL\n",
    "    string = string.replace('_', ' ')\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_page_url(filename):\n",
    "    filename = filename.replace(' ', '_')\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_page_prefix + encoded_filename\n",
    "    url = url.replace('%28', '(').replace('%29', ')').replace('%2C', ',')\n",
    "    return url\n",
    "\n",
    "def retrieve_gallery_classes():\n",
    "    # Find the Q IDs of works with ACT identifiers that do not have one of the artwork types\n",
    "    query = '''select distinct ?item ?actId ?classLabel ?itemLabel ?image where {\n",
    "  ?item wdt:P9092 ?actId.\n",
    "  ?item wdt:P18 ?image.\n",
    "  optional {\n",
    "    ?item rdfs:label ?itemLabel.\n",
    "    filter(lang(?itemLabel)=\"en\")\n",
    "    }\n",
    "  ?item wdt:P31 ?class.\n",
    "  ?class rdfs:label ?classLabel.\n",
    "  filter(lang(?classLabel)=\"en\")\n",
    "  minus {?item wdt:P31 wd:Q3305213.} # painting\n",
    "  minus {?item wdt:P31 wd:Q15711026.} # altarpiece  \n",
    "  minus {?item wdt:P31 wd:Q93184.} # drawing\n",
    "  minus {?item wdt:P31 wd:Q22669139.} # fresco\n",
    "  minus {?item wdt:P31 wd:Q15123870.} # lithograph\n",
    "  minus {?item wdt:P31 wd:Q8362.} # miniature\n",
    "  minus {?item wdt:P31 wd:Q133067.} # mosaic\n",
    "  minus {?item wdt:P31 wd:Q219423.} # mural\n",
    "  minus {?item wdt:P31 wd:Q125191.} # photograph\n",
    "  minus {?item wdt:P31 wd:Q11060274.} # print\n",
    "  minus {?item wdt:P31 wd:Q1064538.} # quilt\n",
    "  minus {?item wdt:P31 wd:Q245117.} # relief sculpture\n",
    "  minus {?item wdt:P31 wd:Q860861.} # sculpture\n",
    "  minus {?item wdt:P31 wd:Q2282251.} # seven-branched candlestick\n",
    "  minus {?item wdt:P31 wd:Q1473346.} # stained glass\n",
    "  minus {?item wdt:P31 wd:Q179700.} # statue\n",
    "  minus {?item wdt:P31 wd:Q18761202.} # watercolor painting\n",
    "  minus {?item wdt:P31 wd:Q48498.} # illuminated manuscript\n",
    "  minus {?item wdt:P31 wd:Q184296.} # tapestry\n",
    "  minus {?item wdt:P31 wd:Q811979.} # architectural structure\n",
    "  minus {?item wdt:P31 wd:Q1278452.} # polyptych\n",
    "  minus {?item wdt:P31 wd:Q15727816.} # painting series\n",
    "  minus {?item wdt:P31 wd:Q16744570.} # tablet\n",
    "  minus {?item wdt:P31 wd:Q87167.} # manuscript\n",
    "  minus {?item wdt:P31 wd:Q16905563.} # cycle of paintings\n",
    "  minus {?item wdt:P31 wd:Q132137.} # icon\n",
    "  minus {?item wdt:P31 wd:Q18887969.} # copper engraving print\n",
    "  minus {?item wdt:P31 wd:Q28823.} # textile\n",
    "  minus {?item wdt:P31 wd:Q2293362.} # sculptural group\n",
    "  }\n",
    "#order by ?classLabel\n",
    "'''\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    print('sending query')\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    print('results returned')\n",
    "    #print(r.text)\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_localname(result['item']['value'])\n",
    "        if 'itemLabel' in result:\n",
    "            label = result['itemLabel']['value']\n",
    "        else:\n",
    "            label = ''\n",
    "        class_label = result['classLabel']['value']\n",
    "        act_id = result['actId']['value']\n",
    "        image = result['image']['value']\n",
    "        return_value.append({'qid': qid, 'act_id': act_id, 'class_label': class_label, 'label': label, 'image': commons_url_to_filename(image)})\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def check_commons_page_for_wikidata_link(image_filename):\n",
    "    # image_filename = 'Christ sur la mer de Galilée (Delacroix) Walters Art Museum 37.186.jpg'\n",
    "    # image_filename = 'Christ_and_SocratesSAAM_1974.28.341A_B_1.jpg'\n",
    "    page_url = 'https://commons.wikimedia.org/wiki/File:' + image_filename\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    # Create a soup object and find the file info table\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.findAll('table', class_= re.compile('fileinfotpl'))\n",
    "    if len(tables) > 0:\n",
    "        # Have to check for this span because there are other subtables with a tags besides the one at the top\n",
    "        span = tables[0].findAll('span', id = 'artwork')\n",
    "        if len(span) > 0:\n",
    "            # The link to the Wikidata item will be in an href\n",
    "            # Need to go up to parent, since the anchor is sometimes a sibling tag and not a child tag\n",
    "            anchors = span[0].parent.findAll('a', href = re.compile('https://www.wikidata.org/wiki/'))\n",
    "            print(anchors)\n",
    "            if len(anchors) > 0:\n",
    "                try:\n",
    "                    link = anchors[0]['href']\n",
    "                    qid = extract_localname(link)\n",
    "                except:\n",
    "                    qid = ''\n",
    "            else:\n",
    "                qid = ''\n",
    "        else:\n",
    "            qid = ''\n",
    "    else:\n",
    "        qid = ''\n",
    "    sleep(get_server_sleep) # Don't hit the API too fast\n",
    "    return qid\n",
    "\n",
    "def check_commons_page_for_wikidata_image_link(image_filename):\n",
    "    # image_filename = 'Christ sur la mer de Galilée (Delacroix) Walters Art Museum 37.186.jpg'\n",
    "    # image_filename = 'Christ_and_SocratesSAAM_1974.28.341A_B_1.jpg'\n",
    "    page_url = 'https://commons.wikimedia.org/wiki/File:' + image_filename\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    # Create a soup object and find the file info table\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    image_tags = soup.findAll('img', src= 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/20px-Wikidata-logo.svg.png')\n",
    "    if len(image_tags) > 0:\n",
    "        anchor = image_tags[0].parent\n",
    "        link = anchor['href']\n",
    "        qid = extract_localname(link)\n",
    "    else:\n",
    "        qid = ''\n",
    "    sleep(get_server_sleep) # Don't hit the API too fast\n",
    "    return qid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to find out if works have items under a different image\n",
    "\n",
    "Check the Commons page to see if there is a Wikidata link pointing to a different item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = retrieve_gallery_classes()\n",
    "print('Items found:', len(hits))\n",
    "print()\n",
    "\n",
    "filename = 'works_already_in_wikidata.csv'\n",
    "works = read_dict(filename)\n",
    "\n",
    "results_list = []\n",
    "for hit in hits:\n",
    "    result_dict = {'bad_qid': hit['qid'], 'bad_label': hit['label']}\n",
    "    # Look up hit in list of works already in Wikidata\n",
    "    found = False\n",
    "    for work in works:\n",
    "        if hit['act_id'] == work['RecordNumber']:\n",
    "            result_dict['filename'] = work['filename']\n",
    "            found = True\n",
    "            \n",
    "            # Check the Commons web page for a Wikidata link\n",
    "            qid = check_commons_page_for_wikidata_link(work['filename'])\n",
    "            if qid != '':\n",
    "                result_dict['good_qid'] = qid\n",
    "                results_list.append(result_dict)\n",
    "            else:\n",
    "                qid = 'no link'\n",
    "            print(hit['qid'], hit['label'], qid)           \n",
    "            break\n",
    "    if not found:\n",
    "        print(hit['qid'], 'not found in ACT database. ', hit['label'])\n",
    "       \n",
    "#print(results_list)\n",
    "fieldnames = ['bad_qid', 'bad_label', 'filename', 'good_qid']\n",
    "write_dicts_to_csv(results_list, 'wikidata_items_to_check.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a hack of above script used after adding abstract artworks that were missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = retrieve_gallery_classes()\n",
    "#print(json.dumps(hits, indent=2))\n",
    "print('Items found:', len(hits))\n",
    "print()\n",
    "\n",
    "\n",
    "filename = 'works_already_in_wikidata.csv'\n",
    "works = read_dict(filename)\n",
    "\n",
    "results_list = []\n",
    "for hit in hits:\n",
    "    result_dict = {'bad_qid': hit['qid'], 'bad_class': hit['class_label'], 'bad_label': hit['label'], 'image': hit['image']}\n",
    "    # Look up hit in list of works already in Wikidata\n",
    "    found = False\n",
    "    for work in works:\n",
    "        if hit['act_id'] == work['RecordNumber']:\n",
    "            result_dict['good_qid'] = work['qid']\n",
    "            found = True\n",
    "            break\n",
    "    results_list.append(result_dict)\n",
    "       \n",
    "#print(results_list)\n",
    "fieldnames = ['bad_qid', 'bad_class', 'good_qid', 'bad_label', 'image']\n",
    "write_dicts_to_csv(results_list, 'wikidata_items_to_delete_act.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to pull data for works needing processing\n",
    "\n",
    "Run after running functions cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = retrieve_gallery_classes()\n",
    "print('Items found:', len(hits))\n",
    "print()\n",
    "\n",
    "filename = 'works_already_in_wikidata.csv'\n",
    "works = read_dict(filename)\n",
    "\n",
    "filename = 'act_all_202109241353_repaired.csv'\n",
    "act_metadata = read_dict(filename)\n",
    "\n",
    "filename = 'already_in_templated_data.csv'\n",
    "commons_metadata = read_dict(filename)\n",
    "\n",
    "act_database_list = []\n",
    "commons_scrape_list = []\n",
    "\n",
    "for hit in hits:\n",
    "    \n",
    "    # Look up hit in the ACT metadata to store in CSV\n",
    "    found = False\n",
    "    for work in act_metadata:\n",
    "        if hit['act_id'] == work['RecordNumber']:\n",
    "            print(hit['qid'], hit['label'])\n",
    "            found = True\n",
    "            act_database_list.append(work)\n",
    "            break\n",
    "    if not found:\n",
    "        print(hit['qid'], 'not found in ACT database. ', hit['label'])\n",
    "\n",
    "    # Look up the hit in the list of works already in Wikidata to get the filename\n",
    "    found = False\n",
    "    for work in works:\n",
    "        if hit['act_id'] == work['RecordNumber']:\n",
    "            filename = work['filename']\n",
    "            found = True\n",
    "    if not found:\n",
    "        print('could not find in list of works already in Wikidata')\n",
    "        \n",
    "    if found:\n",
    "        label = work['filename']\n",
    "        print('image filename:', label)\n",
    "\n",
    "        found = False\n",
    "        for commons_item in commons_metadata:\n",
    "            if commons_item['filename'] == filename:\n",
    "                found = True\n",
    "                commons_scrape_list.append(commons_item)\n",
    "                #artist_data = json.loads(commons_item['artist'])\n",
    "                #author_data = json.loads(commons_item['author'])\n",
    "                #try:\n",
    "                #    if artist_data ==[]:\n",
    "                #        print('Commons artist:', author_data[0])\n",
    "                #    else:\n",
    "                #        print('Commons artist:', artist_data[0])\n",
    "                #except:\n",
    "                #    print('No artist information in Commons.')\n",
    "                break\n",
    "        if not found:\n",
    "            print('Not found in Commons database.')\n",
    "        \n",
    "    print()\n",
    "    \n",
    "#print(json.dumps(works_list, indent = 2))\n",
    "fieldnames = act_database_list[0].keys()\n",
    "write_dicts_to_csv(act_database_list, 'act_data_fix.csv', fieldnames)\n",
    "\n",
    "fieldnames = commons_scrape_list[0].keys()\n",
    "write_dicts_to_csv(commons_scrape_list, 'commons_data_fix.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hack of above script to process works in Commons and known to not be in Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not need to look up ACT IDs in Wikidata because the works aren't there.\n",
    "\n",
    "#hits = retrieve_gallery_classes()\n",
    "#print('Items found:', len(hits))\n",
    "#print()\n",
    "\n",
    "# Commons works to generate metadata for are in this file\n",
    "filename = '../create_items/add_to_wikidata.csv'\n",
    "works_to_add = read_dict(filename)\n",
    "\n",
    "filename = 'act_all_202109241353_repaired.csv'\n",
    "act_metadata = read_dict(filename)\n",
    "\n",
    "filename = 'already_in_templated_data.csv'\n",
    "commons_metadata = read_dict(filename)\n",
    "\n",
    "act_database_list = []\n",
    "commons_scrape_list = []\n",
    "\n",
    "for work_to_add in works_to_add:\n",
    "    \n",
    "    print('image filename:', work_to_add['filename'])\n",
    "\n",
    "    # Look up hit in the ACT metadata to store in CSV\n",
    "    found = False\n",
    "    for work in act_metadata:\n",
    "        if work_to_add['act_id'] == work['RecordNumber']:\n",
    "            found = True\n",
    "            act_database_list.append(work)\n",
    "            break\n",
    "    if not found:\n",
    "        print(work_to_add['act_id'], 'not found in ACT database. ')\n",
    "\n",
    "    found = False\n",
    "    for commons_item in commons_metadata:\n",
    "        if commons_item['filename'] == work_to_add['filename']:\n",
    "            found = True\n",
    "            commons_scrape_list.append(commons_item)\n",
    "            #artist_data = json.loads(commons_item['artist'])\n",
    "            #author_data = json.loads(commons_item['author'])\n",
    "            #try:\n",
    "            #    if artist_data ==[]:\n",
    "            #        print('Commons artist:', author_data[0])\n",
    "            #    else:\n",
    "            #        print('Commons artist:', artist_data[0])\n",
    "            #except:\n",
    "            #    print('No artist information in Commons.')\n",
    "            break\n",
    "    if not found:\n",
    "        print('Not found in Commons database.')\n",
    "        \n",
    "    print()\n",
    "    \n",
    "#print(json.dumps(works_list, indent = 2))\n",
    "fieldnames = act_database_list[0].keys()\n",
    "write_dicts_to_csv(act_database_list, 'act_data_fix.csv', fieldnames)\n",
    "\n",
    "fieldnames = commons_scrape_list[0].keys()\n",
    "write_dicts_to_csv(commons_scrape_list, 'commons_data_fix.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
